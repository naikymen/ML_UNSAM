{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introducción\" data-toc-modified-id=\"Introducción-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introducción</a></span></li><li><span><a href=\"#Regularización\" data-toc-modified-id=\"Regularización-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Regularización</a></span><ul class=\"toc-item\"><li><span><a href=\"#En-el-capítulo-anterior...\" data-toc-modified-id=\"En-el-capítulo-anterior...-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>En el capítulo anterior...</a></span></li><li><span><a href=\"#Regresión-Ridge\" data-toc-modified-id=\"Regresión-Ridge-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Regresión <em>Ridge</em></a></span><ul class=\"toc-item\"><li><span><a href=\"#Valores-de-los-parámetros\" data-toc-modified-id=\"Valores-de-los-parámetros-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Valores de los parámetros</a></span></li><li><span><a href=\"#Función-de-error-modificada\" data-toc-modified-id=\"Función-de-error-modificada-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Función de error modificada</a></span></li><li><span><a href=\"#Shrinkage\" data-toc-modified-id=\"Shrinkage-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Shrinkage</a></span></li></ul></li><li><span><a href=\"#Lasso\" data-toc-modified-id=\"Lasso-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Lasso</a></span></li><li><span><a href=\"#Interpretación-geométrica\" data-toc-modified-id=\"Interpretación-geométrica-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Interpretación geométrica</a></span></li></ul></li><li><span><a href=\"#Bias-Variance-tradeoff\" data-toc-modified-id=\"Bias-Variance-tradeoff-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Bias-Variance tradeoff</a></span><ul class=\"toc-item\"><li><span><a href=\"#Cálculo-completo-de-sesgo-y-varianza\" data-toc-modified-id=\"Cálculo-completo-de-sesgo-y-varianza-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Cálculo completo de sesgo y varianza</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vimos cómo el uso de un modelo demasiado complejo para describir un conjunto de datos de tamaño limitado puede provocar un sobreajuste. Hasta ahora, hemos considerado el *overfitting* como una situación en la que el rendimiento del modelo en el conjunto de entrenamiento es mucho mejor que en los conjuntos de test o validación. Pero hay mucho más.\n",
    "\n",
    "También discutimos cómo al elegir cuidadosamente los hiperparámetros del modelo, se puede alcanzar un nivel adecuado de complejidad del modelo.\n",
    "\n",
    "Sin embargo, en muchas situaciones, nos gustaría mantener un modelo complejo para capturar las sutilezas del proceso de generación de datos. En el ejemplo de la semana pasada, que reconsideraremos acá con ciertas modificaciones, no hay un grado de polinomio que realmente capture el proceso correcto detrás de los datos. En otras palabras, el proceso real no está incluído en el subespacio generado por las funciones de base. Idealmente, necesitaríamos un polinomio de alto grado para hacer esto. Pero en ese caso, también debemos evitar que el modelo aprenda de memoria el ruido de la realización de los datos dados e incurra en un sobreajuste.\n",
    "\n",
    "En este *notebook*, primero describiremos la **regularización**, una técnica para controlar la complejidad de los modelos sin reducir por completo su flexibilidad, y luego analizaremos de manera diferente la idea de sobreajuste y complejidad del modelo, introduciendo el concepto de **sesgo** y **varianza** de un modelo, y discutiendo su **balance** (*tradeoff*) natural. Estas ideas nos acompañarán durante el resto del viaje y serán clave para comprender el funcionamiento interno de algunas técnicas de aprendizaje automático, como los métodos de conjunto.\n",
    "\n",
    "<font size = 3> **¡Empecemos!** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"05_Regularization\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"plots\", CHAPTER_ID)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En el capítulo anterior..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos recreando un dataset con propiedades similares al del martes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N_SAMPLES = 20\n",
    "x = np.linspace(0,1,num=N_SAMPLES).reshape(-1,1)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def ground_truth(x):\n",
    "    return 4 * x + np.sin(x*6)\n",
    "\n",
    "t = ground_truth(x) + 0.5 * np.random.randn(N_SAMPLES,1)\n",
    "\n",
    "x_train, x_test, t_train, t_test = train_test_split(x, t, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_train, t_train, c='blue', label='train')\n",
    "plt.scatter(x_test, t_test, c='red', label='test')\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Habíamos visto que ajustar un polinomio de orden superior (por encima de 4, digamos) conduce a una capacidad de generalización muy pobre. Esto se ve al evaluar las capacidades de predicción del modelo en el conjunto de testeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define practical equations.\n",
    "def plot_data(model, x, y, x_test=[], y_test=[]):\n",
    "    # Plot data\n",
    "    plt.scatter(x, y, c='red', label='train')\n",
    "    \n",
    "    # Test is present\n",
    "    if len(x_test) > 0:\n",
    "        assert len(x_test) == len(y_test), \"Test dataset size incompatible\"\n",
    "        plt.scatter(x_test, y_test, c='blue', label='test')\n",
    "\n",
    "    # Define oversample x array for plotting\n",
    "    x_ = np.linspace(0,1, 100).reshape(-1,1)\n",
    "\n",
    "    # Make model prediction on oversampled array\n",
    "    predictions = model.predict(x_)\n",
    "    \n",
    "    # Plot predicted curve\n",
    "    plt.plot(x_, predictions, c='green', label='predictions', lw=4, alpha=0.8)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.show()\n",
    "    return\n",
    "    \n",
    "def compute_errors(model, x_train, y_train, x_test=None, y_test=None, print_result=True):\n",
    "    \"\"\"Compute and print MSE for training and, if given, test datasets.\"\"\"\n",
    "    training_error = mean_squared_error(y_train, model.predict(x_train))\n",
    "    if print_result: print(f\"The training mse is: {round(training_error,2)}\")\n",
    "    \n",
    "    if x_test is not None and y_test is not None:\n",
    "        test_error = mean_squared_error(y_test, model.predict(x_test))\n",
    "        if print_result: print(f\"The test mse is: {round(test_error,2)}\")\n",
    "        return training_error, test_error\n",
    "    else:\n",
    "        return training_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to perform pipeline for a given degree.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def polynomial_regressor(M):\n",
    "    pr = Pipeline([\n",
    "        ('poly_features', PolynomialFeatures(M)),\n",
    "        ('regressor', LinearRegression(fit_intercept=False) )])\n",
    "    return pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = range(1, 10)\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "models = []\n",
    "\n",
    "for M in degrees:\n",
    "    print(f\"Polynomial degree: {M}\")\n",
    "    pr = polynomial_regressor(M)\n",
    "    pr.fit(x_train, t_train)\n",
    "    \n",
    "    train_e, test_e = compute_errors(pr, x_train, t_train, x_test, t_test, print_result=True)\n",
    "    \n",
    "#     plot_data(pr,x_train,y_train, x_test, y_test)\n",
    "    \n",
    "    train_errors.append(train_e)\n",
    "    test_errors.append(test_e)\n",
    "    models.append(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform multi-plot\n",
    "ncolumns = 3\n",
    "\n",
    "fig = plt.figure(figsize=(14, 14))\n",
    "\n",
    "if len(models) % ncolumns == 0:\n",
    "    extrarow = 0\n",
    "else:\n",
    "    extrarow = 1\n",
    "        \n",
    "axs = fig.subplots(ncols=ncolumns, nrows=int(np.floor(len(models)/ncolumns) + extrarow))\n",
    "\n",
    "x_ = np.linspace(0,1, 100).reshape(-1,1)\n",
    "for i, ax in zip(range(len(models)), axs.flatten()):\n",
    "    ax.plot(x_train, t_train, 'o', ms=10, mfc='None', label='Train')\n",
    "    ax.plot(x_test, t_test, 'or', ms=10, mfc='None', label='Test')\n",
    "    ax.plot(x_, models[i].predict(x_), 'r-', lw=3, alpha=0.8, label='Predicted curve')\n",
    "    ax.plot(x_, ground_truth(x_), 'k-', lw=3, alpha=0.5, label='Ground truth')\n",
    "    #\n",
    "    ax.set_title('Degree: {}'.format(models[i]['poly_features'].degree))\n",
    "    #\n",
    "    ax.set_ylim(0, 3.9)\n",
    "    \n",
    "# Make a single legend\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', ncol=len(handles), \n",
    "           fontsize=mpl.rc_params()['axes.labelsize'], borderaxespad=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos visto que el polinomio con el error de testeo más bajo fue el de grado cuatro. Al inspeccionar la curva, parece ser la que mejor se aproxima al proceso real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "plt.semilogy(range(1, len(test_errors)+1), test_errors, '-or', mfc='None', ms=10, label='Test Error')\n",
    "plt.semilogy(range(1, len(train_errors)+1), train_errors, '-ob', mfc='None', ms=10, label='Train Error')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión *Ridge*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El sobreajuste es un subproducto de la búsqueda (*obsesiva*, se podría decir) de minimizar una función de error utilizando modelos muy flexibles. Parece claro que podemos ganar algo imponiendo algunos límites a la función de error.\n",
    "\n",
    "La escribimos:\n",
    "\n",
    "$$\n",
    "MSE(\\boldsymbol{\\omega}) = \\frac{1}{2} \\sum_{i=1}^{N} \\left\\{y(x_i, \\boldsymbol{\\omega}) - t_i\\right\\}^2\\;\\;.\n",
    "$$\n",
    "\n",
    "**¿Cómo deberíamos modificar esta expresión?** \n",
    "\n",
    "Podemos obtener algunas pistas estudiando los valores de los parámetros de la regresión polinomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valores de los parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = np.full((len(models)+1, len(models)), np.nan)\n",
    "for i in range(len(models)):\n",
    "    coef[:i+2, i] = models[i]['regressor'].coef_[0]\n",
    "\n",
    "coef_df = pd.DataFrame(coef, columns=range(1, len(models)+1))\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Pregunta**\n",
    "\n",
    "* ¿Qué ven?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de error modificada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En vistas de que los valores de los coeficientes aumentan dramáticamente cuando empezamos a sobreajustar, podemos pensar en incluir una penalización para valores muy grandes de los parámetros. Escribimos, entonces, una nueva función de error:\n",
    "\n",
    "$$\n",
    "E_\\text{ridge}(\\boldsymbol{\\omega}; \\lambda) = \\frac{1}{2} \\sum_{i=1}^{N} \\left\\{y(x_i, \\boldsymbol{\\omega}) - t_i\\right\\}^2\n",
    " + \\frac{\\lambda}{2} \\sum_{i=1}^M \\omega_i^2\\;\\;, \n",
    "$$\n",
    "donde $\\lambda$ se llama el término de regularización (o de penalización). El segundo término es equivalente al cuadrado de la norma $L2$ del vector de parámetros. Es decir,\n",
    "\n",
    "$$\n",
    "||\\boldsymbol{\\omega}||^2 = \\boldsymbol{\\omega}^T \\boldsymbol{\\omega} = \\left(\\omega_1 \\ldots \\omega_M\\right)\\begin{pmatrix}\\omega_1 \\\\ \\vdots \\\\ \\omega_M\\end{pmatrix} = \\sum_{i=1}^M \\omega_i^2\\;\\;.\n",
    "$$\n",
    "\n",
    "El parámetro de regularización constituye un nuevo hiperparámetro del modelo (**¿además de qué?**) y debe determinarse, por ejemplo, mediante validación cruzada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con este término adicional, vemos que los valores de parámetros muy grandes serán penalizados. Una de las mejores cosas de esta regresión, llamada **regresión de Ridge**, es que todavía existe una forma analítica de encontrar los parámetros que minimizan la función de error modificada. Esta es una consecuencia directa del uso de la norma $L2$, que es solo la suma de cuadrados de los parámetros del modelo. \n",
    "\n",
    "Por otro lado, noten que estos valores ya no constituyen los estimadores de máxima verosimilitud del problema, como vimos la otra vez. Perderemos algunas de las propiedades agradables que tienen los estimadores de máxima verosimilitud (por ejemplo, el hecho de ser insesgados). Pero ganaremos mucho. En particular, una varianza reducida.\n",
    "\n",
    "Veamos cómo funciona esto en la práctica. Ajustemos nuevamente un polinomio de noveno grado a los datos, pero esta vez usando un regresor regularizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def ridge(m, lam):\n",
    "    return Pipeline([('poly_features', PolynomialFeatures(degree=m)),\n",
    "                     ('regressor', Ridge(alpha=lam/2.0, fit_intercept=False))])\n",
    "\n",
    "# Fit Ridge\n",
    "rr = ridge(degrees[-1], 0.001)\n",
    "rr.fit(x_train, t_train)\n",
    "\n",
    "# Fit OLS\n",
    "ll = polynomial_regressor(degrees[-1])\n",
    "ll.fit(x_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x_train, t_train, 'o', ms=10, mfc='None', label='Train')\n",
    "ax.plot(x_, rr.predict(x_), 'r-', lw=3, alpha=0.8, label='Predicted curve (ridge)', zorder=2)\n",
    "ax.plot(x_, ll.predict(x_), 'g-', lw=1, alpha=0.8, label='Predicted curve (OLS)', zorder=0)\n",
    "ax.plot(x_, ground_truth(x_), 'k-', lw=3, alpha=0.5, label='Ground truth', zorder=1)\n",
    "    #\n",
    "ax.set_title('Degree: {}; $\\lambda$: {:.2e}'.format(rr['poly_features'].degree, \n",
    "                                                    rr['regressor'].alpha *2), fontsize=16)\n",
    "    #\n",
    "ax.set_ylim(0, 3.9)\n",
    "ax.legend(loc='best', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr['regressor'].coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gracias al término de regularización, la curva es menos ondulada, y nos da la impresión de que debería producir una mejor generalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('OLS; degree 9')\n",
    "print('The training mse is: {:.2f}'.format(train_errors[-1]))\n",
    "print('The test mse is: {:.2f}'.format(test_errors[-1]))\n",
    "print('####')\n",
    "print('Ridge Regression; degree {}, lambda = {}'.format(rr['poly_features'].degree, \n",
    "                                                        rr['regressor'].alpha *2))\n",
    "train_e, test_e = compute_errors(rr, x_train, t_train, x_test, t_test, print_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shrinkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas técnicas a veces se denominan **shrinkage** (contracción o encojimiento), porque hacen que los parámetros del modelo se reduzcan a cero. Como veremos más adelante, esto hace que el modelo cambie algo varianza por algo de sesgo.\n",
    "\n",
    "Veamos la evolución de los valores de los parámetros a medida que pasamos de valores pequeños a grandes del término de penalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid values of lambda\n",
    "lls = np.logspace(-5, 2, 100)\n",
    "\n",
    "cc = []\n",
    "\n",
    "# Iterate over values, fit, and record coefficient values\n",
    "for ll in lls:\n",
    "    rr = ridge(degrees[-1], ll)\n",
    "    rr.fit(x_train, t_train)\n",
    "    cc.append(rr['regressor'].coef_[0])\n",
    "\n",
    "cc = np.array(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot coefficient amplitudes versus regularization parameter.\n",
    "fig = plt.figure(figsize=(8,7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for i in range(len(cc[0])):\n",
    "    ax.semilogx(lls, cc[:, i], label='$\\omega_{{{}}}$'.format(i), lw=3, alpha=0.6)\n",
    "ax.legend(ncol=3, fontsize=16)\n",
    "ax.set_xlabel('$\\lambda$')\n",
    "ax.set_ylabel('Parameter value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una regresión regularizada diferente que se usa a menudo es la **regresión LASSO**, que naturalmente selecciona las variables más relevantes y produce modelos más parsimoniosos.\n",
    "\n",
    "En lugar de penalizar la función de error usando la suma de cuadrados de los parámetros del modelo como hicimos más arriba, **LASSO** usa la norma $l1$, que es simplemente la suma de los *valores absolutos* de los parámetros del modelo.\n",
    "\n",
    "En otras palabras, la norma $l1$ de un vector es simplemente:\n",
    "\n",
    "$$\n",
    "||\\boldsymbol{\\omega}||_1 = \\sum_i |\\omega_i|\\;\\;.\n",
    "$$\n",
    "\n",
    "Por tanto, la función de error modificada es\n",
    "\n",
    "$$\n",
    "E_\\text{lasso}(\\boldsymbol{\\omega}; \\lambda) = \\frac{1}{2} \\sum_{i=1}^{N} \\left\\{y(x_i, \\boldsymbol{\\omega}) - t_i\\right\\}^2\n",
    " + \\frac{\\lambda}{2} \\sum_{i=1}^M |\\omega_i|\\;\\;, \n",
    "$$\n",
    "\n",
    "donde hemos vuelto a introducir el hiperparámetro $\\lambda$ para controlar el nivel de penalización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera consecuencia de esta elección de penalización es que la función de error ya no puede optimizarse (minimizarse) de manera analítica. Hay que recurrir, entonces, a diferentes algoritmos iterativos.\n",
    "\n",
    "En `sklearn`, existen al menos dos implementaciones:\n",
    "\n",
    "* `linear_model.Lasso`, que usa *coordinate descent* para encontrar el mínimo.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Coordinate_descent.svg/500px-Coordinate_descent.svg.png\" width=500px></img>\n",
    "\n",
    "* `linear_model.LassoLars`, que usa LARS (least angle regression), que es similar a forward stepwise regression (es decir, se empieza con los coeficientes en cero, $\\boldsymbol{\\omega} = 0$, y se van aumentando prograsivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LassoLars\n",
    "\n",
    "def lasso(m, lam):\n",
    "    return Pipeline([('poly_features', PolynomialFeatures(degree=m)),\n",
    "                     ('regressor', Lasso(alpha=lam/2.0, fit_intercept=False, \n",
    "                                         max_iter=500000))])\n",
    "\n",
    "rr = lasso(9, 0.001)\n",
    "rr.fit(x_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x_train, t_train, 'o', ms=10, mfc='None', label='Train')\n",
    "ax.plot(x_, rr.predict(x_), 'r-', lw=3, alpha=0.8, label='Predicted curve')\n",
    "ax.plot(x_, ground_truth(x_), 'k-', lw=3, alpha=0.5, label='Ground truth')\n",
    "    #\n",
    "ax.set_title('Degree: {}; $\\lambda$: {:.2e}'.format(rr['poly_features'].degree, \n",
    "                                                    rr['regressor'].alpha *2), fontsize=16)\n",
    "    #\n",
    "ax.set_ylim(0, 3.9)\n",
    "ax.legend(loc=0, fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first look, it seems that both regressors produce the same results. But in reality there is a _big_ difference between both methods.\n",
    "\n",
    "Let's redo the shrinkage plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr['regressor'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid values of lambda\n",
    "lls = np.logspace(-4, 1, 100)\n",
    "\n",
    "# Polynomial degree\n",
    "deg = 9\n",
    "\n",
    "cc_lasso = np.empty([len(lls), deg+1])\n",
    "\n",
    "# Iterate over values, fit, and record coefficient values\n",
    "for i, ll in enumerate(lls):\n",
    "    #print(ll)\n",
    "    rr = lasso(deg, ll)\n",
    "    rr.fit(x_train, t_train)\n",
    "    cc_lasso [i] =rr['regressor'].coef_\n",
    "\n",
    "# cc_lasso = np.array(cc_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot coefficient amplitudes versus regularization parameter.\n",
    "fig = plt.figure(figsize=(8,7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for i in range(len(cc_lasso[0])):\n",
    "    ax.semilogx(lls, cc_lasso[:, i], label='$\\omega_{}$'.format(i), lw=3, alpha=0.6)\n",
    "ax.legend(ncol=3, fontsize=16)\n",
    "ax.set_xlabel('$\\lambda$')\n",
    "ax.set_ylabel('Parameter value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all coefficients\n",
    "\n",
    "# Instanciate Figure and Axes\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Create meshgrid based on values of lambda\n",
    "Xp, Yp = np.meshgrid(np.arange(10)-0.5, lls)\n",
    "Zp = np.where(np.abs(cc_lasso)>0, np.log(np.abs(cc_lasso)), -np.inf)\n",
    "\n",
    "# Vertical lines\n",
    "for i in range(10):\n",
    "    ax.axvline(i+0.5, color='0.8', lw=0.5)\n",
    "    \n",
    "# Color plot\n",
    "pcol = ax.pcolor(Xp, Yp, Zp)\n",
    "\n",
    "\n",
    "# Set log scale\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Colorbar and labels\n",
    "plt.colorbar(pcol, label='$\\log\\left(|\\omega_j|\\\\right)$')\n",
    "ax.set_xlabel('$j$', fontsize=18)\n",
    "ax.set_ylabel('$\\lambda$', fontsize=18)\n",
    "\n",
    "ax.set_title('Lasso shrinkage plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = [0, 3, 5]\n",
    "plt.figure(figsize=(9, 4))\n",
    "\n",
    "plt.semilogx(lls, cc_lasso[:, j])\n",
    "    \n",
    "plt.axhline(0, color='r', ls=':')\n",
    "plt.xlabel('Regularization parameter, $\\lambda$')\n",
    "\n",
    "if len(j) > 1:\n",
    "    plt.legend(plt.gca().lines, ['$\\omega_{}$'.format(jj) for jj in j], fontsize=16)\n",
    "    plt.ylabel('Parameter value')\n",
    "else:\n",
    "    plt.ylabel('$\\omega_{}$'.format(j))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver, a medida que aumentamos el término de regularización, algunos parámetros van estrictamente a cero. De esta manera, la regresión Lasso también funciona como una especie de herramienta de selección de modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretación geométrica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E_\\text{ridge}(\\boldsymbol{\\omega}; \\lambda) = \\frac{1}{2} \\sum_{i=1}^{N} \\left\\{y(x_i, \\boldsymbol{\\omega}) - t_i\\right\\}^2\n",
    " + \\frac{\\lambda}{2} \\sum_{i=1}^M \\omega_i^2\\;\\;, \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender el comportamiento diferente entre Ridge y Lasso, usemos un lindo plot del [libro de Bishop](https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/).\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "      <img src=\"images/05_Regularization/ridge.png\" width=250px>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"images/05_Regularization/lasso.png\" width=250px>\n",
    "        </td>\n",
    "    </tr>\n",
    "    </table>\n",
    "\n",
    "Donde el punto azul en el centro de los círculos representa la solución no regularizada, es decir, la solución OLS, y $\\omega^*$ es el vector de parámetros óptimo bajo penalización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<font size=4>Ahora a ustedes! Preparen sus teclados!</font>\n",
    "<!-- ### Prepare your keyboards! -->\n",
    "***\n",
    "\n",
    "* Utilicen validación cruzada (`GridSearchCV` o `RandomizedSearchCV`) para encontrar los valores óptimos de los hiperparámetros (orden polinómico M y parámetro de regularización $\\lambda = 2 \\alpha$.\n",
    "* Hagan esto para **Ridge** y **Lasso** de forma independiente.\n",
    "* Hacer un gráfico de la dependencia del error de testeo con los valore de los hiperparámetros. Dado que solo hay dos hiperparámetros, tal vez quieras probar `plt.pcolor` o `plt.imshow`.\n",
    "* Registrar los valores de los mejores hiper-parámetros.\n",
    "\n",
    "**Nota**: recuerdá que podés guardar las imágenes que quieras con la función `save_fig` definida arriba.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance tradeoff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 15\n",
    "x = np.linspace(0,1,num=N_SAMPLES).reshape(-1,1)\n",
    "\n",
    "def ground_truth(x):\n",
    "    return 4 * x+ np.sin(x*6)\n",
    "\n",
    "N_REPEAT = 50\n",
    "\n",
    "all_data = np.empty([N_REPEAT, N_SAMPLES])\n",
    "\n",
    "# Repeat realization of data many times\n",
    "for i in range(N_REPEAT):\n",
    "    t =  ground_truth(x.flatten()) + 0.5*np.random.randn(N_SAMPLES)\n",
    "#     x_train, x_test, t_train, t_test = train_test_split(x, t)\n",
    "    \n",
    "    all_data[i] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lls = [0.0, 1e-3, 10.0]\n",
    "\n",
    "all_predictions = np.empty([len(lls), N_REPEAT, len(x_)])\n",
    "for i, data in enumerate(all_data):\n",
    "    for j, ll in enumerate(lls):\n",
    "        reg = ridge(9, ll)\n",
    "        reg.fit(x, data)\n",
    "        all_predictions[j, i] = reg.predict(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_columns = 3\n",
    "N_ROWS = 2\n",
    "\n",
    "fig = plt.figure(figsize=(16, 4 * N_ROWS))\n",
    "axs = fig.subplots(ncols=n_columns, nrows=N_ROWS, gridspec_kw={'wspace': 0.3})\n",
    "\n",
    "# randomly choose simulations to draw\n",
    "ind = np.arange(all_predictions.shape[1])\n",
    "np.random.shuffle(ind)\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    # Plot ground truth\n",
    "    ax.plot(x_, ground_truth(x_), color='k', lw=3, label='Ground truth')\n",
    "    \n",
    "    # Plot data\n",
    "    ax.plot(x, all_data[ind[i]], 'ob', mfc=None, ms=10)\n",
    "    \n",
    "    ax.legend(loc=0)\n",
    "    \n",
    "    for j in range(len(lls)):\n",
    "        ax.plot(x_, all_predictions[j, ind[i]], alpha=0.6, \n",
    "                label='$\\lambda = {:.1e}$'.format(lls[j]))\n",
    "    \n",
    "    \n",
    "    #Label\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PLOT = 30\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "axs = fig.subplots(nrows=1, ncols=3, gridspec_kw={'wspace': 0.3})\n",
    "\n",
    "# randomly choose simulations to draw\n",
    "ind = np.arange(all_predictions.shape[1])\n",
    "np.random.shuffle(ind)\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.plot(x_, ground_truth(x_), color='k', lw=3, label='Ground truth')\n",
    "    ax.plot(x_, all_predictions[i, ind[:N_PLOT]].T, color='Red', alpha=0.15)\n",
    "    ax.legend(loc=0)\n",
    "    \n",
    "    #Label\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('t')\n",
    "    \n",
    "    # Title\n",
    "    axs[i].set_title('Reg. Param: {:.1e}'.format(lls[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cálculo completo de sesgo y varianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for finer grid of lambdas\n",
    "lls = np.logspace(-5, np.log10(5), 40)\n",
    "lls = np.concatenate([np.array([0,]), lls])\n",
    "\n",
    "all_predictions_full = np.empty([len(lls), N_REPEAT, len(x_)])\n",
    "for i, data in enumerate(all_data):\n",
    "    for j, ll in enumerate(lls):\n",
    "        reg = ridge(9, ll)\n",
    "        reg.fit(x, data)\n",
    "        all_predictions_full[j, i] = reg.predict(x_)\n",
    "        \n",
    "# All predictions has shape n_lambdas x n_repeats x len(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise added to measurements\n",
    "irreduc_err = 0.1\n",
    "\n",
    "# Compute mean prediction over data realisations\n",
    "Ed_y = np.average(all_predictions_full, axis=1)\n",
    "\n",
    "# Compute bias (difference beween mean prediction and truth)\n",
    "bias2 = (Ed_y - ground_truth(x_.flatten()))**2\n",
    "\n",
    "# Variance\n",
    "var = np.mean((Ed_y[:, np.newaxis, :] - all_predictions_full)**2, axis=1)\n",
    "\n",
    "# Total error\n",
    "err = bias2 + var + irreduc_err**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(var.shape, bias2.shape, err.shape, lls.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "# Plot in same plot, averaged over X\n",
    "ax.semilogx(lls, var.mean(axis=1), color='r', label='variance', lw=4)\n",
    "ax.semilogx(lls, bias2.mean(axis=1), color='b', label='bias$^2$', lw=4)\n",
    "ax.semilogx(lls, err.mean(axis=1), label='Expected error', lw=4, color='k')\n",
    "\n",
    "# Title and labels\n",
    "ax.legend(fontsize=16)\n",
    "ax.set_xlabel('Regularization parameter', size=16)\n",
    "ax.set_ylabel('Error', size=16)\n",
    "\n",
    "# ax.invert_xaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "210.594px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
