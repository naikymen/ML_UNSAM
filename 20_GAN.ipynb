{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/IAI-UNSAM/FDSML-JPMC/blob/main/21_GAN_MDN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nC18EFXdSqmd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#The stars\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "#tensorboard\n",
    "%load_ext tensorboard\n",
    "import datetime, os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "DuLqMAEqS5fa",
    "outputId": "3c8c579d-a814-4de4-8cb4-18d8dac7a0ef"
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_2uDMPde4NfF"
   },
   "outputs": [],
   "source": [
    "# A small detail\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aoNOblmHX1XY"
   },
   "source": [
    "## Data (Fashion_MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0PMm5jBHq_7a"
   },
   "source": [
    "Just as `sklearn` allow loading standard datasets, so does `keras`.\n",
    "\n",
    "In general, `keras` accepts datasets in the form of Numpy Arrays (like `sklearn`), but it also comes with a `Dataset` class that is optimized to load data (even if they are larger than the RAM memory of the computer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xiVqgSJhTeXp"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uIus71mjtqxf"
   },
   "source": [
    "As we know already, in large datasets CrossValidation can be very expensive, so it is recommended to leave apart a validation set from training.\n",
    "\n",
    "We also normalize the pixels (ranging from 0 to 255) to be between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7cB3KYyaUnzP"
   },
   "outputs": [],
   "source": [
    "# Separo en entrenamiento y validacion, y normalizo los pixeles\n",
    "# Los paso a 32 bits para que no haya problemas con TF.\n",
    "X_train_full = X_train_full.astype(np.float32) / 255\n",
    "X_test = X_test.astype(np.float32) / 255\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "colab_type": "code",
    "id": "I16qmEbDVnsa",
    "outputId": "4ce20104-5407-4c65-8fdf-ce98347985f4"
   },
   "outputs": [],
   "source": [
    "print(X_train_full.shape)\n",
    "\n",
    "plt.imshow(X_train[0], cmap='binary', interpolation='None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2nY8YQ_0uEPg"
   },
   "source": [
    "The target values are numeric, from 0 to 9. We can save the labels so it is easier for us to analyze how well or badly our model works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "colab_type": "code",
    "id": "xNkEq7xEU1YD",
    "outputId": "19d6b14f-354b-47ec-968c-8a0d14f67b5c"
   },
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "n_rows = 4\n",
    "n_cols = 10\n",
    "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        index = n_cols * row + col\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[y_train[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qs97Mem62Ks6"
   },
   "outputs": [],
   "source": [
    "# A useful function\n",
    "def plot_multiple_images(images, n_cols=None):\n",
    "    n_cols = n_cols or len(images)\n",
    "    n_rows = (len(images) - 1) // n_cols + 1\n",
    "    if images.shape[-1] == 1:\n",
    "        images = np.squeeze(images, axis=-1)\n",
    "    plt.figure(figsize=(n_cols, n_rows))\n",
    "    for index, image in enumerate(images):\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(image, cmap=\"binary\")\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P_staGAxY9yH"
   },
   "source": [
    "## Define a GAN\n",
    "\n",
    "The architecture is quite straight forward. We could even use the Sequential API from `keras` (**N.B.:** it is actually easier with the sequential API).\n",
    "\n",
    "First define some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding size\n",
    "codings_size = 30\n",
    "\n",
    "# Generator\n",
    "gen_unit_layers = [100, 150]\n",
    "\n",
    "# Discriminator\n",
    "dis_unit_layers = [150, 100]\n",
    "\n",
    "# Training\n",
    "discriminative_batches_per_generative_batch = 1\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Beu1NemvwCOU"
   },
   "outputs": [],
   "source": [
    "# Define generator network as a decoder of a variational encoder\n",
    "input_gen = keras.layers.Input(shape=(codings_size,), name='genin')\n",
    "\n",
    "# Dense layers (iterate using hyperparameters from previous cell)\n",
    "in_ = input_gen\n",
    "for i, u in enumerate(gen_unit_layers):\n",
    "    in_ = keras.layers.Dense(u, activation=\"selu\", name='gen{}'.format(i + 1))(in_)\n",
    "\n",
    "# We use sigmoid activation in the output layer to keep pixels in the range [0, 1]\n",
    "# Reshape back to original size\n",
    "flatgenout_ = keras.layers.Dense(28 * 28, activation=\"sigmoid\", name='genout_flat')(in_)\n",
    "out_gen = keras.layers.Reshape([28, 28], name='genout')(flatgenout_)\n",
    "\n",
    "# Build full model\n",
    "generator = keras.models.Model(input_gen, out_gen, name='Generative')\n",
    "\n",
    "# generator = keras.models.Sequential([\n",
    "#     keras.layers.Dense(100, activation=\"selu\", input_shape=[codings_size]),\n",
    "#     keras.layers.Dense(150, activation=\"selu\"),\n",
    "#     # We use sigmoid activation in the output layer to keep pixels in the range [0, 1]\n",
    "#     # Reshape back to original size\n",
    "#     keras.layers.Dense(28 * 28, activation=\"sigmoid\"),\n",
    "#     keras.layers.Reshape([28, 28])\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disciminator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The discriminator is a MLP for binary classification\n",
    "input_dis = keras.layers.Input(shape=(28, 28), name='disin')\n",
    "flatin_dis = keras.layers.Flatten(name='flat')(input_dis)\n",
    "\n",
    "in_ = flatin_dis\n",
    "for i, u in enumerate(dis_unit_layers):\n",
    "    in_ = keras.layers.Dense(u, activation=\"selu\", name='dis{}'.format(i + 1))(in_)\n",
    "\n",
    "out_dis = keras.layers.Dense(1, activation='sigmoid', name='disout')(in_)\n",
    "\n",
    "discriminator = keras.models.Model(input_dis, out_dis, name='Discriminative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use boh networks to from a GAN\n",
    "# (thanks, keras API!)\n",
    "\n",
    "# Build input layer\n",
    "gan_input = keras.layers.Input(shape=(codings_size,), name='ganin')\n",
    "# Generate new sample\n",
    "H = generator(gan_input)\n",
    "# Pass it to discriminator\n",
    "gan_output = discriminator(H)\n",
    "\n",
    "gan = keras.models.Model(gan_input, gan_output)\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2-Z3glcuxdut"
   },
   "source": [
    "Time to train the GAN. We have to propose a loss function for each of the two elements of the GAN.\n",
    "\n",
    "The discriminating network is doing binary classification (no more, no less), so it is natural to use binary entropy.\n",
    "\n",
    "On the other hand, the generating network is trained using both parts of the model, but without adjusting the weights of network D. Therefore, we have to do a little trick here, but is quite straightforward with `keras`.\n",
    "\n",
    "We are going to use an optimizer a little more appropriate for this type of network: `rmsprop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1WDp6SP2xufj"
   },
   "outputs": [],
   "source": [
    "# Compile discriminator\n",
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "# Define discriminator as non trainable (this will only be effective after compiling)\n",
    "discriminator.trainable = False\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "79qcKoH3y0H3"
   },
   "source": [
    "In this way, the discriminator can be trained independently, but when we train the GAN network, its weights will remain fixed.\n",
    "\n",
    "Now comes the tricky part, because we can't train the network with a simple call to the `fit` method. \n",
    "\n",
    "First we generate a data set, of size `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vRXZ3TE3yhE6"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(1000)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OlPx2VKDzgGu"
   },
   "source": [
    "And now we write a adversarial training routine (Rocky theme plays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_3N2WxXHzuY_"
   },
   "outputs": [],
   "source": [
    "# Codigo de Géron (https://github.com/ageron/handson-ml2)\n",
    "\n",
    "def train_gan(gan, dataset, batch_size, codings_size, n_epochs=50,\n",
    "              dbpgb=discriminative_batches_per_generative_batch, tensorboard=None):\n",
    "    generator, discriminator = gan.layers[1:]\n",
    "    \n",
    "    # Transform train_on_batch return value\n",
    "    # to dict expected by on_batch_end callback\n",
    "    def named_logs(model, logs):\n",
    "      result = {}\n",
    "      for l in zip(model.metrics_names, logs):\n",
    "        result[l[0]] = l[1]\n",
    "      return result\n",
    "    \n",
    "    # ADd model to Tensorboard\n",
    "    #if tensorboard is not None:\n",
    "    #    tensorboard.set_model(gan)\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        if tensorboard is not None:\n",
    "            tensorboard.set_model(gan)\n",
    "    \n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, n_epochs))              # not shown in the book\n",
    "        for i, X_batch in enumerate(dataset):\n",
    "            #\n",
    "            # Phase 1 - train the discriminator\n",
    "            #\n",
    "            \n",
    "            # Create noise (samples) drawn from an isotropic multinormal\n",
    "            # with same dimension as coding_size\n",
    "            # \n",
    "            # Genera ruido (muestras) salido de una multinormal isotrópica con \n",
    "            # tamaño igual al coding_size\n",
    "            codes = tf.random.normal(shape=[batch_size, codings_size])\n",
    "            \n",
    "            # Run generator to produce fake images\n",
    "            # Corre el generador para producir imágenes ficticias\n",
    "            generated_images = generator(codes)\n",
    "            \n",
    "            # Build dataset combining these images with real ones\n",
    "            # Genera un dataset combinando estas imágenes con imágenes reales\n",
    "            X_fake_and_real = tf.concat([generated_images, X_batch], axis=0)\n",
    "            \n",
    "            # Make labels to identify the real ones from the fake ones\n",
    "            # Genera las etiquetas que identifican como verdadero o falso\n",
    "            y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
    "            \n",
    "            # Esta línea es para evitar un warning, el discriminador ya es\n",
    "            # entrenable\n",
    "            # discriminator.trainable = True\n",
    "            \n",
    "            # Train the discriminator\n",
    "            # Entrena al discriminador\n",
    "            discriminator.train_on_batch(X_fake_and_real, y1)\n",
    "\n",
    "            # Every generative_batches_per_discriminative_batch, train also G\n",
    "            if (i+1) % dbpgb == 0:\n",
    "                #\n",
    "                # Phase 2 - train the generator\n",
    "                #\n",
    "                # More noise (they are actually samples in the latent space!)\n",
    "                # Más ruido (en realidad muestras en el espacio latente!)\n",
    "                codes = tf.random.normal(shape=[batch_size, codings_size])\n",
    "\n",
    "                # Labels (this time, true for all)\n",
    "                y2 = tf.constant([[1.]] * batch_size)\n",
    "\n",
    "                # Otra vez, para evitar un warning\n",
    "                # discriminator.trainable = False\n",
    "\n",
    "                # Train full model (generator + discriminator) with these labels\n",
    "\n",
    "                # Entrena todo el modelo: generador + discriminador, con esas \n",
    "                # etiquetas\n",
    "                logs = gan.train_on_batch(codes, y2)\n",
    "        \n",
    "                #\n",
    "                # Tensorboard\n",
    "                if tensorboard is not None:\n",
    "                    tensorboard.on_epoch_end(epoch, named_logs(gan, [logs,]))\n",
    "\n",
    "        tensorboard.on_train_end(None)\n",
    "\n",
    "        plot_multiple_images(generated_images, 8)                     # not shown\n",
    "        plt.show()                                                    # not shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YzyXvd9N1I-V"
   },
   "source": [
    "We are going to train it and see the images generated by the G network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard initialization\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard_callback.set_model(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "3iZyGKrz2eyH",
    "outputId": "29d5c8e3-bf78-472b-ebdd-5a4eb2e0d17c"
   },
   "outputs": [],
   "source": [
    "# train_gan(gan, dataset, batch_size, codings_size, n_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FznJay2d2hc-",
    "outputId": "21bf92f7-a6a9-46da-92a6-88a8cc047af5"
   },
   "outputs": [],
   "source": [
    "# Now we train for longer, and use TensorBoard to follow the evolution.\n",
    "train_gan(gan, dataset, batch_size, codings_size, n_epochs=10, tensorboard=tensorboard_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.save('models/gan_model1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F7DRdg0B9iMR"
   },
   "source": [
    "## Deep Convolutional GAN (DCGAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can do a similar (better) job using CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tgw1QcXX3Tp3"
   },
   "outputs": [],
   "source": [
    "# Codigo de Géron (https://github.com/ageron/handson-ml2)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "codings_size = 100\n",
    "\n",
    "generator = keras.models.Sequential([\n",
    "    keras.layers.Dense(7 * 7 * 128, input_shape=[codings_size]),\n",
    "    keras.layers.Reshape([7, 7, 128]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2, padding=\"SAME\",\n",
    "                                 activation=\"selu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Conv2DTranspose(1, kernel_size=5, strides=2, padding=\"SAME\",\n",
    "                                 activation=\"tanh\"),\n",
    "])\n",
    "discriminator = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(64, kernel_size=5, strides=2, padding=\"SAME\",\n",
    "                        activation=keras.layers.LeakyReLU(0.2),\n",
    "                        input_shape=[28, 28, 1]),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    keras.layers.Conv2D(128, kernel_size=5, strides=2, padding=\"SAME\",\n",
    "                        activation=keras.layers.LeakyReLU(0.2)),\n",
    "    keras.layers.Dropout(0.4),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "dcgan = keras.models.Sequential([generator, discriminator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQPP9E_09uR_"
   },
   "outputs": [],
   "source": [
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n",
    "discriminator.trainable = False\n",
    "dcgan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zl_-2FKY9uHJ"
   },
   "outputs": [],
   "source": [
    "X_train_dcgan = X_train.reshape(-1, 28, 28, 1) * 2. - 1. # reshape and rescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DWdAIjRF93d3"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X_train_dcgan)\n",
    "dataset = dataset.shuffle(1000)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "df-x1fGs95WZ",
    "outputId": "1c0bc4e1-0f80-4dc0-ec47-e4876e6aa31c"
   },
   "outputs": [],
   "source": [
    "train_gan(dcgan, dataset, batch_size, codings_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YPYEf_IL-gVQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's discuss!\n",
    "\n",
    "1. Come up with at least a couple of ways you could use GANs for you work or for the company.\n",
    "2. Compare and contrast this generative method with another generative models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": "true",
    "id": "CRu_u7pQMqli",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Mixture Density Networks (MDN)\n",
    "<a id='mixtures'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple regression problem..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando estudiamos regresión lineal, vimos que el error cuadrático aparecía naturalmente al suponer que la distribución condicional de los labels era normal:\n",
    "\n",
    "$$\n",
    "p(t|x, \\mathbf{w}, \\beta) = \\mathcal{N}(t|y(x,\\mathbf{w}), \\beta^{-1})\\;\\;.\n",
    "$$\n",
    "\n",
    "En ese caso, teníamos que el logaritmo de la versosimilitud de una serie de mediciones, $\\mathbf{X} = \\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_N\\}$, con labels / outputs, $\\mathbf{t} = \\{t_1, \\ldots, t_N\\}$ era:\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\ln p(\\mathbf{t}|\\mathbf{w}, \\beta) &=& \\sum_{n=1}^N \\ln\\mathcal{N}(t_n|y(\\mathbf{x},\\mathbf{w}), \\beta^{-1})\\\\\n",
    "&=& \\frac{N}{2}\\ln\\beta - \\frac{N}{2}\\ln(2\\pi) - \\beta E_D(\\mathbf{w})\\;\\;,\n",
    "\\end{array}\n",
    "$$\n",
    "donde \n",
    "$$\n",
    "E_D(\\mathbf{w}) = \\frac{1}{2}\\sum_{n=1}^N\\left\\{t_n - \\mathbf{w}^T\\phi(\\mathbf{x}_n)\\right\\}^2 \\;\\;.\n",
    "$$\n",
    "\n",
    "En el marco de los modelos de redes neuronales, las funciones de base $\\phi$ se parametrizan para darle mayor flexibilidad al modelo, como estuvimos viendo estas semanas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n3o3qJ1AMqli"
   },
   "source": [
    "Muy bien, veamos entonces qué pueden hacer las redes neuronales frente a un problema sencillo de regresión, con un output y un input. Seguimos de cerca a Bishop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "8K23pR8vMqlj",
    "outputId": "72ceade3-d5ef-4af7-dede-1ddafa094991"
   },
   "outputs": [],
   "source": [
    "# Fix seed\n",
    "np.random.seed(20200616)\n",
    "\n",
    "# Create dataset\n",
    "x = np.random.rand(600).reshape(-1, 1)\n",
    "t = 1.0 * x + 0.3 * np.sin(2 * np.pi * x)\n",
    "\n",
    "noise = (np.random.rand(x.shape[0]) * 0.2 - 0.1).reshape(-1, 1)\n",
    "\n",
    "t += noise\n",
    "# Separemos en train, test, y validation\n",
    "X_train, X_validation, X_test = x[:500], x[500: 550], x[550:]\n",
    "t_train, t_validation, t_test = t[:500], t[500: 550], t[550:]\n",
    "\n",
    "# Veamos los datos\n",
    "plt.plot(X_train, t_train, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "colab_type": "code",
    "id": "y8OXSm6uMqlm",
    "outputId": "56106e1d-4060-4f74-d12c-85554fd7a971"
   },
   "outputs": [],
   "source": [
    "#\n",
    "from tensorflow import keras\n",
    "\n",
    "# Build simple DNN\n",
    "modelo = keras.Sequential()\n",
    "\n",
    "# Input layer\n",
    "modelo.add(keras.layers.InputLayer(input_shape=X_train.shape[1:]))\n",
    "\n",
    "# Agreguemos dos capas ocultas de 10 y 8 neuronas cada una\n",
    "modelo.add(keras.layers.Dense(10, activation='sigmoid', name='Oculta1'))\n",
    "modelo.add(keras.layers.Dense(8, activation='sigmoid', name='Oculta2'))\n",
    "\n",
    "# Agregemos una capa de output que sirva para este problema\n",
    "modelo.add(keras.layers.Dense(1, activation=None))\n",
    "\n",
    "# keras.utils.plot_model(modelo, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "r7L4xHGlWiEk",
    "outputId": "881e6e56-4fd2-4029-fce4-0514cacf3bbd"
   },
   "outputs": [],
   "source": [
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AzVIJTNEMqlp"
   },
   "outputs": [],
   "source": [
    "# Compilemos el modelo. \n",
    "# En este punto se elije la función de périda y el optimizador.\n",
    "# ¿Qué función de pérdida deberíamos usar?\n",
    "modelo.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R3KzEoIQMqls"
   },
   "outputs": [],
   "source": [
    "# Es hora de entrenar el modelo. Corramos 500 épocas, y vayamos mirando también\n",
    "# la validación.\n",
    "\n",
    "# Usemos EarlyStopping para no pasar tiempo de más entrenando\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Set up early stopping\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0.0, patience=50, verbose=0, \n",
    "                      mode='auto')\n",
    "\n",
    "history = modelo.fit(X_train, t_train, epochs=200, \n",
    "                     validation_data=(X_validation, t_validation),)\n",
    "                     #callbacks=[early,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "EOgz6qVdMqlv",
    "outputId": "96f71dff-3e99-4616-e753-c40ee3e1f851"
   },
   "outputs": [],
   "source": [
    "# Veamos el resultado del ajuste\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(history.history)\n",
    "df.plot()\n",
    "ax = plt.gca()\n",
    "# ax.set_ylim(0.01, 0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "pXMDx8w5Mqlz",
    "outputId": "ed29f4b9-4aa6-4dd5-f40f-ff6ce5fabf5c"
   },
   "outputs": [],
   "source": [
    "# Veamos los datos\n",
    "plt.plot(X_train, t_train, '.')\n",
    "\n",
    "# Graficamos la predicción del modelo \n",
    "xx = np.linspace(0, 1, 1000)\n",
    "plt.plot(xx, modelo.predict(xx), '-r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "0yTPz3RmXKcu",
    "outputId": "5cea3a12-bb87-4a89-cfb2-41b403dab579"
   },
   "outputs": [],
   "source": [
    "modelo.layers[-1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo.save('univariate_regression.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xh9PtNSKMql3"
   },
   "source": [
    "Vemos que el modelo funciona bien (hay que prestar atención al learning rate, nomás)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... that suddenly becomes interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veamos qué pasa si invertimos el rol de las variables, de manera que los datos que queremos ajustar son diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "GDjnNDglMql4",
    "outputId": "397c449c-06ac-4617-a63e-f7fb0428acae"
   },
   "outputs": [],
   "source": [
    "# Veamos los datos\n",
    "plt.plot(t_train, X_train, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fVRkb5amMql6"
   },
   "outputs": [],
   "source": [
    "# Escribamos un modelo similar al anterior (¿podemos usar el mismo modelo?)\n",
    "# Build simple DNN\n",
    "modelo2 = keras.Sequential()\n",
    "\n",
    "# Input layer\n",
    "modelo2.add(keras.layers.InputLayer(input_shape=t_train.shape[1:]))\n",
    "\n",
    "# Agreguemos dos capas ocultas de 10 y 8 neuronas cada una\n",
    "modelo2.add(keras.layers.Dense(10, activation='sigmoid', name='Oculta1_2'))\n",
    "modelo2.add(keras.layers.Dense(8, activation='sigmoid', name='Oculta2_2'))\n",
    "\n",
    "# Agregemos una capa de output que sirva para este problema\n",
    "modelo2.add(keras.layers.Dense(1, activation=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "colab_type": "code",
    "id": "TEag19y3Mql8",
    "outputId": "10afb53b-b45e-425b-e579-4560824e8d7f"
   },
   "outputs": [],
   "source": [
    "# Compilemos, ajustemos, miremos el resultado y grafiquemos la predicción.\n",
    "modelo2.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(lr=0.1))\n",
    "\n",
    "# keras.utils.plot_model(modelo2, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "At36YiYFZmbs"
   },
   "outputs": [],
   "source": [
    "# OJO! OJO! En el validation_data, tiene que ir una tupla (no una lista!); \n",
    "# es decir, paréntesis y no corchetes\n",
    "history2 = modelo2.fit(t_train, X_train, epochs=500, \n",
    "                       validation_data=(t_validation, X_validation))\n",
    "\n",
    "# También se puede pasar una fracción del conjunto de entrenamiento para usar\n",
    "# como validación.\n",
    "# modelo2.fit(t_train, X_train, epochs=50, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "6uRPBJVjeD5w",
    "outputId": "1ac3b7c7-2742-41d1-e1a6-04fd09538f36"
   },
   "outputs": [],
   "source": [
    "plt.plot(t_train, X_train, '.')\n",
    "\n",
    "plt.plot(xx, modelo2.predict(xx), 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yD7vjuW5MqmH"
   },
   "source": [
    "Por más que intentemos, la red no va a poder ajustar los datos. \n",
    "\n",
    "El uso del error cuadrático implica, como vimos arriba que la distribución condicionada de las variables target $p(t | x)$ es una normal. Sin embargo, para valores como $x=0.6$ la distribución es bien multimodal, y por lo tanto no puede ser representada por una normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "colab_type": "code",
    "id": "H_KD_bAkMqmI",
    "outputId": "eb7f2c2f-7d9f-4add-8d78-66fd3a3b1435"
   },
   "outputs": [],
   "source": [
    "# Veamos un entorno de x = 0.6\n",
    "epsilon = 0.05\n",
    "cond = np.abs(t_train - 0.6) < epsilon\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "ax = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax.plot(t_train, X_train, '.k')\n",
    "ax.axvline(0.6, color='r', ls='-')\n",
    "for sign in (-1, 1):\n",
    "    ax.axvline(0.9 + sign * epsilon, color='r', ls=':')\n",
    "ax2.hist(X_train[cond], 25)\n",
    "\n",
    "ax.set_xlabel('nuevo X')\n",
    "ax.set_ylabel('nuevo T')\n",
    "ax2.set_xlabel('nuevo X')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wOofYLeKMqmK"
   },
   "source": [
    "We clearly cannot fit this using a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Model presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "OKbTKTlBMqmK"
   },
   "source": [
    "Este ejemplo muestra una de las limitaciones de un modelo normal: solo puede captar distribuciones con un único modo. Una forma de sobreponerse a este problema es usar un modelo que consista en una superposición de $K$ distribuciones normales.\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}) = \\sum_{k=1}^K \\pi_k{\\mathcal{N}(\\mathbf{x} \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}\\;\\;,\n",
    "$$\n",
    "donde cada densidad $\\mathcal{N}(\\mathbf{x} \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$ se conoce como una *componente* de la mixtura, y tiene su propio valor medio y matriz de covarianza. Los $\\pi_k$ son los *coeficientes* de mezcla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "HOjgEu2PMqmL"
   },
   "source": [
    "__Los coeficientes como probabilidades__\n",
    "\n",
    "Integrando a ambos lados de la igualdad sobre todos los valore de $\\mathbf{x}$ y viendo que tanto $p(\\mathbf{x})$ como las componentes normales son funciones de distribución de probabilidad, llegamos a que\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^K \\pi_k = 1\\;\\;.\n",
    "$$\n",
    "\n",
    "Además, como $p(\\mathbf{x})$ tienen que ser positiva, encontramos que una forma de garantizar eso de manera general es definir que los coeficientes sean positivos. Por lo tanto, $0 \\leq \\pi_k \\leq 1$, y los coeficientes satisfacen los requerimientos para ser probabilidades.\n",
    "\n",
    "Podemos, entonces, asignar una función de distribución de masa, $p(k)$, que describe la probabilidad (prior) de que se elija la compoenente $k$, y $p(k) = \\pi_k$. Entonces, distribución sobre $\\mathbf{x}$ queda:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}) = \\sum_{k=1}^K p(\\mathbf{x}, k) = \\sum_{k=1}^K p(k) p(\\mathbf{x} | k)\\;\\;,\n",
    "$$\n",
    "\n",
    "donde la probabilidad condicionada $p(\\mathbf{x} | k) = \\mathcal{N}(\\mathbf{x} \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "hidden": true,
    "id": "z-H2CjfRMqmL",
    "outputId": "b7780165-85cb-4603-c471-cd29d3f005db"
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "# Supongamos una distribución unidimensional de dos modos, con los siguientes parámetros\n",
    "mu1 = [0.0,]\n",
    "sigma1 = [2.0,]\n",
    "mu2 = [5.0,]\n",
    "sigma2 = [1.5,]\n",
    "\n",
    "# Generemos las dos distris\n",
    "n1 = st.norm(mu1, sigma1)\n",
    "n2 = st.norm(mu2, sigma2)\n",
    "\n",
    "# Ahora pensemos que tenemos una distribución prior p(k) con las siguientes elementos\n",
    "p1 = 0.65\n",
    "p2 = 0.35\n",
    "\n",
    "# Veamos como podemos generar, fácilmente, muestras de la mixtura:\n",
    "# p1 * N1 + p2 * N2\n",
    "# Primero muestreo p(k)\n",
    "N = 5000\n",
    "k = np.where(np.random.rand(N) < p1, 1, 2)\n",
    "\n",
    "# Y ahora lo combino con las mixturas\n",
    "\n",
    "# Modo 1 (estricto)\n",
    "# x1 = n1.rvs(sum(k==1), 1)\n",
    "# x2 = n2.rvs(sum(k==2), 1)\n",
    "\n",
    "# x = np.hstack([x1, x2])\n",
    "\n",
    "# Modo 2 (generando de más)\n",
    "x1 = n1.rvs(N, 1)\n",
    "x2 = n2.rvs(N, 1)\n",
    "\n",
    "x = np.where(k==1, x1, x2).reshape((-1, 1))\n",
    "\n",
    "h = plt.hist(x, 50, histtype='step', density=True)\n",
    "\n",
    "# Combinemos esto con la pdf del modelo\n",
    "xx = np.linspace(x.min(), x.max(), 400)\n",
    "plt.plot(xx, p1 * n1.pdf(xx) + p2 * n2.pdf(xx), 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "qOtk3DTyMqmN"
   },
   "source": [
    "__Responsabilidades__\n",
    "\n",
    "Un rol crucial lo van a jugar las posteriores de la probabilidad de cada modo $p(k \\:|\\: \\mathbf{x})$, que se conocen como *responsabilidades* (¡cuánta seriedad!)\n",
    "\n",
    "$$\n",
    "\\gamma_k(\\mathbf{x}) \\equiv p(k \\:|\\: \\mathbf{x})\\;\\;.\n",
    "$$\n",
    "\n",
    "Como es de esperar, podemos calcular la punta que tienen las responsabilidades a partir del teorema de Bayes:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\gamma_k(\\mathbf{x}) &=& \\frac{p(k) p(\\mathbf{x}\\:|\\:k)}{\\sum_i p(i) p(\\mathbf{x}\\:|\\:i)}\\\\\n",
    "   &=&\\frac{\\pi_k{\\mathcal{N}(\\mathbf{x} \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}}{\\sum_i{\\pi_i{\\mathcal{N}(\\mathbf{x} \\:|\\: \\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i)}}}\\;\\;,\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "donde usamos la definición de arriba\n",
    "$$\n",
    "p(\\mathbf{x}) = \\sum_{k=1}^K \\pi_k{\\mathcal{N}(\\mathbf{x} \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}\\;\\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "r7a3vUQiMqmO"
   },
   "source": [
    "__Verosimilitud__ (o Donde arrancan los líos)\n",
    "\n",
    "Naturalmente, necesitaremos poder escribir la verosimilitud para un modelo de este tipo. Como vimos arriba, el problema con las exponenciales de la normal, es que muchas veces generan problemas numéricos.\n",
    "\n",
    "Vamos entonces a escribir el logaritmo de la verosimilitud, como ya hicimos un montón de veces.\n",
    "\n",
    "$$\n",
    "\\ln p(\\mathbf{X} \\:|\\: \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi}) = \\sum_{n=1}^N \\ln \\left\\{\\sum_{k=1}^K \\pi_k{\\mathcal{N}(\\mathbf{x}_n \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}\\right\\}\\;\\;,\n",
    "$$\n",
    "donde estamos usando esta notación: $\\boldsymbol{\\mu} \\equiv \\left\\{\\boldsymbol{\\mu}_1, \\ldots, \\boldsymbol{\\mu}_K\\right\\}$, y lo mismo para $\\boldsymbol{\\Sigma}$ y para $\\boldsymbol{\\pi}$. Y como siempre $\\mathbf{X} = \\left\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_N\\right\\}$.\n",
    "\n",
    "Vemos que el logaritmo actúa sobre la sumatoria y no sobre cada normal de manera individual. Esto hace que sea complicado obtener una forma cerrada para los parámetros que maximizan la verosimilitud. Tenemos que usar un procedimiento iterativo, conocido como el algoritmo de maximización de la expectación, o *expectation maximisation* (EM).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "BmU4U_KiMqmO"
   },
   "source": [
    "__Un paso más__\n",
    "\n",
    "En el contexto de las redes neuronales, vamos a dar un paso más y permitir a los parámetros de cada componente que cambien con la variable del input, $\\mathbf{x}$.\n",
    "\n",
    "Así, tanto los coeficientes de mezcla, como los vectores $\\boldsymbol{\\mu}$, como la varianza de cada normal (que consideraremos isotrópica para cada $\\mathbf{x}$, se convierten en funciones del input.\n",
    "\n",
    "El modelo pasa de ser una normal con covarianza general:\n",
    "\n",
    "$$\n",
    "p(t|\\mathbf{x}) = \\mathcal{N}(t|y(\\mathbf{x},\\mathbf{w}), \\beta^{-1})\\;\\;.\n",
    "$$\n",
    "\n",
    "a ser una mixtura de normales isotrópicas (si bien la extensión a covarianzas generales es posible):\n",
    "\n",
    "$$\n",
    "p(t | \\mathbf{x}) = \\sum_{k=1}^K \\pi_k(\\mathbf{x}){\\mathcal{N}(t \\:|\\: \\boldsymbol{\\mu}_k(\\mathbf{x}), \\sigma^2_k(\\mathbf{x}))}\\;\\;.\n",
    "$$\n",
    "\n",
    "Esta formulación es un ejemplo de un modelo heterocedastico (wow!); es decir, que la varianza cambia con la variable $x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tR7kjDWnMqmP"
   },
   "source": [
    "Todo esto es muy lindo, pero ¿cómo se implementa en un modelo de redes neuronales?\n",
    "\n",
    "La idea es generar una red cuyos outputs sean las parámetros del modelo. Para eso, tenemos que usar la API funcional de `Keras`\n",
    "\n",
    "**Nota**: Nos inspiramos en el código de https://github.com/oborchers/Medium_Repo\n",
    "\n",
    "[](https://drive.google.com/uc?export=view&id=1-a9oAqcGYPgBoSkg9yTXfzxsu5MBjdA5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/mixture_density_model.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lfkG2zljMqmQ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "\n",
    "neurons = 500     # Neurons of the DNN hidden layers\n",
    "components = 3    # Number of components in the mixture\n",
    "no_parameters = 3 # Paramters of the mixture (pi, mu, sigma)\n",
    "\n",
    "input_ = Input(shape=(t_train.shape[1],))\n",
    "\n",
    "# Escribir una o dos capas ocultas\n",
    "h1 = Dense(neurons, activation='relu')(input_)\n",
    "h2 = Dense(neurons, activation='relu')(h1)\n",
    "\n",
    "# Ahora escribir una capa para cada una de las variables del problema\n",
    "# Coeficientes de mezcla (que tienen que sumar 1)\n",
    "# Valores medios\n",
    "# Anchos de la distribución (que tiene que ser positivos)\n",
    "mixcoeff = Dense(components, activation='softmax', name='pi')(h2) \n",
    "means = Dense(components, activation=None, name='mu')(h2)\n",
    "sigmas = Dense(components, activation='exponential', name='sigma')(h2)\n",
    "\n",
    "# Concatenemos las tres capas para dar la capa de salida\n",
    "pvector = Concatenate(name=\"output\")([mixcoeff, means, sigmas])\n",
    "\n",
    "# Y finalmente, generemos el modelo de Keras con todo esto\n",
    "modelMDN = keras.Model(inputs=[input_], outputs=[pvector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xazyWLQXMqmT"
   },
   "outputs": [],
   "source": [
    "# Si quisieramos, podemos hacer un subclase de los modelos de Keras\n",
    "class MDN(keras.Model):\n",
    "\n",
    "    def __init__(self, neurons=100, components = 2):\n",
    "        super(MDN, self).__init__(name=\"MDN\")\n",
    "        self.neurons = neurons\n",
    "        self.components = components\n",
    "        \n",
    "        self.h1 = Dense(neurons, activation=\"relu\", name=\"h1\")\n",
    "        self.h2 = Dense(neurons, activation=\"relu\", name=\"h2\")\n",
    "        \n",
    "        self.mixcoeff = Dense(components, activation=\"softmax\", name=\"mix\")\n",
    "        self.means = Dense(components, name=\"means\")\n",
    "        self.sigmas = Dense(components, activation=\"exponential\", name=\"sigmas\")\n",
    "        self.pvec = Concatenate(name=\"pvec\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.h1(inputs)\n",
    "        x = self.h2(x)\n",
    "        \n",
    "        mix_v = self.mixcoeff(x)\n",
    "        mean_v = self.means(x)\n",
    "        sigma_v = self.sigmas(x)\n",
    "        \n",
    "        return self.pvec([alpha_v, mu_v, sigma_v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qBvqs-Gw3m7V"
   },
   "source": [
    "¡Bien! Ya tenemos el modelo. Pero eso es solo una parte del problema.\n",
    "\n",
    "¿Qué más falta?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yE78wKwTMqmW"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_probability import distributions as tfd\n",
    "\n",
    "def slice_parameter_vectors(parameter_vector):\n",
    "    \"\"\" Returns an unpacked list of paramter vectors.\n",
    "    \"\"\"\n",
    "    return [parameter_vector[:,i*components:(i+1)*components] for \n",
    "            i in range(no_parameters)]\n",
    "\n",
    "def gnll_loss(t, parameter_vector):\n",
    "    \"\"\" Computes the mean negative log-likelihood loss of y given the mixture parameters.\n",
    "    \"\"\"\n",
    "    pi, mu, sigma = slice_parameter_vectors(parameter_vector)  # Unpack parameter vectors\n",
    "    \n",
    "    gm = tfd.MixtureSameFamily(\n",
    "        mixture_distribution=tfd.Categorical(probs=pi),\n",
    "        components_distribution=tfd.Normal(\n",
    "            loc=mu,       \n",
    "            scale=sigma))\n",
    "    \n",
    "    log_likelihood = gm.log_prob(tf.transpose(t))                 # Evaluate log-probability of y\n",
    "    \n",
    "    return -tf.reduce_mean(log_likelihood, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZoV-V-hRMqmY"
   },
   "source": [
    "### Compilation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "8RwcyA3VMqmY",
    "outputId": "095121a1-d579-4ad6-f750-15ef2a6f2b2a"
   },
   "outputs": [],
   "source": [
    "modelMDN.compile(loss=gnll_loss, optimizer=keras.optimizers.Adam(lr=0.002))\n",
    "modelMDN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "colab_type": "code",
    "id": "CVd18HKRMqma",
    "outputId": "04315029-2199-4289-9e36-a3d19c553f57"
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(modelMDN, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2IlHlGMPa0rX"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Set up early stopping\n",
    "mon = EarlyStopping(monitor='val_loss', min_delta=0.0, patience=50, verbose=0, \n",
    "                    mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iuqTmehXMqmc"
   },
   "outputs": [],
   "source": [
    "# Ajustemos el modelo. Volvemos a usar early stopping\n",
    "\n",
    "historyNEW = modelMDN.fit(t_train, X_train, epochs=1000, \n",
    "                          validation_data=(t_validation, X_validation), \n",
    "                          verbose=False, batch_size=16, callbacks=[mon,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "cA8mVU2GPAwr",
    "outputId": "cd51802e-e5cc-4046-f931-a746a37d1973"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(historyNEW.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "WZOzOOs0Mqme",
    "outputId": "be907542-0e23-41e8-d24b-8d8cd2dac512"
   },
   "outputs": [],
   "source": [
    "# Veamos los parámetros del resultado\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "ax = fig.add_subplot(141)\n",
    "ax2 = fig.add_subplot(142)\n",
    "ax3 = fig.add_subplot(143)\n",
    "ax4 = fig.add_subplot(144)\n",
    "\n",
    "# En el primer subplot ponemos los datos.\n",
    "ax.plot(t_train, X_train, '.k')\n",
    "ax.set_xlabel('nuevo X')\n",
    "ax.set_ylabel('nuevo T')\n",
    "\n",
    "# Ahora calculamos la predicción.\n",
    "# En este caso, no tenemos una predicción directa de los datos,\n",
    "# sino los parámetros de la mixtura\n",
    "xx = np.arange(0, 1.001, 0.001)\n",
    "pars = modelMDN.predict(xx)\n",
    "pi, mu, sigma = slice_parameter_vectors(pars)\n",
    "\n",
    "# Podemos calcular el valor medio de la función y ponerla con los datos\n",
    "# El valor medio es la suma de pi * mu sobre las componentes del modelo\n",
    "tmean = np.sum(pi * mu, axis=1)\n",
    "ax.plot(xx, tmean, ',')\n",
    "# ¿Sorprende el resultado?\n",
    "\n",
    "ax2.plot(xx, pi)\n",
    "ax2.set_title('Mixing coefficient')\n",
    "ax3.plot(xx, mu)\n",
    "ax3.set_title('Valor medio')\n",
    "ax4.plot(xx, sigma)\n",
    "ax4.set_title('Varianza')\n",
    "\n",
    "for a in [ax2, ax3, ax4]:\n",
    "  a.set_xlabel('nuevo X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S_XhneRFMqmg"
   },
   "outputs": [],
   "source": [
    "# Ahora la posta. \n",
    "# Calculemos la pdf de la distribución condicional, para una serie de valores\n",
    "# de X.\n",
    "import scipy.stats as st\n",
    "\n",
    "# Primero construyo una grilla de valores\n",
    "# uso xx para ambos en este caso porque los valores están todos en [0, 1].\n",
    "x, y = np.meshgrid(xx, xx)\n",
    "\n",
    "# Calcula distribución para cada componente\n",
    "a = np.array([pi[:, i] * st.norm.pdf(y, loc=mu[:, i], scale=sigma[:, i]) for \n",
    "              i in range(pi.shape[1])])\n",
    "\n",
    "# Sumo sobre todas las componentes\n",
    "pdf = np.sum(a, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "sjBGIfWJYoII",
    "outputId": "2fbfe5b8-1e80-4660-ae7b-11e24e831e2f"
   },
   "outputs": [],
   "source": [
    "# La hora de la verdad.\n",
    "# Graficamos el contorno de la distribución de mixtura para distintos \n",
    "# valores de x. Es decir, p(t | x) con x que va variando.\n",
    "\n",
    "xont = plt.contourf(x, y, pdf, 30, cmap='jet')\n",
    "xont = plt.contour(x, y, pdf, 15, colors=['w',], alpha=0.7)\n",
    "\n",
    "# Y agregamos los datos\n",
    "plt.plot(t_train, X_train, '.w', mfc='None', alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "6mJMzOqNotzn",
    "outputId": "0fd76ed3-4a7d-4495-aaeb-2782263c9ef8"
   },
   "outputs": [],
   "source": [
    "# También podemos mostrar el contorno de cada componente gaussiana.\n",
    "# xont = plt.contour(x, y, pdf, 15, colors=['m',], alpha=0.7, lw=5)\n",
    "for k in range(a.shape[0]):\n",
    "  plt.contour(x, y, a[k], 15, colors=['C{}'.format(k)])\n",
    "\n",
    "# plt.colorbar(xont)\n",
    "# plt.plot(t_train, X_train, '.w', mfc='None', alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "APaNqed6YxrF"
   },
   "outputs": [],
   "source": [
    "# Quedó bueno. Salvémoslo.\n",
    "modelMDN.save('MDN_ok.h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "0zpDNMx45U_y",
    "outputId": "6bcb640d-f682-435e-ab3e-c95998dc5061"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Usemos el resultado de arriba para plotear la función condicionada para algunos\n",
    "# valores de x. Es decir, hacemos cortes verticales para valores de x fijos.\n",
    "xs = [0.1, 0.4, 0.7]\n",
    "\n",
    "fig = plt.figure(figsize=(9,6))\n",
    "\n",
    "for i, xi in enumerate(xs):\n",
    "  # Nos quedamos con el índice correspondiente al valor de x elegido\n",
    "  j = np.argmin(np.abs(xx - xi))\n",
    "  ax = fig.add_subplot(1, len(xs), i+1)\n",
    "  \n",
    "  # Plot de la distribución condicional p(t | x=xi)\n",
    "  ax.plot(xx, pdf[:, j].ravel(), '-k', lw=2)\n",
    "  \n",
    "  # Aporte de cada componente\n",
    "  for k in range(a.shape[0]):\n",
    "    ax.plot(xx, a[k, :, j], alpha=0.6, lw=5)\n",
    "  \n",
    "  ax.set_title('p(t | X={}, wML)'.format(xi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wan2y0Wv6NZc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "21_GAN_MDN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
