{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "violent-stone",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-gospel",
   "metadata": {},
   "source": [
    "Working in high-dimensional spaces is many times unconvenient, and one might want to reduce the dimensionality in order to:\n",
    "* Make visualizations\n",
    "* Save storage space of the data (compression)\n",
    "* Reduce computational costs of training algorithms, by reducing number of features.\n",
    "* Increase a models performance by increasing the signal/noise ratio, as well as the density of points (*curse of dimensionality*).\n",
    "\n",
    "The problem of reducing the dimensionality of features, while retaining most of the information, is called *dimensionality reduction* and there are two main approaches to it:\n",
    "* Projection: Project the high-dimensional space into a hyper-plane, collapsing all the features orthogonal to it.\n",
    "* Manifold-Learning: Learning the geometry of a lower-dimensional manifold, and projecting the data onto it.\n",
    "\n",
    "On the first category, the most known algorithm is Principal Component Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-diamond",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-solid",
   "metadata": {},
   "source": [
    "The aim of Principal Component Analysis is to find the direction in which the data has the most variance. This is the first Principal Component. Then it looks for the second Principal Component as the direction which is orthogonal to the first one and which has the most variance, and so on.  \n",
    "\n",
    "This algorithm corresponds to fitting an ellipsoid to the data, and then center and rotate it so that the axis align with the ellipsoid's principal axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-officer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "angle = np.pi / 5\n",
    "stretch = 5\n",
    "m = 200\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "# Create independent random Gaussian data\n",
    "X = np.random.randn(m, 2) \n",
    "\n",
    "# Stretch matrix\n",
    "S = np.array([[stretch, 0],[0, 1]])\n",
    "\n",
    "# Matrix multiplication\n",
    "X = X @ S\n",
    "\n",
    "\n",
    "# Rotation matrix\n",
    "R = np.array([[np.cos(angle), np.sin(angle)], [-np.sin(angle), np.cos(angle)]])\n",
    "\n",
    "X = X @ R\n",
    "\n",
    "# Plot\n",
    "plt.scatter(*X.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-anxiety",
   "metadata": {},
   "source": [
    "We can project the data in either of the two axis, but looking at the data it might make sense to project on the diagonal directions. Lets see how it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_1 = X[:,0]+X[:,1]\n",
    "variant_2 = X[:,0]-X[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(variant_1, label='wide')\n",
    "plt.hist(variant_2, label='short')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-arctic",
   "metadata": {},
   "source": [
    "We see that there is one direction in which the data is much more spread than the other one, and probably preserves more information. This would be a good option to project it to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-status",
   "metadata": {},
   "source": [
    "The information regarding how is the data spread, can be read from the covariance matrix.\n",
    "\n",
    "Let's take a look at the covariance matrix for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-breast",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.cov(X.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-thinking",
   "metadata": {},
   "source": [
    "The main task of the PCA is to find a suitable rotation such that the covariance matrix looks diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-china",
   "metadata": {},
   "source": [
    "The axis chosen to project are stored in the `components_` attribute, row-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce7acba-e6b8-4acf-adb7-981760b7efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-tuner",
   "metadata": {},
   "outputs": [],
   "source": [
    "size=7\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.scatter(*X.T)\n",
    "\n",
    "ax.arrow(0, 0, pca.components_[0,0]*size, pca.components_[0,1]*size, color='r', width=0.1, head_width=0.5, alpha=0.8,\n",
    "        label='First PC')\n",
    "ax.arrow(0, 0, pca.components_[1,0]*size, pca.components_[1,1]*size, color='g', width=0.1, head_width=0.5, alpha=0.8,\n",
    "        label='Second PC')\n",
    "ax.legend()\n",
    "\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-grant",
   "metadata": {},
   "source": [
    "If we look at the covariance matrix of the transformed coordinates, this is what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-offering",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.cov(X_pca.T).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-century",
   "metadata": {},
   "source": [
    "Note two things:\n",
    "1. the covariance matrix is now diagonal.\n",
    "2. that most of the variance is located in the first coordinate. (N.B.: PCA automatically orders the features by its variance).\n",
    "\n",
    "If we look at the plot of the transformed dataset, now  its an ellipsoid aligned with the new feature's axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*X_pca.T)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-grove",
   "metadata": {},
   "source": [
    "We can inverse-transform our data, and see that it corresponds to the same as the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = pca.inverse_transform(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(X, X_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-running",
   "metadata": {},
   "source": [
    "Now, what would happen if we remove the information from the last feature? \n",
    "Keeping just the feature which carries the most information (variance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1d = X_pca.copy()\n",
    "X_1d[:,1] = 0\n",
    "X_1d[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-danger",
   "metadata": {},
   "source": [
    "If we do the inverse transform, some information was lost, and so the inverted data is not the same as the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-necklace",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = pca.inverse_transform(X_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(X, X_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-valve",
   "metadata": {},
   "source": [
    "Let's take a look at this compressed data, and how it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-letter",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*X.T, label='original')\n",
    "plt.scatter(*X_2.T, label='compressed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-place",
   "metadata": {},
   "source": [
    "What we did when we threw away the second principal component, is to colapse all the data to the direction which has the most variance. \n",
    "\n",
    "If we look at the covariance matrix, we see that not much was lost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-cookbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original covariance matrix\\n', np.cov(X.T))\n",
    "print('\\nCompressed covariance matrix\\n', np.cov(X_2.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-drunk",
   "metadata": {},
   "source": [
    "The difference in the trace of the covariance matrices gives us a hint of how much information was lost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trace(np.cov(X.T))-np.trace(np.cov(X_2.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-reasoning",
   "metadata": {},
   "source": [
    "This information is stored in the `explained variance`, which is nothing but the diagonal elements of the covariance matrix in the transformed space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(X_pca.T).round(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-trouble",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-occasions",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(pca.explained_variance_[1], np.trace(np.cov(X.T))-np.trace(np.cov(X_2.T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-asthma",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PCA for Dimensional Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-loading",
   "metadata": {},
   "source": [
    "In order to reduce dimensionality, we can chose to keep the first `n` principal components of our data. This way, we make sure that we're throwing away the components that carry the least information (as measured by the variance).\n",
    "\n",
    "If we want to keep a fixed number of components, we can just set the `n_components` parameter of scikit-learn's `PCA` class. Let's take a look in the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-courage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "X, t = data['data'], data['target']\n",
    "names = data['feature_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-proxy",
   "metadata": {},
   "source": [
    "Before transforming the data, lets plot every pair of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,3)\n",
    "\n",
    "\n",
    "for ax,(i,j) in zip(axs.flatten(),[(0,1),(2,3),(0,2),(1,3),(0,3), (1,2)]):\n",
    "    ax.scatter(X[:,i], X[:,j],c=t,)\n",
    "    ax.set_xlabel(names[i])\n",
    "    ax.set_ylabel(names[j])\n",
    "    \n",
    "fig.set_size_inches(10,5)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-delta",
   "metadata": {},
   "source": [
    "Let's reduce our data to just two dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-fraud",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-aluminum",
   "metadata": {},
   "source": [
    "Let's take a look at the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*X2D.T, c=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-courtesy",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-sweden",
   "metadata": {},
   "source": [
    "We see that now the data looks the most separated in the fist label.\n",
    "\n",
    "One thing to keep in mind is that PCA doesn't know about the labels, it's only aim is to find the axis with the most variance being *target-blind*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-puppy",
   "metadata": {},
   "source": [
    "The components of each axis in the original features are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-reason",
   "metadata": {},
   "source": [
    "We can inverse the transformation, but as before, some information was lost in the projection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "X4D_inv = pca.inverse_transform(X2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-gasoline",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(X4D_inv, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,3)\n",
    "\n",
    "\n",
    "for ax,(i,j) in zip(axs.flatten(),[(0,1),(2,3),(0,2),(1,3),(0,3), (1,2)]):\n",
    "    ax.scatter(X4D_inv[:,i], X4D_inv[:,j],c=t,)\n",
    "    ax.set_xlabel(names[i])\n",
    "    ax.set_ylabel(names[j])\n",
    "    \n",
    "fig.set_size_inches(10,5)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-indianapolis",
   "metadata": {},
   "source": [
    "A measure of how much is lost is the *reconstruction error*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.sum(np.square(X4D_inv - X), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-exposure",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Kernel PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-credit",
   "metadata": {},
   "source": [
    "A variant of PCA used to learn non-linear mappings is called *kernel-PCA*, which makes use of the *kernel trick* to map the original features into a higher-dimensional (maybe infinite-dimensional) space, in which PCA is applied. This is useful to learn more complex non-linear transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-comedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d309fc-3bdf-4d8a-8d2e-9476de67fce3",
   "metadata": {},
   "source": [
    "### Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da70f2b0-c555-4756-a1a9-9cf568d77aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kpca = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*X_kpca.T, c=t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-aquatic",
   "metadata": {},
   "source": [
    "Looks funny, let's see how it behaves with a more complex dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f35f836-4a89-47c5-9da7-ad81ccb606ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Swiss Roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "\n",
    "X, z = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-canon",
   "metadata": {},
   "source": [
    "This dataset is a rolled plane, which is harder to decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-checkout",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(z,X[:,1], c=z,  cmap=plt.cm.hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-eugene",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "ax = fig.add_subplot(211, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=z, cmap=plt.cm.hot)\n",
    "ax.view_init(10, -70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-restaurant",
   "metadata": {},
   "source": [
    "Let's compare the results of different kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-uniform",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_pca = KernelPCA(n_components = 2, kernel=\"linear\", fit_inverse_transform=True) #equivalent to PCA(n_components=2)\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\n",
    "sig_pca = KernelPCA(n_components = 2, kernel=\"sigmoid\", gamma=0.001, coef0=1, fit_inverse_transform=True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(11, 4))\n",
    "for subplot, pca, title in ((131, lin_pca, \"Linear kernel\"), \n",
    "                            (132, rbf_pca, \"RBF kernel, $\\gamma=0.04$\"), \n",
    "                            (133, sig_pca, \"Sigmoid kernel, $\\gamma=10^{-3}, r=1$\")):\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    if subplot == 132:\n",
    "        X_reduced_rbf = X_reduced\n",
    "    \n",
    "    plt.subplot(subplot)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=z, cmap=plt.cm.hot)\n",
    "    plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "    if subplot == 131:\n",
    "        plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-recommendation",
   "metadata": {},
   "source": [
    "## Manifold Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-composition",
   "metadata": {},
   "source": [
    "Of course, the optimal projection for the swiss roll would be something that _unrolls_ the data, so that it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-reader",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.scatter(X[:, 1], X[:, 2], c=z, cmap=plt.cm.hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-third",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=z, cmap=plt.cm.hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-hygiene",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.scatter(z, X[:, 1], c=z, cmap=plt.cm.hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-inspection",
   "metadata": {},
   "source": [
    "Learning such kind of subspaces, which are embedded into a higher dimensional space, is called *Manifold Learning*. \n",
    "\n",
    "A simple method, which we'll not cover in detail, is the *Locally Linear Embedding*. Naively, it works by fitting to the closest `n_neighbors` points to each instance, a linear hyperplane of `n_components`dimension. Then, it projects the datapoints to these fitted subspaces.\n",
    "\n",
    "Let's see how it works on the swiss roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n",
    "X_reduced = lle.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Unrolled swiss roll using LLE\", fontsize=14)\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=z, cmap=plt.cm.hot)\n",
    "plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "plt.ylabel(\"$z_2$\", fontsize=18)\n",
    "plt.axis([-0.065, 0.055, -0.1, 0.12])\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-agency",
   "metadata": {},
   "source": [
    "Pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-grass",
   "metadata": {},
   "source": [
    "## Application: Data Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-portrait",
   "metadata": {},
   "source": [
    "Finding an optimal lower-dimensional representation of our data, allows us to store it using less space and reconstruct it loosing as little information as possible. Let's see how this works with images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "mnist.target = mnist.target.astype(np.uint8)\n",
    "\n",
    "X = mnist[\"data\"]\n",
    "t = mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(instances, images_per_row=5, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = plt.cm.binary, **options)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits(X[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-montana",
   "metadata": {},
   "source": [
    "Each digit consits on 784 pixels. Let's see how the reconstructed images when we use PCA to store it as a lower-dimensional vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100)\n",
    "\n",
    "X_reduced = pca.fit_transform(X)\n",
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "plt.subplot(121)\n",
    "plot_digits(X[::2100])\n",
    "plt.title(\"Original\", fontsize=16)\n",
    "plt.subplot(122)\n",
    "plot_digits(X_recovered[::2100])\n",
    "plt.title(\"Compressed\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-syndication",
   "metadata": {},
   "source": [
    "If we're not sure about what dimensionality should we impose, we can plot the cumulative sum of the explained variance ratio to see how much information is lost. If all features are used, it should be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-parks",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X) #fit without reducing dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-latvia",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsum = np.cumsum(pca.explained_variance_ratio_) #this tells us how much information is retained if we stop at each dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-grain",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-research",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsum[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-president",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(cumsum, linewidth=3)\n",
    "plt.axis([0, 400, 0, 1])\n",
    "\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-knitting",
   "metadata": {},
   "source": [
    "Then we can set a threhold of how much variance we want to retain, and use it to estimate the number of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-wells",
   "metadata": {},
   "source": [
    "With 154 dimensions, we preserve 95% of the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(cumsum, linewidth=3)\n",
    "plt.axis([0, 400, 0, 1])\n",
    "\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "\n",
    "plt.plot([d, d], [0, 0.95], \"k:\")\n",
    "plt.plot([0, d], [0.95, 0.95], \"k:\")\n",
    "plt.plot(d, 0.95, \"ko\")\n",
    "\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ee9b9d-ca8c-45bc-a287-03dbdbee54e9",
   "metadata": {},
   "source": [
    "### Visualizing PCs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72bda1f-026c-41f0-b5f4-b33b3964d3c1",
   "metadata": {},
   "source": [
    "One may wonder how does the first few PCs look like for this dataset. \n",
    "\n",
    "As the PCs are vectors in original space, we can reshape them as images and visualise them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200eb802-4ad9-4c66-ae57-02c4d177f8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "npc = 20\n",
    "\n",
    "ncolumns = 10\n",
    "nrows = npc // ncolumns\n",
    "\n",
    "# Add extra row, if necessary\n",
    "if npc % ncolumns:\n",
    "    nrows += 1\n",
    "\n",
    "fig, axs = plt.subplots(nrows, ncolumns, figsize=(16, 2*nrows))\n",
    "\n",
    "for i, ax in zip(range(npc), axs.flatten()):\n",
    "    pci_reshaped = pca.components_[i].reshape(28,28)\n",
    "    ax.imshow(pci_reshaped, cmap='gray_r')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-metabolism",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Application: Preprocessing for Classification or Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-component",
   "metadata": {},
   "source": [
    "Sometimes, reducing dimensionality can reduce noise, and group similar instances together. This can be a good preprocessing for a classification task, or a clustering algorithm, or for visualization purposes. \n",
    "\n",
    "Let's see how this works on the digits dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-mortgage",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_reduced_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d77f290-3d5f-4270-84a0-31b62bedeed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom CMAP\n",
    "from matplotlib import cm\n",
    "cmap = cm.get_cmap('jet', 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-mount",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "scat = ax.scatter(*X_reduced_pca[:1000].T, c=t[:1000], s=50, \n",
    "                  edgecolors='None', alpha=1, cmap=cmap)\n",
    "fig.colorbar(scat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-weapon",
   "metadata": {},
   "source": [
    "We can see that some numbers are brought together (0 and 1 for example), while similar numbers mix. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bb29bd-2c3b-4b90-86b9-a37b705c2b3e",
   "metadata": {},
   "source": [
    "### tSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1751c113-35d0-423e-b360-8d75fd992c02",
   "metadata": {},
   "source": [
    "A popular manifold learning technique used for visualizations is the one called *t-Distributed Stochastic Neighbor Embedding* or tSNE. This technique learns a non-linear mapping that tends to group similar instances toghether, while distancing disimilar instances appart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-secretariat",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_reduced_tsne = tsne.fit_transform(X[:1000])#fit a subset to reduce computing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-criminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "scat = ax.scatter(*X_reduced_tsne.T, c=t[:1000], s=50, cmap=cmap, \n",
    "                  edgecolors='None', alpha=0.8)\n",
    "fig.colorbar(scat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0003f71d-37de-4962-975d-622d8aee04d1",
   "metadata": {},
   "source": [
    "Let's try this in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2d9aed-6b78-478d-8c90-ab6315072299",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "X_reduced_tsne_3d = tsne.fit_transform(X[:1000])#fit a subset to reduce computing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3dcbfe-b30e-4844-811c-dc1fc1cb9191",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "scat = ax.scatter(*X_reduced_tsne_3d.T, c=t[:1000], s=10, cmap=cmap, \n",
    "                  edgecolors='None', alpha=0.8)\n",
    "\n",
    "ax.set_xlim(-15, 15)\n",
    "ax.set_ylim(-15, 15)\n",
    "fig.colorbar(scat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc145fd-51e1-4318-b31e-5a72b40291e8",
   "metadata": {},
   "source": [
    "<font size=4> See interactive plot</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea204e6f-d7a5-4d5d-8aba-bae2e7ad8740",
   "metadata": {},
   "source": [
    "The result is a simplified two- or three-dimensional problem that could be easily solved by a SVM, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-intersection",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-smooth",
   "metadata": {},
   "source": [
    "The second most used unsupervised task, after Dimensionality Reduction, is probably clustering: The objective is to divide the dataset into a number of groups (*clusters*), with a rule to assign new instances a given *affinity* to each group. This is extremely useful in a number of settings:\n",
    "\n",
    "* For customer segmentation\n",
    "* For data analysis\n",
    "* As a dimensionality reduction technique\n",
    "* For anomaly detection (also called outlier detection)\n",
    "* For semi-supervised learning\n",
    "* For search engines\n",
    "* To segment an image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-reservation",
   "metadata": {
    "tags": []
   },
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-australia",
   "metadata": {},
   "source": [
    "Let's introduce some mock data, which we'll try to cluster together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "blob_centers = np.array(\n",
    "    [[ 0.2,  2.3],\n",
    "     [-1.5 ,  2.3],\n",
    "     [-2.8,  1.8],\n",
    "     [-2.8,  2.8],\n",
    "     [-2.8,  1.3]])\n",
    "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n",
    "\n",
    "\n",
    "X, y = make_blobs(n_samples=2000, centers=blob_centers,\n",
    "                  cluster_std=blob_std, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-bracelet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(X, y=None):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-titanium",
   "metadata": {},
   "source": [
    "The K-means algorithm consists on:\n",
    "1. Pick random centroids of the clusters\n",
    "2. Label the data according to the closest centroid (the distance to each centroid plays the role of *affinity*)\n",
    "3. Compute new centroids as the mean position of all the instances having the same label\n",
    "4. Repeat steps 2 and 3 until the centroids position stop changing\n",
    "The algorithm is guaranteed to converge in a finite amount of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#the number of clusters has to be set beforehand\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-train",
   "metadata": {},
   "source": [
    "The labels assigned to each training instance is stored in the attribute `label_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-fault",
   "metadata": {},
   "source": [
    "And assignation to labels of new samples is done through the `predict` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = kmeans.predict(X)\n",
    "\n",
    "(y_pred == kmeans.labels_).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-transport",
   "metadata": {},
   "source": [
    "Let's plot the regions, as well as the centroids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-money",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X):\n",
    "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n",
    "\n",
    "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n",
    "    if weights is not None:\n",
    "        centroids = centroids[weights > weights.max() / 10]\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='o', s=35, linewidths=8,\n",
    "                color=circle_color, zorder=10, alpha=0.9)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=2, linewidths=12,\n",
    "                color=cross_color, zorder=11, alpha=1)\n",
    "\n",
    "def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n",
    "                             show_xlabels=True, show_ylabels=True):\n",
    "    mins = X.min(axis=0) - 0.1\n",
    "    maxs = X.max(axis=0) + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
    "                         np.linspace(mins[1], maxs[1], resolution))\n",
    "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                cmap=\"Pastel2\")\n",
    "    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                linewidths=1, colors='k')\n",
    "    plot_data(X)\n",
    "    if show_centroids:\n",
    "        plot_centroids(clusterer.cluster_centers_)\n",
    "\n",
    "    if show_xlabels:\n",
    "        plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundaries(kmeans, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-physiology",
   "metadata": {},
   "source": [
    "We can see how many iterations it took the algorithm to converge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-heart",
   "metadata": {},
   "source": [
    "Nevertheless is quite unstable, and depends on the random seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k, n_init=1, random_state=1)\n",
    "kmeans.fit(X)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundaries(kmeans, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-domain",
   "metadata": {},
   "source": [
    "To surpass this, the algorithm runs on many different random seeds (setted by `n_init`), and uses the *best* final value, where *best* is measured as the *inertia*: The mean squared distance of each instance to its closest centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-store",
   "metadata": {},
   "source": [
    "We can compute (the negative of) the inertia on a given dataset, by using the `score` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.score(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-vaccine",
   "metadata": {},
   "source": [
    "## Optimal number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-template",
   "metadata": {},
   "source": [
    "Similar to what we did in Dimensionality Reduction to find the number of principal components, one may ask what is the optimal number of clusters for a given dataset. One option is to choose look at the improvement as measured by the Inertia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-titanium",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(X)\n",
    "                for k in range(1, 10)]\n",
    "inertias = [model.inertia_ for model in kmeans_per_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 10), inertias, \"bo-\")\n",
    "\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Inertia\", fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-ghost",
   "metadata": {},
   "source": [
    "We can see again a characteristic elbow, which might indicate that 4/5 could be a good number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundaries(kmeans_per_k[4-1], X)\n",
    "plt.title('k=4')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-victory",
   "metadata": {},
   "source": [
    "Another approach is to look at the *silhouette score*, which is the mean silhouette coefficient over all the instances. \n",
    "\n",
    "An instance's silhouette coefficient is equal to $(b - a)/\\max(a, b)$ where $a$ is the mean distance to the other instances in the same cluster (it is the mean intra-cluster distance), and $b$ is the mean nearest-cluster distance, that is the mean distance to the instances of the next closest cluster (defined as the one that minimizes $b$, excluding the instance's own cluster). \n",
    "\n",
    "The silhouette coefficient can vary between -1 and +1: a coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary, and finally a coefficient close to -1 means that the instance may have been assigned to the wrong cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-ballet",
   "metadata": {},
   "source": [
    "This score is implemented in the metrics module of scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-gabriel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_score(X, kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-center",
   "metadata": {},
   "source": [
    "Let's use it to find the optimal number of clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "silhouette_scores = [silhouette_score(X, model.labels_)\n",
    "                     for model in kmeans_per_k[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(2, 10), silhouette_scores, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Silhouette score\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-donna",
   "metadata": {},
   "source": [
    "Effectively, 4 seems to be the optimal number of clusters for the dataset in question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-civilization",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Application: Clustering for Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41e281d-c4e6-403f-a2f4-0e4a49b9b9cd",
   "metadata": {},
   "source": [
    "Clustering can be used as a preprocessing step for supervised training.\n",
    "\n",
    "Here, we explore this application on the digits dataset, the small brother of the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-philosophy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_digits, t_digits = load_digits(return_X_y=True)\n",
    "X_train, X_test, t_train, t_test = train_test_split(X_digits, t_digits, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c2d51-9ab6-4c1d-95c9-e055e4e12c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what we have\n",
    "print('Labels:', np.unique(t_digits))\n",
    "print('Size of datasets (train; test):', X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-boutique",
   "metadata": {},
   "source": [
    "Let's fit a LogisticRegression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\n",
    "log_reg.fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-finish",
   "metadata": {},
   "source": [
    "The accuracy on the test set is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_score = log_reg.score(X_test, t_test)\n",
    "log_reg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-private",
   "metadata": {},
   "source": [
    "Now let's use a K-means algorithm as a pre-processing pipeline.\n",
    "\n",
    "This means that instead of using the full dataset of size `X_train.shape`, we will used the transformed version, with a reduced number of features, corresponding to the distance to each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1fe014-53dd-47d5-bd02-88dccc8f7287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say we want 50 clusters\n",
    "pp_Kmeans = KMeans(n_clusters=50, random_state=42)\n",
    "\n",
    "X_reduced_KM = pp_Kmeans.fit_transform(X_train)\n",
    "\n",
    "print(X_train.shape, X_reduced_KM.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"kmeans\", pp_Kmeans),\n",
    "    (\"log_reg\", LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)),\n",
    "])\n",
    "pipeline.fit(X_train, t_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-wallet",
   "metadata": {},
   "source": [
    "And compute the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_score = pipeline.score(X_test, t_test)\n",
    "pipeline_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-wisconsin",
   "metadata": {},
   "source": [
    "We see that the error rate on the test set dropped by a factor of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - (1-pipeline_score)/(1-log_reg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-seventh",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Application: Clustering for Semi-Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-speaking",
   "metadata": {},
   "source": [
    "Imagine the situation in which you don't any labels, so you decide to label by hand a few of them. Let's do this with the digits dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_digits, t_digits = load_digits(return_X_y=True)\n",
    "X_train, X_test, t_train, t_test = train_test_split(X_digits, t_digits, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-station",
   "metadata": {},
   "source": [
    "We choose the first 50 (random choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-elimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labeled = 50\n",
    "\n",
    "X_representative_digits = X_train[:n_labeled]\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for index, X_representative_digit in enumerate(X_representative_digits):\n",
    "    plt.subplot(n_labeled // 10, 10, index + 1)\n",
    "    plt.imshow(X_representative_digit.reshape(8, 8), cmap=\"binary\", interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-collar",
   "metadata": {},
   "source": [
    "and label them by hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_representative_digits = t_train[:n_labeled]\n",
    "t_representative_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-clause",
   "metadata": {},
   "source": [
    "And now we use this to train our dataset. Let's fit and see the performance of our model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", random_state=42)\n",
    "log_reg.fit(X_representative_digits, t_representative_digits)\n",
    "log_reg.score(X_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-material",
   "metadata": {},
   "source": [
    "The accuracy is pretty low, but is reasonable given that we're training in only 50 instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-kruger",
   "metadata": {},
   "source": [
    "One way we can improve this is by picking better digits to label. If we pick good representatives of our data, our model will have more information to learn from. \n",
    "\n",
    "We can do this by clustering our digits, and picking one representative from each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-school",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "X_digits_dist = kmeans.fit_transform(X_train)\n",
    "\n",
    "# Keep instance closer to each cluster centrer\n",
    "representative_digit_idx = np.argmin(X_digits_dist, axis=0)\n",
    "X_representative_digits = X_train[representative_digit_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-aurora",
   "metadata": {},
   "source": [
    "Now, we didn't pick representative digits at random, but we picked the ones closer to each clusters centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-influence",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 2))\n",
    "for index, X_representative_digit in enumerate(X_representative_digits):\n",
    "    plt.subplot(k // 10, 10, index + 1)\n",
    "    plt.imshow(X_representative_digit.reshape(8, 8), cmap=\"binary\", interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-allah",
   "metadata": {},
   "source": [
    "We can proceed and label these 50 by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-internet",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_representative_digits = t_train[representative_digit_idx]\n",
    "t_representative_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-sacrifice",
   "metadata": {},
   "source": [
    "And train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\n",
    "log_reg.fit(X_representative_digits, t_representative_digits)\n",
    "log_reg.score(X_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-referral",
   "metadata": {},
   "source": [
    "The accuracy on the test set is much better! And we're still training on only 50 instances, the difference is that we picked correctly the representatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-fault",
   "metadata": {},
   "source": [
    "But we can use our clustering algorithm to go even further. Let's replicate the label of each representative to the rest of the instances of that cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-nation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty vector of the right shape\n",
    "t_train_propagated = np.empty(len(X_train), dtype=np.int32)\n",
    "\n",
    "#lets iterate over the clusters\n",
    "for i in range(k):\n",
    "    #the predicted cluster for each instance in the training set is saved in kmeans.labels_\n",
    "    t_train_propagated[kmeans.labels_==i] = t_representative_digits[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-platinum",
   "metadata": {},
   "source": [
    "And now, let's train with this new automatically labelled **full** dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a10866-ed1c-4cbd-b824-5863decc997a",
   "metadata": {},
   "source": [
    "<img width=600px src=\"https://media.giphy.com/media/xUA7ba9aksCuKR9dgA/giphy.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-ceiling",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(multi_class=\"ovr\", solver=\"lbfgs\", max_iter=5000, random_state=42)\n",
    "log_reg.fit(X_train, t_train_propagated)\n",
    "log_reg.score(X_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-baltimore",
   "metadata": {},
   "source": [
    "The score got even better! This technique is called semi-supervised learning: An unsupervised learning technique is used to propagate labels on the training set, which is used as input for a supervised model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-champion",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-entry",
   "metadata": {},
   "source": [
    "There are many methods for Dimensionality Reduction and Clustering. PCA and K-Means are by far the most populars, but there are many others. Feel free to look at the scikit-learn documentation to see the clustering algorithms implemented on the library:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/clustering.html\n",
    "\n",
    "![Comparison of Algorithms](https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-newman",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Breakout rooms! Eigenfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-directory",
   "metadata": {},
   "source": [
    "We invite you to build a Face Recognizer using PCA + a classification algorithm of your choice:\n",
    "\n",
    "* Use PCA to project the dataset in the N principal components (try N=150 for example)\n",
    "* Plot the image corresponding to the first few (20? 30?) principal components (an *eigenface*)\n",
    "* Use this lower-dimensional projection as an input for a classification algorithm, such as a linear regressor or a SVM\n",
    "\n",
    "**BonusTrack**: Use K-means to cluster the data (in PCA-processed version). Plot different clusters to see if it manages to distinguish each person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "lfw_people = datasets.fetch_lfw_people(min_faces_per_person=70, resize=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2912aaae-a202-4a6e-b6ae-ffefd668451f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lfw_people.data\n",
    "\n",
    "# Number of features\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# The label to predict is the id of the person\n",
    "t = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "\n",
    "print('A dataset with {} instances of {} features and {} classes.'.format(len(X), n_features, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Who are these people\n",
    "target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a387220-5ccd-439d-a534-fad76e965092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the dataset balanced?\n",
    "for i in range(len(target_names)):\n",
    "    print(target_names[i],':',sum(t == i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets plot a few of examples\n",
    "n_pics_per_person = 6\n",
    "\n",
    "n_cols = n_pics_per_person\n",
    "n_rows = len(target_names)\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(1.6*n_cols, 2*n_rows))\n",
    "\n",
    "for i in range(len(target_names)):\n",
    "    # Select instances of that class\n",
    "    Xi = X[t == i]\n",
    "    # Randomly select n_pics_per_person\n",
    "    idj = np.random.choice(len(Xi), size=n_pics_per_person, replace=False)\n",
    "    \n",
    "    for j, jj in enumerate(idj):\n",
    "        axs[i, j].imshow(Xi[jj].reshape(50, 37), cmap='gray')\n",
    "        axs[i, j].axis('off')\n",
    "    axs[i, 0].set_title(target_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-occasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, t_train, t_test = train_test_split(X, t, random_state=42)\n",
    "\n",
    "# your turn ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-stranger",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
