{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación con modelos lineales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Índice<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Clasificación-como-reducción-de-la-dimensión\" data-toc-modified-id=\"Clasificación-como-reducción-de-la-dimensión-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Clasificación como reducción de la dimensión</a></span></li><li><span><a href=\"#Discriminante-lineal-de-Fischer\" data-toc-modified-id=\"Discriminante-lineal-de-Fischer-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Discriminante lineal de Fischer</a></span><ul class=\"toc-item\"><li><span><a href=\"#Solución-analítica\" data-toc-modified-id=\"Solución-analítica-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Solución analítica</a></span></li></ul></li><li><span><a href=\"#Perceptrón\" data-toc-modified-id=\"Perceptrón-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Perceptrón</a></span><ul class=\"toc-item\"><li><span><a href=\"#Stochastic-Gradient-Descent-(SGD)\" data-toc-modified-id=\"Stochastic-Gradient-Descent-(SGD)-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Stochastic Gradient Descent (SGD)</a></span></li><li><span><a href=\"#Optimización-del-perceptrón\" data-toc-modified-id=\"Optimización-del-perceptrón-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Optimización del perceptrón</a></span></li></ul></li><li><span><a href=\"#Qué-es-linealmente-separable\" data-toc-modified-id=\"Qué-es-linealmente-separable-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Qué es linealmente separable</a></span></li><li><span><a href=\"#Datos-de-MNIST\" data-toc-modified-id=\"Datos-de-MNIST-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Datos de MNIST</a></span><ul class=\"toc-item\"><li><span><a href=\"#Clasificación-binaria\" data-toc-modified-id=\"Clasificación-binaria-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Clasificación binaria</a></span></li><li><span><a href=\"#Extras\" data-toc-modified-id=\"Extras-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Extras</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pqoMFY44ddu6"
   },
   "source": [
    "Primero, corremos la celda de preparación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6Qox9gcddu7"
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os, sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"07_Clasificación\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"plots\", CHAPTER_ID)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "# import warnings\n",
    "# warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definamos un par de funciones prácticas\n",
    "if 'google.colab' in sys.modules:\n",
    "    print('Running in colab; defining functions plot_clasi and makew.')   \n",
    "    def plot_clasi(x, t, ws, labels=[], xp=[-1., 1.], thr=[0, ], spines='zero',\n",
    "                   equal=True, join_centers=False, margin=None):\n",
    "        \"\"\"\n",
    "        Plot results of linear classification problems.\n",
    "\n",
    "        :param np.array x: Data matrix\n",
    "        :param np.array t: Label vector.\n",
    "        :param list or tuple ws: list with fitted paramter vector of models\n",
    "                                 (excluding bias), one element per model\n",
    "        :param tuple xp: start and end x-coordinates of decision boundaries and\n",
    "                         margins.\n",
    "        :param list or tuple thr: threshold (-bias) values for each model.\n",
    "        :param str or None spines: whether the spines go through zero. If None,\n",
    "                                   the default behaviour is used.\n",
    "        :param bool equal: whether to use equal axis aspect (default=True;\n",
    "                           recomended to see the parameter vector normal to\n",
    "                           boundary)\n",
    "        :param bool join_centers: whether to draw lines between classes centres.\n",
    "        :param None or tuple margin: tupler of booleans that define whether\n",
    "                                     to plot margin for each model being plotted.\n",
    "                                     If None, False for all models.\n",
    "        \"\"\"\n",
    "        assert len(labels) == len(ws) or len(labels) == 0\n",
    "        assert len(ws) == len(thr)\n",
    "\n",
    "        if margin is None:\n",
    "            margin = [False] * len(ws)\n",
    "        else:\n",
    "            margin = np.atleast_1d(margin)\n",
    "        assert len(margin) == len(ws)\n",
    "\n",
    "        if len(labels) == 0:\n",
    "            labels = np.arange(len(ws)).astype('str')\n",
    "\n",
    "        # Agregemos el vector al plot\n",
    "        fig = plt.figure(figsize=(9, 7))\n",
    "        ax = fig.add_subplot(111)\n",
    "\n",
    "        xc1 = x[t == np.unique(t).max()]\n",
    "        xc2 = x[t == np.unique(t).min()]\n",
    "\n",
    "        ax.plot(*xc1.T, 'ob', mfc='None', label='C1')\n",
    "        ax.plot(*xc2.T, 'or', mfc='None', label='C2')\n",
    "\n",
    "        for i, w in enumerate(ws):\n",
    "\n",
    "            # Compute vector norm\n",
    "            wnorm = np.sqrt(np.sum(w**2))\n",
    "\n",
    "            # Ploteo vector de pesos\n",
    "            ax.quiver(0, thr[i]/w[1], w[0]/wnorm, w[1]/wnorm,\n",
    "                      color='C{}'.format(i+2), scale=10, label=labels[i],\n",
    "                      zorder=10)\n",
    "\n",
    "            # ploteo plano perpendicular\n",
    "            xp = np.array(xp)\n",
    "            yp = (thr[i] - w[0]*xp)/w[1]\n",
    "\n",
    "            plt.plot(xp, yp, '-', color='C{}'.format(i+2))\n",
    "\n",
    "            # Plot margin\n",
    "            if margin[i]:\n",
    "                for marg in [-1, 1]:\n",
    "                    ym = yp + marg/w[1]\n",
    "                    plt.plot(xp, ym, ':', color='C{}'.format(i+2))\n",
    "\n",
    "        if join_centers:\n",
    "            # Ploteo línea que une centros de los conjuntos\n",
    "            mu1 = xc1.mean(axis=1)\n",
    "            mu2 = xc2.mean(axis=1)\n",
    "            ax.plot([mu1[0], mu2[0]], [mu1[1], mu2[1]], 'o:k', mfc='None', ms=10)\n",
    "\n",
    "        ax.legend(loc=0, fontsize=12)\n",
    "        if equal:\n",
    "            ax.set_aspect('equal')\n",
    "\n",
    "        if spines is not None:\n",
    "            for a in ['left', 'bottom']:\n",
    "                ax.spines[a].set_position('zero')\n",
    "            for a in ['top', 'right']:\n",
    "                ax.spines[a].set_visible(False)\n",
    "\n",
    "        return\n",
    "\n",
    "    def makew(fitter, norm=False):\n",
    "        \"\"\"\n",
    "        Prepare parameter vector for an sklearn.liner_model predictor.\n",
    "\n",
    "        :param sklearn.LinearModel fitter: the model used to classify the data\n",
    "        :param bool norm: default: False; whether to normalize the parameter vector\n",
    "        \"\"\"\n",
    "        # # Obtengamos los pesos\n",
    "        w = fitter.coef_.copy()\n",
    "\n",
    "        # # Incluye intercept\n",
    "        if fitter.fit_intercept:\n",
    "            w = np.hstack([fitter.intercept_.reshape(1, 1), w])\n",
    "\n",
    "        # # Normalizon\n",
    "        if norm:\n",
    "            w /= np.linalg.norm(w)\n",
    "        return w.T\n",
    "    \n",
    "else:\n",
    "    print('Not running in colab; importing functions from utils module.')\n",
    "    from utils import makew, plot_clasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OvXvAf62ddu2"
   },
   "source": [
    "## Clasificación como reducción de la dimensión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\vv}[1]{\\boldsymbol{#1}}$\n",
    "$\\newcommand{\\om}[0]{\\boldsymbol{\\omega}}$\n",
    "$\\newcommand{\\norm}[0]{\\mathcal{N}}$\n",
    "$\\newcommand{\\b}[1]{\\mathrm{\\mathbf{#1}}}$\n",
    "$\\newcommand{\\T}{^\\mathrm{T}}$\n",
    "$\\newcommand{\\cu}{\\mathcal{C}_1}$\n",
    "$\\newcommand{\\cd}{\\mathcal{C}_2}$\n",
    "La clasificación puede pensarse como un proceso de reducción de la dimensión de los datos. La clasificación consiste en tomar un dato vectorial, $\\boldsymbol{x}$, y convertirlo a un valor unidimensional (y discreto), que corresponde a su clase, $C_k$.\n",
    "\n",
    "En el caso de un modelo lineal y un problema de clasificación binaria tenemos que\n",
    "\n",
    "$$\n",
    "y = \\om\\T \\boldsymbol{x}\\;\\;,\n",
    "$$\n",
    "donde ahora no incluimos a $\\omega_0$ en el vector $\\om$. En cambio, el valor $-\\omega_0$ sirve como umbral para la clasificación. Si $y \\geq -\\omega_0$, asignamos el dato a la clase $\\cu$, de lo contrario, lo asignamos a la clase $\\cd$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***\n",
    "\n",
    "Veamos un ejemplo de este tema de la proyección.\n",
    "\n",
    "Creemos un set aleatorio en dos dimensiones, compuesto de dos clases, cuya distribución es multinormal\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x} | C_k) = \\mathcal{N}(\\vv{\\mu}_k, \\vv{\\Sigma}_k)\\;\\;.\n",
    "$$\n",
    "\n",
    "Esto es lo que se llama un modelo generativo. Pero acá, en lugar de usarlo para clasificar, lo estamos usando para generar datos. Por supuesto, si supieramos a priori que esta es la distribución, podríamos usar las técnicas que vamos a ver más adelante.\n",
    "\n",
    "Elijamos parámetros a ojo, y muestremos elementos de cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "\n",
    "size1 = 250\n",
    "mu1 = [0, 0]\n",
    "cov1 = [[1, 0.95],\n",
    "        [0.95, 1]]\n",
    "\n",
    "size2 = 200\n",
    "mu2 = [-3, 0.5]\n",
    "cov2 = [[1, 0.8],\n",
    "        [0.8, 1]]\n",
    "\n",
    "np.random.seed(20200922)\n",
    "# Sample classes\n",
    "xc1 = multivariate_normal(mean=mu1, cov=cov1, size=size1).T\n",
    "xc2 = multivariate_normal(mean=mu2, cov=cov2, size=size2).T\n",
    "\n",
    "print(xc1.shape, xc2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos cómo se ven\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(*xc1, 'ob', mfc='None', label='C1')\n",
    "ax.plot(*xc2, 'or', mfc='None', label='C2')\n",
    "\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.legend(loc='lower right', fontsize=16)\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claramente, si proyectamos, digamos sobre el eje de $x_1$, las clases no se separan muy bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos cómo se ven\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "nbins = 20\n",
    "ax.hist(xc1[0], nbins, histtype='step', color='b')\n",
    "ax.hist(xc2[0], nbins, histtype='step', color='r')\n",
    "\n",
    "ax.set_xlabel('$y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno puede entonces preguntarse en qué dirección hay que proyectar los datos para tener la mejor separación entre clases. Y también cabe la pregunta de qué quiere uno decir por \"separación entre clases\".\n",
    "\n",
    "Una idea intuitiva es decir que, una vez proyectados, los centros de las distribuciones se encuentren lo más separados posible. Recuerden que la frontera de decisión es perpendicular a $\\om$:\n",
    "\n",
    "<img align='center' width=450px, src='images/07_Clasificación/Figure4.1.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, la operación $y = \\om\\T \\mathbf{x}$ consiste en realizar la proyección sobre la dirección definida por el vector $\\om$, y los centros de los datos pertenecientes a cada clase, una vez proyectados, son:\n",
    "\n",
    "$$\n",
    "m_k = \\om\\T \\mathbf{m}_k\\;\\;,\n",
    "$$\n",
    "donde\n",
    "$$\n",
    "\\mathbf{m}_k = \\frac{1}{N_k}\\sum_{i \\in \\mathcal{C}_k}\\mathbf{x}_i\\;\\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces, tenemos que elegir $\\om$ que maximise\n",
    "\n",
    "$$\n",
    "m_2 - m_1 = \\om\\T \\left(\\mathbf{m}_2 - \\mathbf{m}_1\\right)\\;\\;,\n",
    "$$\n",
    "\n",
    "pero como esta expresión puede hacerse tan grande como uno quiera, aumentando el módulo de $\\om$, ponemos la condición de que el vector de los pesos esté normalizado:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^D \\omega_i^2 = 1\\;\\;,\n",
    "$$\n",
    "donde $D$ es la dimensión del espacio de los datos (es decir, los elementos de *un* vector de datos $\\mathbf{x}$). \n",
    "\n",
    "Podemos entonces maximizar la distancia $m_2 - m_1$ con esa condición, usando multiplicadores de Lagrange (es el Ejercicio 4.4 del Bishop, que está resuelto [online](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/05/prml-web-sol-2009-09-08.pdf)). El resultado es que\n",
    "\n",
    "$$\n",
    "\\om \\propto \\left(\\mathbf{m}_1 - \\mathbf{m}_2\\right)\\;\\;,\n",
    "$$\n",
    "\n",
    "es decir, que hay que proyectar a lo largo de la línea que separa los centros de las distribuciones de cada clase. Esto está al revés que en el Bishop, porque la respuesta completa es proporcional a $\\left(\\mathbf{m}_1 - \\mathbf{m}_2\\right)$, pero con una constante negativa. Veamos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo las medias muestrales (son vectores!)\n",
    "m1_ = xc1.mean(axis=1).reshape(-1, 1)\n",
    "m2_ = xc2.mean(axis=1).reshape(-1, 1)\n",
    "\n",
    "# Hago la resta y normalizo (por amor al arte)\n",
    "# Cambio el signo con respecto a \n",
    "dm = m1_ - m2_\n",
    "w = dm / np.sqrt(np.dot(dm.T, dm))\n",
    "\n",
    "# Veámoslo!\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Juntamos todo en un solo vector porque así lo podemos usar con la función de recién\n",
    "# Ahora lo ponemos como una matriz de las que le gusta a sklearn\n",
    "X = np.hstack([xc1, xc2]).T\n",
    "\n",
    "tc1 = np.ones(xc1.shape[1])\n",
    "tc2 = np.zeros(xc2.shape[1])\n",
    "\n",
    "t = np.concatenate([tc1, tc2])\n",
    "\n",
    "plot_clasi(X, t, [w,], labels=['means',], xp=[-0.5, 0.5], spines='zero', join_centers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos la proyección de lo largo de esa dirección\n",
    "y1 = np.dot(w.T, xc1)\n",
    "y2 = np.dot(w.T, xc2)\n",
    "\n",
    "# Veamos cómo se ven\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# nbins = 20\n",
    "ax.hist(y1.ravel(), nbins, histtype='step', color='b')\n",
    "ax.hist(y2.ravel(), nbins, histtype='step', color='r')\n",
    "\n",
    "ax.set_xlabel('$y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quedaría elegir el valor del umbral $-\\omega_0$ a partir del cual separamos las clases. Pero vemos que, no importa cómo lo elijamos, y a pesar de que el conjunto de datos es separable linealmente, este criterio no alcanza para separar las clases completamente. \n",
    "\n",
    "Pero no hay que desesperar, ya llega...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Discriminante lineal de Fischer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1S5Jyl0Cddu4"
   },
   "source": [
    "La idea del discriminante lineal de Fischer es aumentar la separación de las clases, a la vec que uno disminuye la varianza de cada una de ellas, de forma de que los histogramas de cada clase sean lo más angosto posible.\n",
    "\n",
    "El criterio de Fischer es\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\om) = \\frac{\\left(m_2 - m_1\\right)^2}{s_1^2 + s_2^2}\\;\\;,\n",
    "\\label{fischer1}\\tag{1}\n",
    "\\end{equation}\n",
    "donde \n",
    "$$\n",
    "s^2_k = \\sum_{i\\in\\mathcal{C}_k}\\left(y_k - m_k\\right)^2\n",
    "$$\n",
    "es la varianza muestral intra-clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora en un ratito vemos cómo maximizar $J(\\om)$ con respecto a $\\om$ y tener el resultado analítico. Pero aprovechemos las computadoras para ganar un poco de intuición al respecto.\n",
    "\n",
    "Veamos primero que el vector $\\om$, dado que está normalizado, podemos parametrizarlo con un ángulo, $\\theta$, medido en sentido contrario a las agujas del reloj desde $x_1$ positivos:\n",
    "$$\n",
    "\\om = (\\cos(\\theta), \\sin(\\theta))\\;\\;.\n",
    "$$\n",
    "\n",
    "Y calculemos el numerador y denominador de $J(\\om)$ para varios valores de $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.linspace(-np.pi, np.pi, 500)\n",
    "num = np.zeros_like(theta)\n",
    "den = np.zeros_like(theta)\n",
    "\n",
    "for i, th in enumerate(theta):\n",
    "    om = np.array([np.cos(th), np.sin(th)]).reshape(-1, 1)\n",
    "    \n",
    "    # Calculamos las proyeccciones de los centros y su diferencia\n",
    "    m1 = np.dot(om.T, m1_)\n",
    "    m2 = np.dot(om.T, m2_)\n",
    "    num[i] = (m1 - m2)**2\n",
    "  \n",
    "    # calculemos las varianzas de cada clase\n",
    "    s1 = np.sum( (np.dot(om.T, xc1) - m1)**2 )\n",
    "    s2 = np.sum( (np.dot(om.T, xc2) - m2)**2 )\n",
    "    den[i] = s1 + s2\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "axd = fig.add_subplot(211)\n",
    "axn = axd.twinx()\n",
    "\n",
    "axn.plot(theta, num, label='Num')\n",
    "axd.plot(theta, den, color='C1', label='Den')\n",
    "axd.axvline(np.arctan2(w[1],w[0]), color='r', ls=':', label='Mean Criterion')\n",
    "#\n",
    "axn.set_xlabel(u'$\\\\theta$ [rad]')\n",
    "axn.set_ylabel(u'$(m_2 - m_1)^2$')\n",
    "axd.set_ylabel(u'$s_1^2 + s_2^2$')\n",
    "fig.legend()\n",
    "\n",
    "# Plotieamos el criterio de Fischer, J(w)\n",
    "axj = fig.add_subplot(212)\n",
    "axj.plot(theta, num/den)\n",
    "axj.axvline(np.arctan2(w[1],w[0]), color='r', ls=':', label='Mean Criterion')\n",
    "axj.set_xlabel(u'$\\\\theta$ [rad]')\n",
    "axj.set_ylabel(u'$J(\\omega)$')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el valor de $\\theta$ que maximiza el numerador, no es el mismo que maximiza el criterio de Fischer (menos mal!). \n",
    "\n",
    "Busquemos a mano ese valor de theta y veamos el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_fischer = theta[np.argmax(num/den)]\n",
    "w_fischer = np.array([np.cos(theta_fischer), np.sin(theta_fischer)]).reshape(-1, 1)\n",
    "\n",
    "plot_clasi(X, t, [w_fischer,], thr=[-1.1,], labels=['fischer',], xp=[-3.8, 1.8], spines=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos la proyección de lo largo de esa dirección\n",
    "y1 = np.dot(w_fischer.T, xc1)\n",
    "y2 = np.dot(w_fischer.T, xc2)\n",
    "\n",
    "# Veamos cómo se ven\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "nbins = 20\n",
    "ax.hist(y1.ravel(), nbins, histtype='step', color='b')\n",
    "ax.hist(y2.ravel(), nbins, histtype='step', color='r')\n",
    "ax.axvline(-1.12)\n",
    "ax.set_xlabel('$y$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, hermoso!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solución analítica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Pero no podemos _calcular_ el $\\om$ que minimiza el criterio de Fischer, en lugar de hacer una exploración numérica?\n",
    "\n",
    "Claro que sí. Pero para eso tenemos que hacer aparecer de forma explícita $\\om$ en la expresión ([1](#mjx-eqn-fischer1)) de arriba (para poder derivar):\n",
    "\n",
    "$$\n",
    "J(\\om) = \\frac{\\left(m_2 - m_1\\right)^2}{s_1^2 + s_2^2}\\;\\;.\n",
    "$$\n",
    "\n",
    "Si consideramos las definiciones de $m_k$ y $s_k^2$, metemos todo en esta expresion y reordenamos (Ejercicio 4.5 del Bishop), encontramos que:\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\om) = \\frac{\\om\\T\\mathbf{S}_E\\om}{\\om\\T\\mathbf{S}_I\\om}\\;\\;,\n",
    "\\label{fischer}\\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "donde $\\mathbf{S}_E$ y $\\mathbf{S}_I$ son las matrices de covarianza entre clases (**E**) e intraclases (**I**), respectivamente, con\n",
    "\n",
    "$$\n",
    "\\mathbf{S}_E = (\\mathbf{m}_2 - \\mathbf{m}_1)(\\mathbf{m}_2 - \\mathbf{m}_1)\\T\n",
    "$$\n",
    "y\n",
    "$$\n",
    "\\mathbf{S}_I = \\underbrace{\\sum_{n\\,\\in\\,\\mathcal{C}_1} \\left(\\mathbf{x}_n - \\mathbf{m}_1\\right)\\left(\\mathbf{x}_n - \\mathbf{m}_1\\right)^\\mathrm{T}}_{\\text{Matriz de covarianza intra-clase 1}}\n",
    "+ \\underbrace{\\sum_{n\\,\\in\\,\\mathcal{C}_2} \\left(\\mathbf{x}_n - \\mathbf{m}_2\\right)\\left(\\mathbf{x}_n - \\mathbf{m}_2\\right)^\\mathrm{T}}_{\\text{Matriz de covarianza intra-clase 2}}\\;\\;.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferenciando la expresión ([2](#mjx-eqn-fischer)) con respecto a $\\om$ e igualando a cero, obtenemos que $J(\\om)$ se maximiza cuando\n",
    "$$\n",
    "\\om \\propto \\mathbf{S}^{-1}_I \\left(\\mathbf{m}_1 - \\mathbf{m}_2\\right)\\;\\;.\n",
    "$$\n",
    "\n",
    "Calculemos esto y comparemos con el cálculo numérico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = m1_ - m2_\n",
    "S1 = np.dot(xc1 - m1_, (xc1 - m1_).T)\n",
    "S2 = np.dot(xc2 - m2_, (xc2 - m2_).T)\n",
    "\n",
    "# Matrix de covarianza intra-clase\n",
    "Si = S1 + S2\n",
    "\n",
    "# Encuentro omega resolviendo la ecuación\n",
    "w_fischer_a = np.linalg.solve(Si, dm)\n",
    "\n",
    "# normalizo\n",
    "w_fischer_a /= np.sqrt(np.dot(w_fischer_a.T, w_fischer_a))\n",
    "\n",
    "print(w_fischer_a)\n",
    "print(w_fischer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, son parecidos...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clasi(X, t, [w_fischer, w_fischer_a], thr=[-1.1,-1.1], labels=['num', 'ana'], xp=[-4, 2], spines=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son idénticas. Cambiamos un poco el umbral para separarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clasi(X, t, [w_fischer, w_fischer_a], thr=[-1.0,-1.2], labels=['num', 'ana'], xp=[-4, 2], spines=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Falta encontrar el omega0. Se puede hacer por cuadrados mínimos, usando valores de t específicos.\n",
    "# ver Bishop Sect. 4.1.5\n",
    "m_ = (xc1.shape[1] * m1_ + xc2.shape[1] * m2_) / (xc1.shape[1] + xc2.shape[1])\n",
    "\n",
    "omega0 = -np.dot(w_fischer.T, m_)\n",
    "\n",
    "print(omega0)\n",
    "\n",
    "plot_clasi(X, t, [w_fischer_a,], labels=['ana'], xp=[-4, 1.5], spines=None, thr=-omega0[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sí, está completo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1S5Jyl0Cddu4"
   },
   "source": [
    "## Perceptrón"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra función discriminativa lineal es el perceptrón. Esta vez, se usa un criterio diferente. Se busca minimizar la función de error (o criterio) del perceptrón.\n",
    "\n",
    "$$\n",
    "E_P(\\om) = -\\sum_{n \\in \\mathcal{M}}\\om\\T \\boldsymbol{\\phi}(\\mathbf{x}_n) \\mathbf{t}_n\\;\\;,\n",
    "$$\n",
    "donde se usa el *encoding* $t \\in \\{-1, 1\\}$, y la suma es sobre el conjunto $\\mathcal{M}$ de los datos mal clasificados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un algoritmo de optimización (es decir, para encontrar el vector de pesos $\\om$ que minimiza $E(\\om)$) que se usa frecuentemente es el *descenso por gradiente estocástico* (SGD).\n",
    "\n",
    "En general, el agoritmo de descenso por gradiente consiste en modificar el vector de pesos en la dirección en la que disminuye más rápidamente la función de error ($E(\\om)$). Es decir, el proceso iterativo\n",
    "\n",
    "$$\n",
    "\\om^{(\\tau + 1)} = \\om^{(\\tau)} - \\eta \\nabla E\\left(\\om^{(\\tau)}\\right)\\;\\;.\n",
    "$$\n",
    "\n",
    "El subíndice $\\tau$ es un entero que indica en qué paso del proceso nos encontramos, y $\\eta$ es la *tasa de aprendizaje* (*learning rate*), que controla cuán rápido nos movemos en la dirección del gradiente (usualmente se usa un valor inferior a 1). El algoritmo se inicia con un vector de pesos $\\om^{(0)}$ y se procede hasta alcanzar la convergencia, que viene dada por un criterio de tolerancia, o hasta alcanzar el número máximo de iteraciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fíjense que en cada paso del algoritmo hay que evaluar el gradiente de la función de error en la posición del nuevo vector de pesos $\\om^{(\\tau+1)}$, y para eso es necesario procesar el conjunto de entrenamiento entero. Esto puede ser costoso numéricamente, o directamente imposible, para grandes conjuntos de datos. Además, existen algoritmos de optimización más eficientes para optimización en *batch*.\n",
    "\n",
    "Por otro lado, el algoritmo del descenso por gradiente puede adaptarse fácilmente para un aprendizaje *online* o secuencial, que resulta útil en la práctica. Se basa en el hecho de que la función de error se puede escribir como la suma de la función de error para cada punto del conjunto de entrenamiento:\n",
    "\n",
    "$$\n",
    "E(\\om) = \\sum_{n=1}^N E_n(\\om)\\;\\;.\n",
    "$$\n",
    "\n",
    "Entonces, podemos actualizar el vector de pesos usando el gradiente de un solo punto de entrenamiento (o de un conjunto reducido), e ir pasando por todos los puntos del conjunto secuencialmente o al azar. En este caso, hablamos del **algoritmo SGD**:\n",
    "\n",
    "$$\n",
    "\\om^{(\\tau + 1)} = \\om^{(\\tau)} - \\eta \\nabla E_n\\left(\\om^{(\\tau)}\\right)\\;\\;,\n",
    "$$\n",
    "\n",
    "donde la única diferencia con lo de arriba es el subíndice $n$ en la función de error, que puede indicar el dato $n$ o algún subconjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimización del perceptrón"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso particular del perceptrón, SGD toma una forma bien simpática y con una interpretación clara.\n",
    "\n",
    "Considerando la expresión para el error del perceptrón, el SGD toma la forma:\n",
    "\n",
    "$$\n",
    "\\om^{(\\tau + 1)} = \\om^{(\\tau)} - \\eta \\nabla E_P\\left(\\om^{(\\tau)}\\right) = \\om^{(\\tau)} + \\eta \\boldsymbol{\\phi}(\\mathbf{x}_n) \\mathbf{t}_n\\;\\;,\n",
    "$$\n",
    "\n",
    "y donde, como arriba, solo se consideran los puntos que están mal clasificados. Como el algoritmo es insensible frente a un cambio de escala de $\\om$, en este caso podemos fijar $\\eta = 1$ sin ninguna pérdida de generalidad.\n",
    "\n",
    "Si un punto de la clase $\\cu$ está mal clasificado, el vector de pesos se modifica sumando $\\boldsymbol{\\phi}(\\mathbf{x}_n)$ a $\\om$; si se trata de la clase $\\cd$, se *resta* $\\boldsymbol{\\phi}(\\mathbf{x}_n)$ a $\\om$.\n",
    "\n",
    "Vamos con un ejemplo en las diapositivas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "*Tarea*: Encontrar la superficie de decisión para los datos que generamos arriba usando el modelo del perceptron\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "Perceptron?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "perce = Perceptron(max_iter=1000, random_state=20210315)\n",
    "\n",
    "# En ese caso, la matriz de diseño es simplemente, x\n",
    "phi = X.copy()\n",
    "\n",
    "# Pero podríamos probar otras cosas, como esto que está comentado más abajo.\n",
    "perce = perce.fit(phi, t)\n",
    "\n",
    "w_perce = makew(perce)\n",
    "print(w_perce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparamos con Fischer\n",
    "plot_clasi(X, t, [w_perce[1:],], ['Perceptron',], xp=[-4, 2], thr=[-w_perce[0],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_perce / np.sqrt(np.dot(w_perce.T, w_perce))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cada perceptrón es un mundo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La manera en la que se optimiza el perceptrón, y el hecho de que el algoritmo se detiene cuando encuentra una superficie de separación que no genera error de clasificación implican que dependiendo de cómo se inicialice el perceptrón (es decir, cuáles son los valores del primer vector $\\om^{(0)}$, y de en qué orden se tomen los datos en el SGD, el resultado puede ser diferente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la clase de `sklearn`, la aleatoreidad entra de dos maneras: \n",
    "\n",
    "1. el valor de `random_state` con el que se instancia el `Perceptron` define la semilla con la cual se *shufflean* (barajan) los datos antes del SGD.\n",
    "\n",
    "2. los valores de los coeficientes iniciales del vector de parámetros pueden pasarse al método `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = np.array([1.0, 1.0]).reshape(1, 2)\n",
    "intercept = np.array([-2,])\n",
    "perce.fit(phi, t, coef_init=w0, intercept_init=intercept)\n",
    "w_perce2 = makew(perce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clasi(X, t, [w_perce[1:], w_perce2[1:]], ['Perceptron', 'Perceptron2'], xp=[-4, 2], thr=[-w_perce[0], -w_perce2[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's go crazy**\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/y7Y3KGJ7l8KbK/giphy.gif\" width=\"480\" height=\"207\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0s = np.random.rand(10, 2) * 5.0\n",
    "intercepts = np.random.rand(10) * 5.0\n",
    "\n",
    "wlist = []\n",
    "thlist = []\n",
    "for i in range(len(w0s)):\n",
    "    perce.fit(phi, t, coef_init=w0s[i], intercept_init=intercepts[i])\n",
    "    w_p = makew(perce)\n",
    "    wlist.append(w_p[1:])\n",
    "    thlist.append(-w_p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clasi(phi, t, wlist, xp=[-3, 1], thr=thlist, legend=False, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas de estas fronteras de decisión serán diferentes en términos de generalización.\n",
    "\n",
    "En pocas clases veremos cómo se puede mejorar esto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interludio: Qué es linealmente separable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora probamos lo mismo en un problema linealmente separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_circle_data():\n",
    "    np.random.seed(2020)\n",
    "    x = np.random.multivariate_normal(mean=[0,0], cov=[[1, 0],[0, 1]], size=1000).T\n",
    "    r = (x[0]**2 + x[1]**2)\n",
    "\n",
    "    xc1 = x[:, r < 0.9]\n",
    "    xc2 = x[:, r > 1.1]\n",
    "    \n",
    "    # Uso coordenadas polares\n",
    "    rc1 = np.sqrt(xc1[0]**2 + xc1[1]**2)\n",
    "    thetac1 = np.arctan2(xc1[1], xc1[0])\n",
    "    rc2 = np.sqrt(xc2[0]**2 + xc2[1]**2)\n",
    "    thetac2 = np.arctan2(xc2[1], xc2[0])\n",
    "\n",
    "    phi1 = np.vstack([rc1[np.newaxis, :], thetac1[np.newaxis, :]])\n",
    "    phi2 = np.vstack([rc2[np.newaxis, :], thetac2[np.newaxis, :]])\n",
    "\n",
    "#     phi1 = rc1[np.newaxis, :]\n",
    "#     phi2 = rc2[np.newaxis, :]\n",
    "\n",
    "    # Creamos la matrix de diseño completa y los labels\n",
    "    t1 = np.ones_like(rc1)\n",
    "    t2 = np.zeros_like(rc2)\n",
    "\n",
    "    return xc1, xc2, phi1, phi2, t1, t2\n",
    "\n",
    "xc1, xc2, phi1, phi2, t1, t2 = make_circle_data()\n",
    "plt.plot(*xc1, '.b')\n",
    "plt.plot(*xc2, '.r')\n",
    "plt.xlabel('X_1')\n",
    "plt.ylabel('X_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos datos son linealmente separables. Solo hay que elegir bien los features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(phi1[0], phi1[1], '.b')\n",
    "plt.plot(phi2[0], phi2[1], '.r')\n",
    "plt.xlabel('r')\n",
    "plt.ylabel('theta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la matrix de diseño completa y los labels\n",
    "#t1 = np.ones_like(rc1)\n",
    "#t2 = -np.ones_like(rc2)\n",
    "\n",
    "t = np.concatenate([t1, t2])\n",
    "phi = np.vstack([phi1.T, phi2.T])\n",
    "\n",
    "print(phi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perce = Perceptron(max_iter=1000, tol=1e-10, random_state=2020202)\n",
    "\n",
    "# Fit data\n",
    "perce.fit(phi, t)\n",
    "w = makew(perce)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps=1e-2\n",
    "plot_clasi(phi, t, [w[1:],], thr=[-w[0],], xp=[0.9, 1.1], spines=None, equal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Veamos que tal funcional\n",
    "for l, f in zip(['Perceptron', ], [perce,]):\n",
    "        \n",
    "    y = cross_val_predict(f, phi, t, cv=5)\n",
    "\n",
    "    print('Matriz de confusión para {}:\\n'.format(l), confusion_matrix(y, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvamos al espacio de input, $x1, x2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.plot(*xc1, '.r')\n",
    "plt.plot(*xc2, '.b')\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "\n",
    "for ww, l in zip([w,], ['Perceptron',]):\n",
    "    # Puntos sobre el plano\n",
    "    yp = phi[:, 1]\n",
    "    xp = (ww[0] - ww[2]*yp)/ww[1]\n",
    "\n",
    "    # Convierto de vuelta al espacio x1 x2 (conservo el signo)\n",
    "    signx1 = np.where((yp > -np.pi/2) * (yp <= np.pi/2), 1, -1)\n",
    "    signx2 = np.where((yp > 0) * (yp <= np.pi), 1, -1)\n",
    "\n",
    "    x1p = signx1 * np.sqrt(xp**2/(1 + np.tan(yp)**2))\n",
    "    x2p = signx2 * np.sqrt(xp**2 - x1p**2)\n",
    "\n",
    "    # Ploteo\n",
    "    i = np.argsort(yp)\n",
    "    plt.plot(x1p[i], x2p[i], '-', lw=4, label=l)\n",
    "    \n",
    "plt.legend()\n",
    "# plt.xlim(-1.5, 1.5)\n",
    "# plt.ylim(-1.5, 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Pregunta**: ¿Cómo cambian las matrices de confusión si achicamos el hueco entre las clases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema no linealmente separable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seamos malvados y sometamos al algoritmo a una situación que no puede resolver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size1 = 250\n",
    "mu1 = [0, 0]\n",
    "cov1 = [[1, 0.95],[0.95, 1]]\n",
    "\n",
    "size2 = 200\n",
    "mu2 = [-1, 0.5]\n",
    "cov2 = [[1, 0.8],[0.8, 1]]\n",
    "\n",
    "np.random.seed(20200922)\n",
    "# Sample classes\n",
    "xc1 = multivariate_normal(mean=mu1, cov=cov1, size=size1).T\n",
    "xc2 = multivariate_normal(mean=mu2, cov=cov2, size=size2).T\n",
    "\n",
    "print(xc1.shape, xc2.shape)\n",
    "\n",
    "# Join arrays\n",
    "X = np.hstack([xc1, xc2]).T\n",
    "t = np.concatenate([np.ones(xc1.shape[1]), -np.ones(xc2.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clasi(X, t, ws=[], thr=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img src=\"https://media.giphy.com/media/3o6Ztq5WTvF7LqQBgI/giphy.gif\" width=\"500\" height=\"270\" frameBorder=\"0\"></img> -->\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/3orif7aLUehOfdmlXy/giphy.gif\" width=\"450\" height=\"270\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perce = Perceptron(max_iter=1000, random_state=20210315)\n",
    "perce.fit(X, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w_nls = makew(perce)\n",
    "plot_clasi(X, t, ws=[w_nls[1:],], thr=[-w_nls[0]], xp=[-3, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos de MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar un set de datos clásico de Machine Learning, los números de MNIST. Se trata de 70 000 imágenes pequeñas de dígitos escritos a mano. El \"target\" de cada uno de estos dígitos es el número que representan.\n",
    "\n",
    "Este set de datos es tan común, que en <tt>sklearn</tt> hay una función que permite bajarlos directamente. Dependiendo de la versión que <tt>sklearn</tt> que estén usando, la función relevante del paquete <tt>datasets</tt> cambia. Además, cambia la forma en la que devuelven los datos (antes estaban ordenados por valor del *target*, ahora viene así nomás. Para que el resultado sea idéntico con ambas versiones, usamos este código, que nos presta amablemente Géron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_target(mnist):\n",
    "    reorder_train = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[:60000])]))[:, 1]\n",
    "    reorder_test = np.array(sorted([(target, i) for i, target in enumerate(mnist.target[60000:])]))[:, 1]\n",
    "    mnist.data[:60000] = mnist.data[reorder_train]\n",
    "    mnist.target[:60000] = mnist.target[reorder_train]\n",
    "    mnist.data[60000:] = mnist.data[reorder_test + 60000]\n",
    "    mnist.target[60000:] = mnist.target[reorder_test + 60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "    mnist.target = mnist.target.astype(np.int8) # fetch_openml() returns targets as strings\n",
    "    sort_by_target(mnist) # fetch_openml() returns an unsorted dataset\n",
    "except ImportError:\n",
    "    from sklearn.datasets import fetch_mldata\n",
    "    mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado es un tipo que todavía no habíamos visto. No vamos a entrar en detalles, pero digamos que tiene los datos en el atributo data y los valores de los labels en el atributo target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist.data.shape)\n",
    "print(mnist.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos tienen 784 *features*, que corresponden a cada uno de los píxeles de las imágenes de 28 x 28. El valor oscila entre 0 (blanco) y 255 (negro). Veamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist.data[500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separemos ahora datos de labels. Seguimos usando nuestra notación, en la que los labels se llaman *t*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, t = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agarremos un dígito cualquiera\n",
    "un_digito = X[120]\n",
    "\n",
    "# Lo ponemos en forma de imagen y lo vemos.\n",
    "un_digito_image = un_digito.reshape(28, 28)\n",
    "plt.imshow(un_digito_image, cmap = mpl.cm.binary,\n",
    "           interpolation=\"None\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "#save_fig(\"some_digit_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es un cinco (creo). Confirmemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que hicimos recién para plotear el número está piola. Vamos a convertirlo en una función para tener a mano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary,\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veamos varios números. Usamos otro código que nos vuelve a prestar nuestro amigo Aurélien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA\n",
    "def plot_digits(instances, images_per_row=10, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary, **options)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "example_images = np.r_[X[:12000:600], X[13000:30600:600], X[30600:60000:590]]\n",
    "plot_digits(example_images, images_per_row=10)\n",
    "#save_fig(\"more_digits_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de seguir mirando, tenemos que separar el conjunto en entrenamiento y testeo. Por suerte, los datos MNIST ya vienen separados, de forma de tener buena representación de cada clase. Las primeras 60000 instancias son de entrenamiento y las últimas 10000 de testeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, t_train, t_test = X[:60000], X[60000:], t[:60000], t[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a barajar el conjunto de entrenamiento. Esto es para asegurarse de que todos los dígitos estarán bien representandos en distintos *folds* de validación cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "X_train, t_train = X_train[shuffle_index], t_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='binary'></a>\n",
    "### Clasificación binaria\n",
    "\n",
    "Vamos a empezar haciendo las cosas fáciles y dividir el problema. Vamos a intentar detectar \"cincos\". Para eso, generamos un nuevo label, que sea 1 cuando el número es cinco, y cero cuando no lo sea. Obviamente, hacemos lo mismo para el test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train_5 = (t_train == 5)\n",
    "t_test_5 = (t_test == 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train_5[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tt>sklearn</tt> tiene una clase <tt>Perceptron</tt>, que es la que vamos a usar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "perce = Perceptron()\n",
    "\n",
    "# Hagamos un fit usando como features los valores de los píxeles directamente (linear regression)\n",
    "# En ese caso, la matriz de diseño es simplemente, Xtrain\n",
    "phi = X_train.copy()\n",
    "\n",
    "# Pero podríamos probar otras cosas, como esto que está comentado más abajo.\n",
    "#phi = np.log(1 + X_train.copy()**2)\n",
    "perce = perce.fit(phi, t_train_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Perceptron says', perce.predict([X[36000],]))\n",
    "# print('Logistic regression says', logi.predict([un_digito]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bueno, parece que ese cinco raro lo identifica correctamente. Pero claro, estaba dentro del conjunto de entrenamiento. Esto no quiere decir absolutamente nada.\n",
    "\n",
    "***\n",
    "**Pregunta**: ¿O sí?\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(t_test_5, perce.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "#cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\n",
    "scores = cross_val_score(perce, X_train, t_train_5, cv=5, scoring=\"accuracy\")\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto parece increíble. Más del 96% de \"accuracy\", con un modelo lineal muy simple. ¿Es posible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Pregunta**: Pensemos un poco más en detenimiento, considerando la naturaleza del dataset. ¿Es realmente un valor tan alto? ¿Qué pasaría si hicieramos un clasificador que dijera que el número nunca es cinco?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En efecto, en este tipo de datasets, donde los datos están muy desbalanceados, no es muy útil la \"accuracy\". Es mejor usar la matriz de confusión.\n",
    "\n",
    "Para crearla, neceistamos calcular predicciones en cada uno de nuestras instancias de entrenamiento, si queremos evitar usar el conjunto de test. En ese caso, podemos hacer CV y calcular las predicciones en cada uno de los folds. Eso lo hace una función de <tt>sklearn</tt> (gracias por tanto!), aunque la implementación no sería complicada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "t_train_pred_perce = cross_val_predict(perce, phi, t_train_5, cv=5)\n",
    "# t_train_pred_logi = cross_val_predict(logi, phi, t_train_5, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(t_train_5, t_train_pred_perce)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con estos valores en mano, podemos calcular el exhaustividad (*recall*) y la precisión.\n",
    "\n",
    "Recordemos:\n",
    "\n",
    "$$\n",
    "\\mathrm{precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathrm{recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "donde TP son los *true positives* (es decir, la cantidad de casos relevantes recuperados correctamente), FP son los *falsos positivos* (es decir, la cantidad de casos no relevante recuperados incorrectamente), y FN son los *falsos negativos* (es decir, la cantidad de casos relevantes **no** recuperados).\n",
    "\n",
    "Calculemos estas cosas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = cm[1, 1]\n",
    "fp = cm[0, 1]\n",
    "fn = cm[1, 0]\n",
    "\n",
    "print('Precision: ', tp/(tp + fp))\n",
    "print('Recall: ',  tp/(tp + fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos resultados hay que interpretarlos de la siguiente manera: la precisión nos dice en qué fracción de los casos en los que perceptrón dice que tiene un \"5\", realmente lo tiene. Además, el *recall* nos dice que fracción de todos los \"5\" encontró."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno puede, en principio, ajustar estos valores mirando el nivel del umbral que usa para clasificar una instancia como verdadera o no. Como vimos, por defecto esto es $y(\\mathbf{x}) = 0$ para el perceptron, pero podemos cambiarlo. Para eso, necesitamos el perceptron nos diga el valor de $y(\\mathbf{x_k})$, para cada imagen $x_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "3HNFr3DPddvC"
   },
   "outputs": [],
   "source": [
    "# Podríamos hacer así\n",
    "y = perce.decision_function(X_train)\n",
    "\n",
    "# Pero mejor sería hacerlo con CV, usando 5 folds\n",
    "y = cross_val_predict(perce, X_train, t_train_5, cv=3, method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos cómo se ven\n",
    "A = plt.hist(y, 100)\n",
    "plt.xlabel('y(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora hagamos un código que vaya variando el umbral y nos calcule la precisión y el recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umb = np.linspace(y.min()-1, y.max()+1, 5000)\n",
    "\n",
    "recall = np.zeros_like(umb)\n",
    "precision = np.zeros_like(umb)\n",
    "fpr = np.zeros_like(umb)\n",
    "\n",
    "for i, u in enumerate(umb):\n",
    "    \n",
    "    # Calcula los índices con detecciones para este umbral\n",
    "    det = y > u\n",
    "    \n",
    "    # Compara esto con los verdaderos casos en esos índices\n",
    "\n",
    "    tp = np.sum(t_train_5[det] == True)\n",
    "    # Falsos positivos\n",
    "    fp = np.sum(t_train_5[det] == False)\n",
    "\n",
    "    # Verdaderos y falsos negativos\n",
    "    tn = np.sum(t_train_5[~det] == False)\n",
    "    fn = np.sum(t_train_5[~det] == True)\n",
    "    \n",
    "    recall[i] = tp/(tp + fn)\n",
    "    precision[i] = tp/(tp + fp)\n",
    "    fpr[i] = fp/(fp + tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "plt.plot(umb, precision, label='Precisión', lw=2)\n",
    "plt.plot(umb, recall, label='Recall', lw=2)\n",
    "plt.xlabel(\"Umbral\", fontsize=16)\n",
    "plt.legend(loc=0, fontsize=16)\n",
    "plt.xlim(-umb.max(), umb.max())\n",
    "plt.axvline(0.0, ls=':', color='0.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede plotear directo uno vs. el otro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 7))\n",
    "plt.plot(precision, recall)\n",
    "plt.axvline(0.1, ls=':', color='0.5')\n",
    "plt.xlabel('Precisión')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Curva PR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de ver esto es con la curva ROC (Receiver Operating Characteristic, o Característica Operativa del Receptor), que es muy similar, salvo que plotea la tasa de verdaderos positivos (es decir, el recall, en función de la tasa de falsos positivos). Cuanto más alejado de la recta unidad esté el sistema mejor, pero por supuesto esto depende del problema a resolver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 7))\n",
    "plt.plot(fpr, recall)\n",
    "plt.plot([0, 1], [0, 1], color='0.5', ls=':')\n",
    "plt.xlabel('Tasa de falsos positivos (FPR)')\n",
    "plt.ylabel('Tasa de verdaderos positivos (TPR) / Recall')\n",
    "plt.title('Curva ROC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra característica interesante es el area bajo la curva. Podemos hacer una estimación veloz, sumando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(recall[1:] * np.diff(fpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay código de <tt>sklearn</tt> para todo esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_curve, roc_auc_score\n",
    "precisions, recalls, thresholds = precision_recall_curve(t_train_5, y)\n",
    "fpr, tpr, thresholds = roc_curve(t_train_5, y)\n",
    "print(roc_auc_score(t_train_5, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_precision_recall_curve, plot_roc_curve\n",
    "plot_precision_recall_curve(perce, X_train, t_train_5)\n",
    "plot_roc_curve(perce, X_train, t_train_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='extras'></a>\n",
    "### Extras\n",
    "\n",
    "¿Cómo se verá el cinco promedio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cinco = X_train[t_train==5].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digit(cinco)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué distribución tienen los pesos en la imagen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digit(perce.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "07_Clasificación.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Índice",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
