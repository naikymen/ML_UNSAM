{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pqoMFY44ddu6"
   },
   "source": [
    "## Preparatory cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6Qox9gcddu7"
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"11_RF\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"plots\", CHAPTER_ID)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "# import warnings\n",
    "# warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us define a couple of useful functions (if in colab, otherwise, take from utils module)\n",
    "if 'google.colab' in sys.modules:\n",
    "\n",
    "    def plot_clasi(x, t, ws, labels=[], xp=[-1., 1.], thr=[0,], spines='zero', equal=True, \n",
    "                   join_centers=False, margin=None):\n",
    "        \"\"\"\n",
    "        Figura con el resultado del ajuste lineal\n",
    "        \"\"\"\n",
    "        assert len(labels) == len(ws) or len(labels) == 0\n",
    "        assert len(ws) == len(thr)\n",
    "\n",
    "        if margin is None:\n",
    "            margin = [False] * len(ws)\n",
    "        else:    \n",
    "            margin = np.atleast_1d(margin)\n",
    "        assert len(margin) == len(ws)\n",
    "\n",
    "        if len(labels) == 0:\n",
    "            labels = np.arange(len(ws)).astype('str')\n",
    "\n",
    "        # Agregemos el vector al plot\n",
    "        fig = plt.figure(figsize=(9, 7))\n",
    "        ax = fig.add_subplot(111)\n",
    "\n",
    "        xc1 = x[t == np.unique(t).max()]\n",
    "        xc2 = x[t == np.unique(t).min()]\n",
    "\n",
    "        ax.plot(*xc1.T, 'ob', mfc='None', label='C1')\n",
    "        ax.plot(*xc2.T, 'or', mfc='None', label='C2')\n",
    "\n",
    "        for i, w in enumerate(ws):\n",
    "\n",
    "            # Compute vector norm\n",
    "            wnorm = np.sqrt(np.sum(w**2))\n",
    "\n",
    "            # Ploteo vector de pesos\n",
    "            x0 = 0.5 * (xp[0] + xp[1])\n",
    "            ax.quiver(0, thr[i]/w[1], w[0]/wnorm, w[1]/wnorm, \n",
    "                      color='C{}'.format(i+2), scale=10, label=labels[i], \n",
    "                      zorder=10)\n",
    "\n",
    "            # ploteo plano perpendicular\n",
    "            xp = np.array(xp)\n",
    "            yp = (thr[i] - w[0]*xp)/w[1] \n",
    "\n",
    "            plt.plot(xp, yp, '-', color='C{}'.format(i+2))\n",
    "\n",
    "            # Plot margin\n",
    "            if margin[i]:\n",
    "                for marg in [-1, 1]:\n",
    "                    ym = yp + marg/w[1]\n",
    "                    plt.plot(xp, ym, ':', color='C{}'.format(i+2))\n",
    "\n",
    "\n",
    "        if join_centers:\n",
    "            # Ploteo línea que une centros de los conjuntos\n",
    "            mu1 = xc1.mean(axis=1)\n",
    "            mu2 = xc2.mean(axis=1)\n",
    "            ax.plot([mu1[0], mu2[0]], [mu1[1], mu2[1]], 'o:k', mfc='None', ms=10)    \n",
    "\n",
    "        ax.legend(loc=0, fontsize=12)\n",
    "        if equal:\n",
    "            ax.set_aspect('equal')\n",
    "\n",
    "        if spines is not None:\n",
    "            for a in ['left', 'bottom']:\n",
    "                ax.spines[a].set_position('zero')\n",
    "            for a in ['top', 'right']:\n",
    "                ax.spines[a].set_visible(False)\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def makew(fitter):\n",
    "\n",
    "        # # Obtengamos los pesos y normalicemos\n",
    "        w = fitter.coef_.copy()\n",
    "\n",
    "        # # Incluye intercept\n",
    "        if fitter.fit_intercept:\n",
    "            w = np.hstack([fitter.intercept_.reshape(1,1), w])\n",
    "\n",
    "        # # Normalizon\n",
    "        #w /= np.linalg.norm(w)\n",
    "        return w.T\n",
    "    \n",
    "#Utility form A. Gèron\n",
    "def plot_decision_regions(clf, X, t, axes=None, npointsgrid=500, legend=False, \n",
    "                          plot_training=True, \n",
    "                          figkwargs={'figsize': [12, 8]}, \n",
    "                          contourkwargs={'alpha':0.3}):\n",
    "    \"\"\"\n",
    "    Plot decision regions produced by classifier.\n",
    "\n",
    "    :param Classifier clf: sklearn classifier supporting XXX\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure(**figkwargs)\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    if axes is None:\n",
    "        dx = X[:, 0].max() - X[:, 0].min()\n",
    "        dy = X[:, 1].max() - X[:, 1].min()\n",
    "        axes = [X[:, 0].min() - 0.1*dx, X[:, 0].max() + 0.1*dx, \n",
    "                X[:, 1].min() - 0.1*dy, X[:, 1].max() + 0.1*dy]\n",
    "        \n",
    "    # Define grid for regions\n",
    "    x1s = np.linspace(axes[0], axes[1], npointsgrid)\n",
    "    x2s = np.linspace(axes[2], axes[3], npointsgrid)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "\n",
    "    # Make predictions on points of grid; reshape to grid format\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "\n",
    "    #custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    ax.contourf(x1, x2, y_pred, **contourkwargs)\n",
    "\n",
    "#     custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "#         plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "\n",
    "    if plot_training:\n",
    "        for label in np.unique(t):\n",
    "            ax.plot(X[:, 0][t==label], X[:, 1][t==label], \"o\", label=\"C{}\".format(label))\n",
    "\n",
    "    # Axis\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n",
    "\n",
    "    if legend:\n",
    "        plt.legend(loc=\"lower right\", fontsize=14)\n",
    "\n",
    "    plt.show()\n",
    "    return fig\n",
    "    \n",
    "    \n",
    "from utils import plot_clasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of Moons dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us again make use of the `make_moons` function to produce a non-linearly separable dataset in two dimensions (which will allow us to plot things)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, t = make_moons(n_samples=400, noise=0.25, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clasi(X, t, [], [], [], [], spines=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split\n",
    "X, X_test, t, t_test = train_test_split(X, t, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple training of a Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=2, n_jobs=6)\n",
    "rf.fit(X, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_decision_regions(rf, X, t, legend=True, npointsgrid=500, \n",
    "                            figkwargs={'figsize': [12, 8]}, \n",
    "                            contourkwargs={'alpha':0.5, 'levels':5, 'cmap':'viridis'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is clearly a model with poor generalisation performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_train = rf.predict(X)\n",
    "y_test = rf.predict(X_test)\n",
    "print('Accuracy (train): {:.3f}'.format(accuracy_score(t, y_train)))\n",
    "print('Accuracy (test): {:.3f}'.format(accuracy_score(t_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Su turno!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divídanse en grupos y:\n",
    "\n",
    "__Parte 1__\n",
    "\n",
    "* Split the moons dataset in a train and test dataset.\n",
    "* Train a `DecistionTreeClassifier` on this dataset. Use Cross-Validation to find the optimal hyperparameters. \n",
    "* Record the best hyperparameters. Reflect on which are the most relevant parameters to explore (choose no more than three).\n",
    "* Train a `RandomForestClassifier` on the same dataset. \n",
    "* Again, use CV to find the optimal hyperparameters \n",
    "\n",
    "**Tip**: remember that besides the parameters of the previous point, you will need to optimise `n_estimators`;\n",
    "\n",
    "**Tip2**: coffee break here.\n",
    "\n",
    "* Compare the hyperparameters obtained with each method. Can you make sense of what you get? \n",
    "* Define a Decision Tree with the parameters optimised for the Random Forest. Train it and evaluate its performance on a test set. What do you see? Does this make sense?\n",
    "\n",
    "**Note:** in all instances, use the function `plot_trees` to see the results.\n",
    "\n",
    "__Parte 2__\n",
    "\n",
    "* Use Random Forests on larger datasets, such as California and MNIST.\n",
    "* Evaluate the feature importances in each case.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árbol Salvaje (sin restricciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Árbol regularizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'param1': ...,\n",
    "              'param2': ...,\n",
    "              ...\n",
    "             }\n",
    "\n",
    "gscv_dt = GridSearchCV(..., ..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenar DT parámetros del RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Larger datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  Regression; back to California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOUSING_PATH = \"datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in sys.modules:\n",
    "        \n",
    "    import tarfile\n",
    "\n",
    "#     DOWNLOAD_ROOT = \"https://github.com/ageron/handson-ml2/raw/master/\"\n",
    "#     HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "    DOWNLOAD_ROOT = \"https://github.com/IAI-UNSAM/datasets/raw/master/\"\n",
    "    HOUSING_URL = DOWNLOAD_ROOT + \"housing/housing_pp_\"\n",
    "\n",
    "    def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "        os.makedirs(housing_path, exist_ok=True)\n",
    "        for tt in ['train', 'test']:\n",
    "            full_path = housing_url + tt + '.csv'\n",
    "            !wget {full_path} -P {housing_path}\n",
    "            \n",
    "    # Corramos la función\n",
    "    fetch_housing_data()\n",
    "\n",
    "else: \n",
    "    print(\"Not running on Google Colab. This cell is did not do anything.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_preprocessed_housing_data(housing_path=HOUSING_PATH, kind='train'):\n",
    "    assert kind in ['train', 'test'], \"kind must be 'train' or 'test'\"\n",
    "    \n",
    "    csv_path = os.path.join(housing_path, \"housing_pp_{}.csv\".format(kind))\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "# The function loads the data as a Pandas DataFrame instance.\n",
    "housing_train = load_preprocessed_housing_data(kind='train')\n",
    "housing_test = load_preprocessed_housing_data(kind='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor variables (in a matrix, as required by sklearn)\n",
    "X = housing_train.drop('median_house_value', axis=1, inplace=False)\n",
    "t = housing_train.median_house_value.values\n",
    "\n",
    "X_test = housing_test.drop('median_house_value', axis=1, inplace=False)\n",
    "t_test = housing_test.median_house_value.values\n",
    "\n",
    "print(X.shape, t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  Classification; MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "bMTVP_DfWu9o"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    mnist = fetch_openml('mnist_784', version=1)\n",
    "    mnist.target = mnist.target.astype(np.int64)\n",
    "except ImportError:\n",
    "    from sklearn.datasets import fetch_mldata\n",
    "    mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "NQ2VejcTXfip"
   },
   "outputs": [],
   "source": [
    "def plot_digit(data):\n",
    "    image = data.reshape(28, 28)\n",
    "    plt.imshow(image, cmap = 'plasma',\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "238px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc-autonumbering": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
