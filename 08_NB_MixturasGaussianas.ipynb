{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes y Mixturas Gaussianas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celdas preparatorias, como de constumbre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os, sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"08_NB-GM\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"plots\", CHAPTER_ID)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "# import warnings\n",
    "# warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimación ML de los parámetros de una distribución normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discutimos extensamente cómo obtener los parámetros de máxima verosimilitud de varios problemas (binomial, regresión lineal, etc.).\n",
    "\n",
    "Ahora veamos con un poco más de detalle cómo se obtienen los parámetros de una multinormal, $\\boldsymbol{\\mu}$ (vector) y la matriz de covarianza $\\Sigma$, a partir de la maximización de la verosimilitud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para facilitar las cosas, trabajemos en dos dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth\n",
    "mu = np.array([0.0, 0.0])\n",
    "cov = np.array([[1.0, 0.5], [0.5, 1.0]])\n",
    "\n",
    "# Muestreamos una multinormal con esos parámetros\n",
    "np.random.seed(20210427)\n",
    "X = np.random.multivariate_normal(mean=mu, cov=cov, size=1000)\n",
    "\n",
    "# Veamos qué nos dieron\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Ok, cada punto es un vector fila.\n",
    "# Para que esté listo para matplotlib, hay que transponer.\n",
    "ax.plot(*X.T, '.')\n",
    "ax.set_xlabel('$X_1$', fontsize=16)\n",
    "ax.set_ylabel('$X_2$', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro juego habitual: supongamos que tenemos los datos y queremos conocer el valor del vector $\\boldsymbol{\\mu}$ y de la matriz de covarianza $\\Sigma$.\n",
    "\n",
    "Para lograr esto, necesitamos un criterio; una función que maximizar. La función que se impone es la verosimiltud de los datos.\n",
    "\n",
    "Suponiendo que los datos están distribuídos como una normal y son independientes (**¡ojo! nada que ver con la independencia de los features; acá hablamos de la independencia entre cada instancia observada**), la verosimilitud se puede escribir como el producto de las verosimilitudes de cada punto. Noten que estas suposiciones acá son válidas\n",
    "\n",
    "$$\n",
    "p(\\mathbf{X} | \\boldsymbol{\\mu}, \\Sigma) = \\prod_{n=1}^N \\mathcal{N}(x_n | \\boldsymbol{\\mu}, \\Sigma)\\;\\;.\n",
    "$$\n",
    "\n",
    "Hermoso, ahora solo queda encontrar los valores que maximizan esta expresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como de costumbre, es conveniente usar el logaritmo de esta expresión, que convierte productoria en suma.\n",
    "\n",
    "$$\n",
    "\\ln p(\\mathbf{X} | \\boldsymbol{\\mu}, \\Sigma) = \\sum_{n=1}^N \\ln \\mathcal{N}(x_n | \\boldsymbol{\\mu}, \\Sigma) = -\\frac{1}{2} \\sum_{n=1}^N (x_n - \\boldsymbol{\\mu})^T \\Sigma^{-1} (x_n - \\boldsymbol{\\mu}) -\\frac{N}{2}\\left|\\Sigma\\right| - \\frac{N D}{2}\\ln(2\\pi)\\;\\;,\n",
    "$$\n",
    "\n",
    "donde $D$ es la dimensión de los datos, y usamos la expresión típica de la normal (p.ej. Bishop, ecuación 2.43).\n",
    "\n",
    "El último término es una constante, y podemos ignorarlo olímpicamente. \n",
    "\n",
    "Para avanzar, supongamos primero que solo queremos encontrar $\\boldsymbol{\\mu}$, pero que conocemos $\\Sigma$. ¿Cómo hacemos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuerza bruta??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analítico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede mostrar que la derivada de la ecuación de arriba se vuelve cero para \n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu}_\\mathrm{ML} = \\frac{1}{N}\\sum_{n=1}N x_n\\;\\;,\n",
    "$$\n",
    "\n",
    "como podríamos haber esperado también de la Ley de los Grandes Números."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En la práctica, esto corresponde con hacer la media a lo largo de las filas\n",
    "mu_ML = np.average(X, axis=0)\n",
    "\n",
    "print(mu_ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por supuesto, no es exactamente cero; estos estimadores tienen su varianza, pero vamos a ignorar eso y concentrarnos en la estimación puntual.\n",
    "\n",
    "En este caso, es fácil ver que:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\mu}_\\mathrm{ML}\\right] = \\boldsymbol{\\mu}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La situación con la covarianza es bastante más complicada, pero vamos a dar el resultado directo.\n",
    "\n",
    "$$\n",
    "\\Sigma_\\text{ML} = \\frac{1}{N}\\sum_{n=1}^N (x_n - \\boldsymbol{\\mu}_\\text{ML})(x_n - \\boldsymbol{\\mu}_\\text{ML})^T\\;\\;,\n",
    "$$\n",
    "\n",
    "donde tenemos el producto de un vector columna por un vector fila, lo que nos da una matriz, tal como queremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = X - mu_ML\n",
    "\n",
    "print(h.shape)\n",
    "\n",
    "# Tenemos que transponer h para que tenga la forma de la ecuación de arriba\n",
    "hh = h.T\n",
    "\n",
    "# Listo, ahora la suma se hace con el producto dot y normalizamos.\n",
    "sigma_ML = np.dot(hh, hh.T) / len(X)\n",
    "print(sigma_ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, se puede ver que \n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left[\\Sigma_\\mathrm{ML}\\right] = \\frac{N-1}{N}{\\Sigma}\\;\\;,\n",
    "$$\n",
    "\n",
    "por lo que conviene, para tener un estimador no sesgado, usar la definición\n",
    "\n",
    "$$\n",
    "\\tilde{\\Sigma} = \\frac{1}{N-1}\\sum_{n=1}^N (x_n - \\boldsymbol{\\mu}_\\text{ML})(x_n - \\boldsymbol{\\mu}_\\text{ML})^T\\;\\;,\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sigma_ML * len(X) / (len(X) - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar las ideas de arriba para realizar un modelo de Naive Bayes usando Gaussianas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caso isotrópico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creemos unos datos con `make_blobs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "gt_center = np.array([[2.0, 2.0],[-2.0, -2.0]])\n",
    "X, t = make_blobs(1000, n_features=2, centers=gt_center, cluster_std=1.5, random_state=1234, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_clasi\n",
    "plot_clasi(X, t, [], [], [], [], join_centers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El entrenamiento consiste en calcular los estimadores de máxima verosimilitud usando solo los datos de cada una de las clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hagamos a mano las medias\n",
    "mu_c0 = np.average(X[t == 0], axis=0)\n",
    "mu_c1 = np.average(X[t == 1], axis=0)\n",
    "\n",
    "print('__Centers__ C0:', mu_c0, 'C1:', mu_c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb = gnb.fit(X, t)\n",
    "\n",
    "# Let's see the parameters\n",
    "print('Means', *gnb.theta_)\n",
    "print('Sigma\\n', gnb.sigma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute performance by cross validations\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cvs = cross_val_score(gnb, X, t, cv=5, scoring='accuracy')\n",
    "print('CV accuracy {:.2f} +/- {:.2f}'.format(cvs.mean(), cvs.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caso no isotrópico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacerle las cosas un poco más difíciles con un dataset no tan adaptado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "\n",
    "def make_dataset(mu1=[0, 0], mu2=[-4, 1.5], \n",
    "                 cov1=[[1, 0.95],[0.95, 1]], cov2=[[1, 0.8],[0.8, 1]], \n",
    "                 size1=250, size2=200, random_state=20200922):\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "    # Sample classes\n",
    "    xc1 = multivariate_normal(mean=mu1, cov=cov1, size=size1).T\n",
    "    xc2 = multivariate_normal(mean=mu2, cov=cov2, size=size2).T\n",
    "\n",
    "    print(xc1.shape, xc2.shape)\n",
    "\n",
    "    # Concatenate both classes\n",
    "    x = np.hstack([xc1, xc2]).T\n",
    "    tc1 = np.ones(xc1.shape[1])\n",
    "    tc2 = -np.ones(xc2.shape[1])\n",
    "    # tc2 = -np.ones((1, xc2.shape[1]))\n",
    "    t = np.hstack([tc1, tc2])\n",
    "    \n",
    "    # Make desing matrix adding a column of ones\n",
    "    phi0 = np.ones([len(x), 1])\n",
    "    phi = np.hstack([phi0, x.copy()])\n",
    "\n",
    "    return x, t, phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov1 = np.array([[1.5**2, 1.0],[1.0, 1.5**2]])\n",
    "cov2 = [[1.5**2, -2.0],[-2.0, 1.5**2]]\n",
    "\n",
    "X, t, phi = make_dataset(mu1=gt_center[0], mu2=gt_center[1], cov1=cov1, cov2=cov2, size1=500, size2=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clasi(X, t, [], [], [], [], join_centers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustemos y evalumos por CV\n",
    "gnb = gnb.fit(X, t)\n",
    "\n",
    "cvs = cross_val_score(gnb, X, t, cv=5, scoring='accuracy')\n",
    "print('CV accuracy {:.2f} +/- {:.2f}'.format(cvs.mean(), cvs.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.theta_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo de p(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las grandes ventajas de los modelos generativos es la posibilidad de calcular la probabilidad marginal de un dato (es decir, marginalizando sobre las clases). Para eso, usamos el teorema de Bayes.\n",
    "\n",
    "Suponemos $p(C_1) = p(C_2) = 0.5$\n",
    "\n",
    "$$\n",
    "p(x) = \\sum_{k=1}^2 p(x | C_k) p(C_k)\\;\\;,\n",
    "$$\n",
    "\n",
    "Entonces, podemos, dado un punto, calcular ese valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = np.array([4, 5]).reshape(1, -1)\n",
    "\n",
    "h = [st.multivariate_normal(mean=gnb.theta_[i], cov=np.eye(2)*gnb.sigma_[i,i]).pdf(new_x) * 0.5 for i in range(2)]\n",
    "print('p(x) = {:.6f}'.format(np.sum(h)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = np.linspace(-6, 6, 500)\n",
    "Y_ = np.linspace(-6, 6, 500)\n",
    "\n",
    "XX, YY = np.meshgrid(X_, Y_)\n",
    "\n",
    "# Prepare array\n",
    "aa =np.array([XX.ravel(), YY.ravel()]).T\n",
    "allZ = np.array([st.multivariate_normal(mean=gnb.theta_[i], cov=np.eye(2)*gnb.sigma_[i,i]).pdf(aa) * 0.5 for i in range(2)])\n",
    "\n",
    "# Sum over classes\n",
    "Z_ = np.sum(allZ, axis=0)\n",
    "\n",
    "# Reshape\n",
    "ZZ = Z_.reshape(500, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 9))\n",
    "plt.plot(*X.T, '.w')\n",
    "plt.pcolor(XX, YY, np.log(ZZ), alpha=0.5, shading='nearest')\n",
    "plt.colorbar(label='log(p(x))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.predict_proba(np.array([3, -3]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixturas Gaussianas\n",
    "<a id='mixtures'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Las limitaciones de la distribución normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando estudiamos la distribución normal, vimos que uno de las ventajas de sus propiedades analíticas es que sirven como elementos de modelos más complejos. \n",
    "\n",
    "Una técnica para hacer esto es la mixtura de Gaussianas, que nos permite captar mucha más complejidad que una simple normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in sys.modules:\n",
    "    # Si están en el colab, primero corran este código\n",
    "    !mkdir datasets\n",
    "    !wget https://raw.githubusercontent.com/IAI-UNSAM/datasets/master/faithful.csv\n",
    "    !mv faithful.csv datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos datos contienen información sobre la erupciones de geisers en Old Faithful, en el parque [Yellowstone](https://www.nps.gov/yell/learn/photosmultimedia/webcams.htm).\n",
    "\n",
    "Los datos son la duración de las erupciones en segundos, y el tiempo entre una erupción y la siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leamos y miremos datos de Faithful\n",
    "import pandas as pd\n",
    "df = pd.read_csv('datasets/faithful.csv')\n",
    "\n",
    "# Vamos las primeras columnas\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que hay una columna inutil, que nos vamos a sacar de encima y después veamos los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trash = df.pop('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0 = df.plot('eruptions', 'waiting', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claramente, no podremos ajustar esto con una normal.\n",
    "Calculemos los valores de máxima verosimilitud para $\\mu$ y $\\Sigma$ usando los datos y veamos qué tal da.\n",
    "\n",
    "Recordemos los resultados\n",
    "\n",
    "$$\n",
    "\\mathbf{\\mu}_\\mathrm{ML} = \\frac{1}{N}\\sum_{n=1}^{N}\\mathbf{x}_n \\\\\n",
    "$$\n",
    "$$\n",
    "\\mathbf{\\Sigma}_\\mathrm{ML} = \\frac{1}{N}\\sum_{n=1}^{N} (\\mathbf{x}_n - \\mathbf{\\mu}_\\mathrm{ML})(\\mathbf{x}_n - \\mathbf{\\mu}_\\mathrm{ML})^T\n",
    "$$\n",
    "\n",
    "Calculemos eso para los datos de Old Faithful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_ml = df.mean().to_numpy()\n",
    "\n",
    "dx = (df - mu_ml)\n",
    "sigma_ml = np.dot(dx.T, dx) / len(dx)\n",
    "\n",
    "print(u'$\\mu_ML$ = {}'.format(mu_ml))\n",
    "print(u'$\\Sigma_ML$ = {}'.format(sigma_ml))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con estos valores, construyamos una binormal. Como no vamos a sacar valores al azar, usamos la implementación en <tt> scipy.stats</tt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "p = st.multivariate_normal(mean=mu_ml, cov=sigma_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora viene la parte divertida. Vamos a graficar los contornos de la binormal sobre los datos de Old Faithful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero construyo una grilla de valores\n",
    "x, y = np.mgrid[1.0:6.0:0.1, 40:100:1]\n",
    "pos = np.dstack((x, y))\n",
    "\n",
    "# Prepara figure\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Vuelve a graficar datos grafica contornos (elegimos usar 4 contornos)\n",
    "df.plot('eruptions', 'waiting', kind='scatter', ax=ax)\n",
    "l0 = ax.contour(x, y, p.pdf(pos), 4, colors='0.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claramente el modelo de una Gaussiana no es bueno. El modelo no solo no recoje el comportamiento bimodal de los datos, sino que además contiene mucha densidad en el centro entre ambos grupos de puntos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un paréntesis\n",
    "\n",
    "Podemos calcular la verosimulitud de estos puntos. Suponiendo que son independientes:\n",
    "\n",
    "$$\n",
    "p(D | \\mu_\\mathrm{ML}, \\Sigma_\\mathrm{ML}) = \\prod_{n=1}^N \\mathcal{N(\\mathbf{x}_n | \\mu_\\mathrm{ML}, \\Sigma_\\mathrm{ML}})\\;\\;.\n",
    "$$\n",
    "\n",
    "Evaluo entonces la densidad de la distribución en cada punto y multiplico. Para eso, usamos la práctica función `apply` de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densities = df.apply(p.pdf, axis=1)\n",
    "print('La verosimilitud de este modelo es {}'.format(np.prod(densities)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué pasó? Las densidades no son cero. Veamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densities.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Alguna idea?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos encontramos con un problema típico cuando tratamos con muchos datos. La verosimilitud de cada uno de ellos es baja, y al multiplicarlos, llegamos por debajo del número más pequeño que puede representar la computadora. Este problema se hará más agudo cuantos más datos tengamos y cuanto mayor sea la dimensionalidad del espacio.\n",
    "\n",
    "La solución es trabajar con el logaritmo de la verosimilitud.\n",
    "\n",
    "En ese caso:\n",
    "\n",
    "$$\n",
    "\\ln p(D | \\mu_\\mathrm{ML}, \\Sigma_\\mathrm{ML}) = \\sum_{n=1}^N \\ln \\mathcal{N(\\mathbf{x}_n | \\mu_\\mathrm{ML}, \\Sigma_\\mathrm{ML}}) = -\\frac{1}{2}\\sum_{n=1}^{n} \\left[\\ln(|\\mathbf{\\Sigma}|) + \\ln(2\\pi) + (\\mathbf{x}_n - \\mathbf{\\mu}_\\mathrm{ML})^T \\mathbf{\\Sigma}^{-1}(\\mathbf{x}_n - \\mathbf{\\mu}_\\mathrm{ML})\\right]\\;\\;.\n",
    "$$\n",
    "\n",
    "Por suerte esto está implementado en <tt>scipy.stats</tt>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdensities = df.apply(p.logpdf, axis=1)\n",
    "print(logdensities.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora solo hay que sumar\n",
    "lnlike = np.sum(logdensities)\n",
    "print(lnlike)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fíjense que podríamos haber intentado directamente hacer el log de las densidades que ya teníamos; pero esto solo va a funcionar si la densidad de cada punto es (numéricamente) distinta de cero, lo cual no ocurrirá si el espacio tiene muchas dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnlike2 = np.sum(np.log(densities))\n",
    "print(lnlike2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este ejemplo muestra una de las limitaciones de un modelo normal: solo puede captar distribuciones con un único modo. Una forma de sobreponerse a este problema es usar un modelo que consista en una superposición de $K$ distribuciones normales.\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}) = \\sum_{k=1}^K \\pi_k{\\mathcal{N}(\\mathbf{x} \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}\\;\\;,\n",
    "$$\n",
    "donde cada densidad $\\mathcal{N}(\\mathbf{x} \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$ se conoce como una *componente* de la mixtura, y tiene su propio valor medio y matriz de covarianza. Los $\\pi_k$ son los *coeficientes* de mezcla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Los coeficientes como probabilidades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrando a ambos lados de la igualdad sobre todos los valores de $\\mathbf{x}$ y viendo que tanto $p(\\mathbf{x})$ como las componentes normales son funciones de distribución de probabilidad, llegamos a que\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^K \\pi_k = 1\\;\\;.\n",
    "$$\n",
    "\n",
    "Además, como $p(\\mathbf{x})$ tienen que ser positiva, encontramos que una forma de garantizar eso de manera general es definir que los coeficientes sean positivos. Por lo tanto, $0 \\leq \\pi_k \\leq 1$, y los coeficientes satisfacen los requerimientos para ser probabilidades.\n",
    "\n",
    "Podemos, entonces, asignar una función de distribución de masa, $p(k)$, que describe la probabilidad (prior) de que se elija la compoenente $k$, y $p(k) = \\pi_k$. Entonces, distribución sobre $\\mathbf{x}$ queda:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}) = \\sum_{k=1}^K p(\\mathbf{x}, k) = \\sum_{k=1}^K p(k) p(\\mathbf{x} | k)\\;\\;,\n",
    "$$\n",
    "\n",
    "donde la probabilidad condicionada $p(\\mathbf{x} | k) = \\mathcal{N}(\\mathbf{x} \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$.\n",
    "\n",
    "En otras palabras, visto de esta manera, para muestrear el modelo de mixtura puede pensarse como una cadena, donde uno decide primero a qué modo pertence un punto y luego muestrea ese modo. Esto se llama muestreo ancestral. Veámoslo en acción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supongamos una distribución unidimensional de dos modos, con los siguientes parámetros\n",
    "mu1 = [0.0,]\n",
    "sigma1 = [2.0,]\n",
    "mu2 = [5.0,]\n",
    "sigma2 = [1.5,]\n",
    "\n",
    "# Generemos las dos distris\n",
    "n1 = st.norm(mu1, sigma1)\n",
    "n2 = st.norm(mu2, sigma2)\n",
    "\n",
    "# Ahora pensemos que tenemos una distribución prior p(k) con las siguientes elementos\n",
    "p1 = 0.65\n",
    "p2 = 0.35\n",
    "\n",
    "# Veamos como podemos generar, fácilmente, muestras de la mixtura:\n",
    "# p1 * N1 + p2 * N2\n",
    "# Primero muestreo p(k)\n",
    "N = 5000\n",
    "k = np.where(np.random.rand(N) < p1, 1, 2)\n",
    "# Para más modos, se puede hacer a mano, pero es más dificil. Mejor usar st.multinomial\n",
    "\n",
    "# Y ahora lo combino con las mixturas\n",
    "\n",
    "# Modo 1 (estricto)\n",
    "# x1 = n1.rvs(sum(k==1), 1)\n",
    "# x2 = n2.rvs(sum(k==2), 1)\n",
    "\n",
    "# x = np.hstack([x1, x2])\n",
    "\n",
    "# Modo 2 (generando de más)\n",
    "x1 = n1.rvs(N, 1)\n",
    "x2 = n2.rvs(N, 1)\n",
    "\n",
    "x = np.where(k==1, x1, x2).reshape((-1, 1))\n",
    "\n",
    "h = plt.hist(x, 50, histtype='step', density=True)\n",
    "\n",
    "# Combinemos esto con la pdf del modelo\n",
    "xx = np.linspace(x.min(), x.max(), 400)\n",
    "plt.plot(xx, p1 * n1.pdf(xx) + p2 * n2.pdf(xx), 'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obviamente, como sabemos de dónde viene cada muestra, las podemos plotear por separado.\n",
    "bins = h[1]\n",
    "h = plt.hist(x, bins, histtype='step', lw=5, color='k', alpha=0.2)\n",
    "h1 = plt.hist(x[k==1], bins, label='Componente 1', histtype='step')\n",
    "h2 = plt.hist(x[k==2], bins, label='Componente 2', histtype='step')\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero mantengamos en mente que en los problemas que vamos a resolver, contaremos con la distribución azul de la celda anterior y sin información sobre el origen de cada muestra. Es como si hubiéramos perdido el arreglo $\\mathbf{k}$. Al conjunto de datos $\\{\\mathbf{x}, \\mathbf{k}\\}$ se lo llama dataset completo, y a $\\mathbf{x}$ se lo conoce como dataset incompleto. \n",
    "\n",
    "Claro que nunca estuvo completo, en principio...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responsabilidades\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un rol crucial lo van a jugar las posteriores de la probabilidad de cada modo $p(k \\:|\\: \\mathbf{x})$, que se conocen como *responsabilidades* (¡cuánta seriedad!)\n",
    "\n",
    "$$\n",
    "\\gamma_k(\\mathbf{x}) \\equiv p(k \\:|\\: \\mathbf{x})\\;\\;.\n",
    "$$\n",
    "\n",
    "Como es de esperar, podemos calcular la pinta que tienen las responsabilidades a partir del teorema de Bayes:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\gamma_k(\\mathbf{x}) &=& \\frac{p(k) p(\\mathbf{x}\\:|\\:k)}{\\sum_i p(i) p(\\mathbf{x}\\:|\\:i)}\\\\\n",
    "   &=&\\frac{\\pi_k{\\mathcal{N}(\\mathbf{x} \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}}{\\sum_i{\\pi_i{\\mathcal{N}(\\mathbf{x} \\:|\\: \\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i)}}}\\;\\;,\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "donde usamos la definición de arriba\n",
    "$$\n",
    "p(\\mathbf{x}) = \\sum_{k=1}^K \\pi_k{\\mathcal{N}(\\mathbf{x} \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}\\;\\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Verosimilitud__ (o Donde arrancan los líos)\n",
    "\n",
    "Naturalmente, necesitaremos poder escribir la verosimilitud para un modelo de este tipo. Como vimos arriba, el problema con las exponenciales de la normal, es que muchas veces generan problemas numéricos.\n",
    "\n",
    "Vamos entonces a escribir el logaritmo de la verosimilitud, como ya hicimos un montón de veces.\n",
    "\n",
    "$$\n",
    "\\ln p(\\mathbf{X} \\:|\\: \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi}) = \\sum_{n=1}^N \\ln \\left\\{\\sum_{k=1}^K \\pi_k{\\mathcal{N}(\\mathbf{x}_n \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}\\right\\}\\;\\;,\n",
    "$$\n",
    "donde estamos usando esta notación: $\\boldsymbol{\\mu} \\equiv \\left\\{\\boldsymbol{\\mu}_1, \\ldots, \\boldsymbol{\\mu}_K\\right\\}$, y lo mismo para $\\boldsymbol{\\Sigma}$ y para $\\boldsymbol{\\pi}$. Y como siempre $\\mathbf{X} = \\left\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_N\\right\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el logaritmo actúa sobre la sumatoria y no sobre cada normal de manera individual. Esto hace que sea complicado obtener una forma cerrada para los parámetros que maximizan la verosimilitud. Tenemos que usar un procedimiento iterativo, conocido como el algoritmo de maximización de la expectación, o *expectation maximisation* (EM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Variables latentes / ocultas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Para presentar de forma correcta el EM, primero tenemos que introducir un conjunto de nuevas variables, llamadas latentes. Más que un deseo de rigurosidad insaciable, nos mueve el hecho de que los modelos que tienen variables latentes suelen ser capaces de captar mucha complejidad de forma sencilla, y resultan una herramienta fundamental a la hora de modelar datos de la vida real. (**NB**: diría que, más allá de cualquier algoritmo particular que se puedan llevar de la materia, el concepto de las variables latentes y el truco del kernel son las dos herramientas más poderosas que presentamos; son como el martillo y el serrucho de un constructor de modelos). Vamos a ver enseguida que agrandar el problema y pensar en una distribución conjunta facilita la resolución del problema condicional, $p(\\mathbf{x} \\:|\\: k)$, que presentamos más arriba.\n",
    "\n",
    "Vamos a introducir una variable latente binaria, $K$-dimensional, $z$, con una representación 1-de-K. Es decir, todos los elementos son 0, excepto por el elemento $k$ que es uno y que indica a qué modo pertenece una determinada observación. Claramente, se verifica que $\\sum_k z_k = 1$ (la suma de los elementos de la variable $z$ es uno).\n",
    "\n",
    "Entonces, la relación con los coeficientes anteriores es:\n",
    "\n",
    "$$\n",
    "p(z_k = 1) = \\pi_k\\;\\;,\n",
    "$$\n",
    "y podemos escribir la distribución del _vector_ $\\mathbf{z}$ como\n",
    "\n",
    "$$\n",
    "p(\\mathbf{z}) = \\prod_{k=1}^K \\pi_k^{z_k}\\;\\;.\n",
    "$$\n",
    "\n",
    "Además, la condicional\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x} \\:|\\: z_k = 1) = \\mathcal{N}(\\mathbf{x} \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\n",
    "$$\n",
    "también puede escribirse para el vector $\\mathbf{z}$ de la misma manera\n",
    "$$\n",
    "p(\\mathbf{x} \\:|\\: \\mathbf{z}) = \\prod_{k=1}^K \\mathcal{N}(\\mathbf{x} \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)^{z_k}\\;\\;.\n",
    "$$\n",
    "\n",
    "Bien, podemos entonces, plantear el problema original en términos de las distribuciones conjuntas, condicionales y marginales, $p(\\mathbf{x}, \\mathbf{z})$, $p(\\mathbf{x} \\:|\\: \\mathbf{z})$ y $p(\\mathbf{z})$, que, como sabemos desde la primera clase, se relacionan a través de la regla del producto:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}, \\mathbf{z}) = p(\\mathbf{x} \\:|\\: \\mathbf{z}) p(\\mathbf{z})\\;\\;,\n",
    "$$\n",
    "y también podemos marginalizar sobre las variables latentes para recuperar el modelo anterior:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}) = \\sum_\\mathbf{z} p(\\mathbf{x} \\:|\\: \\mathbf{z}) p(\\mathbf{z}) = \\sum_{k=1}^K \\pi_k{\\mathcal{N}(\\mathbf{x} \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}\\;\\;.\n",
    "$$\n",
    "\n",
    "O sea, estoy hace media hora dándole vueltas a las ecuaciones para llegar a lo mismo. ¿O no? \n",
    "En principio, vean que para cada dato $n$ habrá una variable latente $\\mathbf{z}_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El algoritmo EM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La gran ventaja de este planteo es que ahora podemos trabajar con la distribución conjunta $p(\\mathbf{x}, \\mathbf{z})$, que nos da pie para poner en marcha un algoritmo muy general, que sirva para todos los modelos que usen variables latentes. \n",
    "\n",
    "Acá vamos a presentarlo solo para las mixturas gaussianas. Pueden encontrar un tratamiento mucho más general en la Sect. 9.4 del Bishop.\n",
    "\n",
    "Vamos a definir dos matrices, con los datos incompletos $\\mathbf{X}$ y con las variables latentes $\\mathbf{Z}$, cuya fila $n$ corresponde al vector de variables latentes del dato $\\mathbf{x}_n$, $\\mathbf{z}_n$.\n",
    "Primero, volvamos a escribir el logaritmo de la verosimilitud:\n",
    "$$\n",
    "\\ln p(\\mathbf{X} \\:|\\: \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}, \\boldsymbol{\\pi}) = \\sum_{n=1}^N \\ln \\left\\{\\sum_{k=1}^K \\pi_k{\\mathcal{N}(\\mathbf{x}_n \\:|\\: \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}\\right\\}\\;\\;.\n",
    "$$\n",
    "\n",
    "Derivando con respecto a $\\boldsymbol{\\mu}_k$ e igualando a cero, tenemos:\n",
    "\n",
    "$$\n",
    "0 = \\sum_{n=1}^N \\gamma(z_{nk}) \\Sigma_k^{-1} (\\mathbf{x_n} - \\boldsymbol{\\mu}_k)\\;\\;.\n",
    "$$\n",
    "\n",
    "Multiplicando a ambos lados por $\\Sigma_k$, llegamos a:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu}_k = \\frac{1}{N_k} \\sum_{n=1}^N {\\gamma(z_{nk}) \\mathbf{x}_n}\\;\\;,\n",
    "$$\n",
    "\n",
    "con\n",
    "\n",
    "$$\n",
    "N_k = \\sum_{n=1}^N {\\gamma(z_{nk})}\\;\\;,\n",
    "$$\n",
    "\n",
    "que es una especie de número efectivo de muestras provenientes del modo $k$.\n",
    "\n",
    "Lo mismo podemos hacer para la covarianza $\\Sigma_k$ y da\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Sigma}_k = \\frac{1}{N_k} \\sum_{n=1}^N {\\gamma(z_{nk}) (\\mathbf{x}_n - \\boldsymbol{\\mu}_k)(\\mathbf{x}_n - \\boldsymbol{\\mu}_k)^T}\\;\\;,\n",
    "$$\n",
    "\n",
    "y para los coeficientes de mezcla, usando la condición (via multiplicadores de Lagrange) de que su suma tienen que ser uno.\n",
    "\n",
    "$$\n",
    "\\pi_k = \\frac{N_k}{N}\\;\\;.\n",
    "$$\n",
    "\n",
    "Hay que ver que la expresión para las medias y la covarianza tiene la misma pinta que para el caso de una sola distribución gaussiana. Pero como aparecen las responsabilidades, la forma no es cerrada.\n",
    "\n",
    "Se vislumbra un proceso iterativo de la siguiente forma:\n",
    "\n",
    "1. Se inicializan los parámetros $\\boldsymbol{\\mu}_k$, $\\boldsymbol{\\Sigma}_k$, y $\\pi_k$.\n",
    "\n",
    "2. Se calculan las responsabilidades para esos valores de los parámetros (paso E).\n",
    "\n",
    "3. Con las nuevas responsabilidades, se recalculan los parámetros que maximizan la verosimilitud (paso M). Como antes, podemos actualizar las medias antes de calcular el nuevo valor de las covarianzas.\n",
    "\n",
    "4. Volver a calcular el logaritmo de la verosimilitud.\n",
    "\n",
    "5. Evaluar algún criterio de detención, sea en LogL o en los parámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver cómo funciona con nuestro set unidimensional. `sklearn` tiene implementado el algo EM en la clase `\n",
    "mixture.GaussianMixture`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Veamos la docu\n",
    "GaussianMixture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gm = GaussianMixture(2, means_init=np.array([0.0, 5.0]).reshape(2, 1), weights_init=[0.5, 0.5], tol=1e-8)\n",
    "gm = GaussianMixture(2, tol=1e-8)\n",
    "gm.fit(x)\n",
    "\n",
    "print('Fitted parameters')\n",
    "print('Mixing coeff:', gm.weights_.flatten())\n",
    "print('Means:', gm.means_.flatten())\n",
    "print('widhts:', np.sqrt(gm.covariances_).flatten())\n",
    "\n",
    "print('')\n",
    "\n",
    "# Recordemos los parámetros\n",
    "print('Parámetros Gound Truth')\n",
    "print('Mixing coeff: {}, {}'.format(p1, p2))\n",
    "print('Means:', np.array([n1.args[0], n2.args[0]]).flatten())\n",
    "print('widhts:',  np.array([n1.args[1], n2.args[1]]).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nada mal. Comparemos ahora con la densidad real. Esta clase de `sklearn` tiene un método, `score_samples`, que hace exactamente eso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparemos con la densidad real\n",
    "plt.plot(xx, p1 * n1.pdf(xx) + p2 * n2.pdf(xx), 'r-', label='Truth', lw=5, alpha=0.4)\n",
    "plt.plot(xx, np.exp(gm.score_samples(xx.reshape(-1, 1))), label='ML fit', color='k')\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta clase, las predicciones son sobre el modo al que pertenece cada punto. \n",
    "\n",
    "Es un modelo de clasificación generativo __no supervisado__! Veamos que tan bien funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = gm.predict(x)\n",
    "\n",
    "# Preparemos un vector de labels con valores {0, 1} en lugar de {1, 2}.\n",
    "t = np.where(k==1, 1, 0)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, precision_recall_curve\n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(t, y)))\n",
    "print('Precision: {:.2f}'.format(precision_score(t, y)))\n",
    "print('Recall: {:.2f}'.format(recall_score(t, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = gm.predict(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algo un poco más robusto sería con cross val predict\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "y = cross_val_predict(gm, x, t, cv=5)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, precision_recall_curve\n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(t, y)))\n",
    "print('Precision: {:.2f}'.format(precision_score(t, y)))\n",
    "print('Recall: {:.2f}'.format(recall_score(t, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probas de cada clase\n",
    "probas = gm.predict_proba(x)\n",
    "\n",
    "# Tenemos que elegir una de las clases como la positiva\n",
    "p, r, th = precision_recall_curve(t, probas[:, 1])\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(th, p[:-1], lw=2, alpha=0.8, label='Precision')\n",
    "plt.plot(th, r[:-1], lw=2, alpha=0.8, label='Recall')\n",
    "plt.legend(loc=0, fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisitamos a Old Faithful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora a ustedes. Tomen el dataset de Old Faithful de acá arriba y ajusten un modelo de mixtura. Usen la clase de `sklearn`.\n",
    "\n",
    "Después copien el código que está arriba para plotear los contornos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = GaussianMixture(2, tol=1e-3)\n",
    "gm.fit(df)\n",
    "\n",
    "print('Fitted parameters')\n",
    "print('Mixing coeff:', gm.weights_)\n",
    "print('Means:\\n', gm.means_)\n",
    "print()\n",
    "print('Covariances:\\n', gm.covariances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero construyo una grilla de valores\n",
    "x, y = np.mgrid[1.0:6.0:0.1, 40:100:1]\n",
    "pos = pos = np.dstack((x, y))\n",
    "\n",
    "# Compute mixture pdf\n",
    "\n",
    "# Usando sklearn\n",
    "X_ = np.c_[x.ravel(), y.ravel()]\n",
    "\n",
    "# Calculo la clase\n",
    "clase1d = gm.predict(X_)\n",
    "clase2d = clase1d.reshape(x.shape)\n",
    "\n",
    "# Calculo la densidad\n",
    "pdf = np.exp(gm.score_samples(X_))\n",
    "P = pdf.reshape(x.shape)\n",
    "\n",
    "# \"A mano\"\n",
    "# P = (gm.weights_[0] * st.multivariate_normal.pdf(pos, mean=gm.means_[0], cov=gm.covariances_[0]) +\n",
    "#        gm.weights_[1] * st.multivariate_normal.pdf(pos, mean=gm.means_[1], cov=gm.covariances_[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara figure\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Vuelve a graficar datos y densidad\n",
    "l0 = ax.contourf(x, y, P, 256, alpha=1)\n",
    "df.plot('eruptions', 'waiting', kind='scatter', ax=ax, c='w', alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepara figure\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Vuelve a graficar datos y clases\n",
    "l0 = ax.contourf(x, y, clase2d, alpha=1)\n",
    "df.plot('eruptions', 'waiting', kind='scatter', ax=ax, c=gm.predict(df), alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción de clase para un nuevo dato y su responsabilidad.\n",
    "x_nuevo = np.array([3.0, 65.0]).reshape(1, 2)\n",
    "print(gm.predict(x_nuevo))\n",
    "print(gm.predict_proba(x_nuevo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marginalizamos uno de los features (la duración de la erupción).\n",
    "\n",
    "xx = np.linspace(40, 100, 200)\n",
    "\n",
    "scale1 = np.sqrt(gm.covariances_[0][1,1])\n",
    "scale2 = np.sqrt(gm.covariances_[1][1,1])\n",
    "\n",
    "pp = (gm.weights_[0] * st.norm.pdf(xx, loc=gm.means_[0][1], scale=scale1) + \n",
    "      gm.weights_[1] * st.norm.pdf(xx, loc=gm.means_[1][1], scale=scale2))\n",
    "\n",
    "plt.plot(xx, pp)\n",
    "plt.xlabel('waiting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nos quedamos con una única componente\n",
    "k = 1\n",
    "\n",
    "# No vale la pena incluir el coeficiente, porque después hay que normalizar\n",
    "pp = st.multivariate_normal.pdf(pos, mean=gm.means_[k], cov=gm.covariances_[k])\n",
    "\n",
    "l0 = plt.contourf(x, y, pp, alpha=1)\n",
    "#plt.colorbar(l0)\n",
    "#df.plot('eruptions', 'waiting', kind='scatter', ax=ax, c='w', alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus track: K-means\n",
    "\n",
    "Es un algoritmo de clustering que funciona minimizando la distancia de los puntos a los centros de los clusters.\n",
    "\n",
    "Veamos su funcionamiento con los datos de Old Faithful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=2)\n",
    "km.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos los parámetros\n",
    "km.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.eruptions, df.waiting, c=km.labels_)\n",
    "plt.plot(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], 'or', markersize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "265px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
