{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01p_Proyecto_de_cabo_a_rabo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "VCp1NAk0Gy3H",
        "_ZL_AUp4G1jT",
        "uc6cdepvJQhN"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVZ6uqpoteyT"
      },
      "source": [
        "# Una breve introduccion a lo que queremos del curso\n",
        "\n",
        "Este notebook introduce un ejemplo de lo que queremos que sepan hacer hacia el final del curso. Conceptualmente, el procedimiento es:\n",
        "\n",
        "*   Explorar los datos\n",
        "*   Plantear el problema a resolver\n",
        "*   Preprocesar los datos a un formato adecuado\n",
        "*   Elegir algoritmos\n",
        "*   Fittear y validar\n",
        "*   Decidir el algoritmo final, y testear\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1BGWisEvtpg"
      },
      "source": [
        "Antes que nada, importamos algunos paquetes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBKZj7ZusPd9"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import tarfile\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "import pandas as pd\n",
        "from pandas.plotting import scatter_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bcuzVcmudwx"
      },
      "source": [
        "# Los datos\n",
        "\n",
        "Vamos a utilizar el dataset de California. Una buena practica es, si los datos lo permiten, separar un conjunto de test que voy a utilizar solamente al final de todo, para evaluar todo lo que hice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF9F9DBRwwoa"
      },
      "source": [
        "## Traemos los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwCqIy_2n2GL"
      },
      "source": [
        "HOUSING_PATH = \"datasets\"\r\n",
        "def load_housing_data(housing_path=HOUSING_PATH):\r\n",
        "    csv_path = os.path.join(housing_path, \"housing.csv\")\r\n",
        "    return pd.read_csv(csv_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UHxjrY7ne2k"
      },
      "source": [
        "if 'google.colab' in sys.modules:\r\n",
        "        \r\n",
        "    import tarfile\r\n",
        "\r\n",
        "    DOWNLOAD_ROOT = \"https://github.com/ageron/handson-ml2/raw/master/\"\r\n",
        "    HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\r\n",
        "\r\n",
        "    !mkdir -p ./datasets/housing\r\n",
        "\r\n",
        "    def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\r\n",
        "        os.makedirs(housing_path, exist_ok=True)\r\n",
        "        tgz_path = os.path.join(housing_path, \"housing.tgz\")\r\n",
        "        #urllib.request.urlretrieve(housing_url, tgz_path)\r\n",
        "        !wget {HOUSING_URL} -P {housing_path}\r\n",
        "        housing_tgz = tarfile.open(tgz_path)\r\n",
        "        housing_tgz.extractall(path=housing_path)\r\n",
        "        housing_tgz.close()\r\n",
        "\r\n",
        "    # Corramos la función\r\n",
        "    fetch_housing_data()\r\n",
        "\r\n",
        "else: \r\n",
        "    print(\"Not running on Google Colab. This cell is did not do anything.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKzlNnomoKFL"
      },
      "source": [
        "## Preprocesamos un poco"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRbFJgIOn4Qu"
      },
      "source": [
        "housing_pre = load_housing_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNWyQRI6BUR3"
      },
      "source": [
        "### Train/test splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpK0OY0DCS7p"
      },
      "source": [
        "Como bien discutimos, es menester separar los datos en dos conjuntos: el de entrenamiento y el de testeo. Este ultimo debe utilizarse al FINAL del proyecto para garantizar una prediccion no sesgada de la performance del modelo final.\r\n",
        "\r\n",
        "La opcion mas sencilla es utilizar `train_test_split`, donde especificamos el porcentaje de datos que separamos para testear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3NaMEJ2CnHl"
      },
      "source": [
        "train_df, test_df = train_test_split(housing_pre, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHa8Vq71Cvcv"
      },
      "source": [
        "Un problema que puede aparecer es que este splitting no sea representativo en algunos features. Por ejemplo, `median_income`. Definiendo una variable auxiliar `income_cat` con los valores binneados de `median_income` podemos estudiar esto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwTkEj56DB7S"
      },
      "source": [
        "housing_pre[\"income_cat\"] = pd.cut(housing_pre[\"median_income\"],\r\n",
        "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\r\n",
        "                               labels=[1, 2, 3, 4, 5])\r\n",
        "\r\n",
        "plt.hist([housing_pre[housing_pre[\"income_cat\"] == cat].median_income for cat in range(1,6)], \r\n",
        "         label =  list(range(1,6)), \r\n",
        "         bins=50,\r\n",
        "         stacked=True)\r\n",
        "plt.legend()\r\n",
        "plt.ylabel('# of districts')\r\n",
        "plt.xlabel('Median House Income')\r\n",
        "plt.show()\r\n",
        "plt.hist(housing_pre['income_cat'], density=True)\r\n",
        "plt.xticks([1,2,3,4,5])\r\n",
        "plt.ylabel('# of districts')\r\n",
        "plt.xlabel('Median House Income Category')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "housing_pre['income_cat'].value_counts() / len(housing_pre)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REHdqb7hDUVY"
      },
      "source": [
        "Veamos que pasa con `income_cat` con el `train_test_split` por defecto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aTGb4NeC071"
      },
      "source": [
        "train_df, test_df = train_test_split(housing_pre, test_size=0.2, random_state=42)\r\n",
        "plt.hist(train_df['income_cat'], density=True)\r\n",
        "plt.xticks([1,2,3,4,5])\r\n",
        "plt.ylabel('# of districts')\r\n",
        "plt.xlabel('Median House Income Category')\r\n",
        "plt.title('Train')\r\n",
        "test_df['income_cat'].value_counts() / len(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okrjbtHLDl0D"
      },
      "source": [
        "Otra opcion es obligar a respetar proporciones utilizando `StratifiedShuffleSplit`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7ocQKpVwx_c"
      },
      "source": [
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=445543)\n",
        "for train_index, test_index in split.split(housing_pre, housing_pre[\"income_cat\"]):\n",
        "    california_housing_train = housing_pre.loc[train_index]\n",
        "    california_housing_test = housing_pre.loc[test_index]\n",
        "\n",
        "#for set_ in (california_housing_train, california_housing_test):\n",
        "#    set_.drop(\"income_cat\", axis=1, inplace=True)\n",
        "\n",
        "plt.hist(california_housing_train['income_cat'], density=True)\n",
        "plt.xticks([1,2,3,4,5])\n",
        "plt.ylabel('# of districts')\n",
        "plt.xlabel('Median House Income Category')\n",
        "plt.title('Train')\n",
        "california_housing_test['income_cat'].value_counts() / len(california_housing_test)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7TCnwFmD5R0"
      },
      "source": [
        "Para comparar ambos metodos, podemos hacer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNXzUp5_D7x7"
      },
      "source": [
        "comparison_df = pd.concat([housing_pre['income_cat'].value_counts() / len(housing_pre), test_df['income_cat'].value_counts() / len(test_df),california_housing_test['income_cat'].value_counts() / len(california_housing_test)], axis=1)\r\n",
        "comparison_df.columns = ['original', 'random_split', 'stratified_split']\r\n",
        "comparison_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6koeKx3tEO19"
      },
      "source": [
        "La diferencia no es _enorme_ pero puede ser importante, especialmente para datasets chicos/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PncqkbP0D79G"
      },
      "source": [
        "Una vez que decidi, puedo sacarme de encima esta categoria auxiliar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtJZF2XfD_H_"
      },
      "source": [
        "for set_ in (california_housing_train, california_housing_test):\r\n",
        "    set_.drop(\"income_cat\", axis=1, inplace=True)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfNjCB8vyDLv"
      },
      "source": [
        "california_housing_train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "253kKL45yhtm"
      },
      "source": [
        "california_housing_test.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwaNIKHnykOA"
      },
      "source": [
        "## Exploremos los datos (nuevamente)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVcyUq2Yyjrl"
      },
      "source": [
        "housing=california_housing_train.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNAP4XRRy3lg"
      },
      "source": [
        "housing.hist(bins=50, figsize=(20,15))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjcLoYcuy4PW"
      },
      "source": [
        "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
        "              \"housing_median_age\"]\n",
        "scatter_matrix(housing[attributes], figsize=(12, 8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jm37PZGwzZlO"
      },
      "source": [
        "corr_matrix = housing.corr()\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3G0l8oodVjk"
      },
      "source": [
        "### Valores raros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upMSl0iiFVSR"
      },
      "source": [
        "Como vimos en la clase pasada, hay features con valores raros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTL_Yt6UFYMd"
      },
      "source": [
        "columns = housing.columns.to_list()\r\n",
        "\r\n",
        "N_col = 4\r\n",
        "N = len(columns)\r\n",
        "N_rows = int(np.ceil(N/N_col))\r\n",
        "\r\n",
        "fig, ax = plt.subplots(N_rows,N_col, figsize=(5*N_col,5*N_rows))\r\n",
        "\r\n",
        "for i in range(N_rows):\r\n",
        "    for j in range(N_col):\r\n",
        "        ax[i,j].hist(housing[columns[i*N_rows+j]], bins=50)\r\n",
        "        ax[i,j].set_title(columns[i*N_rows+j])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnStfAe6Fyup"
      },
      "source": [
        "Vemos que existe una saturacion en algunos features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTm4fJVMFnbn"
      },
      "source": [
        "problematic_columns = ['median_house_value', 'housing_median_age', 'median_income']\r\n",
        "max_values=[]\r\n",
        "for col in problematic_columns:\r\n",
        "    max_value = housing[col].max()\r\n",
        "    print(f\"{col}: {sum(housing[col] == max_value)} districts with {col} = {max_value} ({round(sum(housing[col] == max_value)/len(housing)*100,2)}%).\")\r\n",
        "    max_values.append(max_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cxUAc-uF5J5"
      },
      "source": [
        "Cuando pasa esto, tenemos que decir que hacemos. Una opcion es descartarlos y poner una cota a partir de la cual no confiamos en el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEhKf-muGCbJ"
      },
      "source": [
        "housing_clean = housing.copy()\r\n",
        "for col, max_value in zip(problematic_columns, max_values):\r\n",
        "    housing_clean = housing_clean[housing_clean[col] != max_value]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5w0yKXEGFPI"
      },
      "source": [
        "columns = housing_clean.columns.to_list()\r\n",
        "\r\n",
        "N_col = 4\r\n",
        "N = len(columns)\r\n",
        "N_rows = int(np.ceil(N/N_col))\r\n",
        "\r\n",
        "fig, ax = plt.subplots(N_rows,N_col, figsize=(5*N_col,5*N_rows))\r\n",
        "\r\n",
        "for i in range(N_rows):\r\n",
        "    for j in range(N_col):\r\n",
        "        ax[i,j].hist(housing_clean[columns[i*N_rows+j]], bins=50)\r\n",
        "        ax[i,j].set_title(columns[i*N_rows+j])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvF7KgQsGM2G"
      },
      "source": [
        "Pero ojo! si hacemos esto en entrenamiento tambien tenemos que hacerlo con el conjunto de testeo. Pero ahora utilizamos los max_values ya aprendidos, no volvemos a aprenderlos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITTHNxA4Gbrx"
      },
      "source": [
        "housing_test=california_housing_test.copy()\r\n",
        "housing_test_clean = housing_test.copy()\r\n",
        "for col, max_value in zip(problematic_columns, max_values):\r\n",
        "    housing_test_clean = housing_test_clean[housing_test_clean[col] != max_value]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlQJOoylGrA0"
      },
      "source": [
        "### Features faltantes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB0plfIBGsyS"
      },
      "source": [
        "Si nos fijamos bien en los datos, vemos algo molesto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zV460TMGww_"
      },
      "source": [
        "housing_clean.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI9b6D0pG2oT"
      },
      "source": [
        "En efecto, `total_bedrooms` esta incompleto! Cuando pasa esto, tenemos _a grosso modo_ tres opciones\r\n",
        "\r\n",
        "1. Excluir los features incompletos del analisis (esto reduce la cantidad de columnas)\r\n",
        "2. Excluir las observaciones o datos donde faltan features del analisis (esto reduce la cantidad de filas).\r\n",
        "3. Rellenar los valores faltantes con datos sinteticos utilizando algun criterio.\r\n",
        "\r\n",
        "La eleccion optima depende, como siempre, de los datos y del problema. Nosotros vamos a usar la opcion 3 y llenamos `total_bedrooms` con la mediana.\r\n",
        "\r\n",
        "Podemos hacerlo a mano o aprovechar `sklearn` y utilizar `SimpleImputer`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZRlbye0HxBh"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\r\n",
        "imputer = SimpleImputer(strategy='median')\r\n",
        "\r\n",
        "train_imputer = imputer.fit_transform(housing_clean.drop(['ocean_proximity'], axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-0aMCeTIAyO"
      },
      "source": [
        "Veamos que funciona"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTokMOKgIB_y"
      },
      "source": [
        "train_imputer[np.where(housing_clean['total_bedrooms'].isnull()>0),4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TfW6aRMINFG"
      },
      "source": [
        "Nuevamente, si lo hacemos en entrenamiento tenemos que hacerlo en testeo. Esto se hace utiliznado unicamente `transform`, no volviendo a fittear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV5QlOk6ISEm"
      },
      "source": [
        "test_imputer = imputer.transform(housing_test_clean.drop(['ocean_proximity'], axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAJn9P8fIj_t"
      },
      "source": [
        "### Variables categoricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz_NpSdbIn9r"
      },
      "source": [
        "Las variables categoricas presentan un desafio a la hora de entrenar. Los modelos necesitan numeros reales. Para transformar una variable categorica en un numero real se suelen considerar dos opciones:\r\n",
        "\r\n",
        "*   Asignar un numero a cada categoria. En el caso binario, se suele tomar 0 y 1, para distinguir entre \"apagado\" y \"prendido\". Para el caso de $\\geq2$ categorias, esto tiene sentido si existe un \"orden\" que sigue la numericacion.\r\n",
        "*   Mapear las categorias a un espacio de menor dimensionalidad (lo que se conoce como \"embedding\".\r\n",
        "\r\n",
        "Un ejemplo de lo segundo es el `One Hot Encoding`. Si hay K categorias posibles para la variable, se mapea cada medicion a un vector de K dimensiones con 0s en todos lados salvo en el lugar correspondiente a su categoria."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_aWGfAQJbBH"
      },
      "source": [
        "housing_clean['ocean_proximity'].hist()\r\n",
        "housing_clean['ocean_proximity'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKZ-5tKdInbT"
      },
      "source": [
        "ohe=OneHotEncoder()\r\n",
        "housing_cat_ohe=ohe.fit_transform(housing_clean[['ocean_proximity']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrSCdHLWIneb"
      },
      "source": [
        "housing_cat_ohe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TClif473JDqa"
      },
      "source": [
        "housing_cat_ohe.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1Rdpk0FJNw_"
      },
      "source": [
        "Veamos a que se refiere ese encodeo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZlRMyLmJOT8"
      },
      "source": [
        "housing_clean[['ocean_proximity']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eti-rlSKIVNm"
      },
      "source": [
        "### Estandarizacion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fWe9KbiIXBf"
      },
      "source": [
        "Cuando tenemos muchas features continuas, podemos tener problemas de unidades. Para evitar eso y que el algoritmo no asigne importancias espurias, conviene estandarizar. Estandarizar es fijar una estrategia para pasar los valores al intervalo [0,1], [-1,1] o lo que sea.\r\n",
        " \r\n",
        "En particular, el `StandardScaler` transforma a x en \"cantidad de desviaciones estandar de la media:\r\n",
        "\r\n",
        "$x\\rightarrow \\frac{x-\\mu}{\\sigma}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPgjwhVfK5wa"
      },
      "source": [
        "scaler = StandardScaler()\r\n",
        "\r\n",
        "train_num_scaled = scaler.fit_transform(housing_clean.drop(\"ocean_proximity\", axis=1))\r\n",
        "test_num_scaled = scaler.transform(housing_clean.drop(\"ocean_proximity\", axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq8Yz1YGFJgd"
      },
      "source": [
        "### Agregando features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PKaPzvULO8j"
      },
      "source": [
        "La ingenieria de datos tambien ayuda! En particular, podemos agregar los siguientes features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2omPvrrz-3T"
      },
      "source": [
        "housing_clean[\"rooms_per_household\"] = housing_clean[\"total_rooms\"]/housing_clean[\"households\"]\n",
        "housing_clean[\"bedrooms_per_room\"] = housing_clean[\"total_bedrooms\"]/housing_clean[\"total_rooms\"]\n",
        "housing_clean[\"population_per_household\"]=housing_clean[\"population\"]/housing_clean[\"households\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR_qCyEkBPw9"
      },
      "source": [
        "housing_clean.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUaKp0J2ATpE"
      },
      "source": [
        "set(housing_clean[\"ocean_proximity\"].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwWYHDKBptjx"
      },
      "source": [
        "# Definion del problema y target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7D-hXewpx0i"
      },
      "source": [
        "El objetivo es poder predecir la mediana del precio de un distrito por sus caracteristicas. Es un problema de regresion univariada supervisada donde mi target es \"median_house_value\" y mis features son todas las otras categorias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdZVxIWIpzFk"
      },
      "source": [
        "Dado que es un problema de regresion, voy a usar una de las metricas mas comunes. El root mean squared error. Si mi target es $\\vec{t}=(t_1,t_2,...,t_N)^{T}$ y mis predicciones son $\\vec{y}=(y_1,y_2,...,y_N)^{T}$, entonces\r\n",
        "\r\n",
        "$\\text{RMSE}(\\vec{t},\\vec{y})=\\sqrt{\\frac{1}{N}\\sum_{n=1}^{N}(t_n-y_n)^{2}}$\r\n",
        "\r\n",
        "La idea del RMSE es dar un error esperado a la prediccion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp25P6ANqaIL"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\r\n",
        "\r\n",
        "def rmse(y,t):\r\n",
        "  return np.sqrt(mean_squared_error(y,t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hJiWTt1qfXB"
      },
      "source": [
        "t_test_rmse=[0.1,0.3,-0.1]\r\n",
        "y_test_rmse=[0.05,0.35,-0.05]\r\n",
        "print(rmse(t_test_rmse,y_test_rmse))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ecDPO7K0C86"
      },
      "source": [
        "# Preprocesado de datos con pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBpv3Py5-QlU"
      },
      "source": [
        "Voy a separar el target y escalear las variables numericas y re-expresar las categoricas. Combino todo en un pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goHRMguT-T7M"
      },
      "source": [
        "housing_labels = housing_clean[\"median_house_value\"].copy()\n",
        "# label_scaler=StandardScaler()\n",
        "# housing_labels_scaled=label_scaler.fit_transform(np.asarray(housing_labels).reshape(-1,1))[:,0]\n",
        "housing_clean = housing_clean.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
        "housing_cat = housing_clean[[\"ocean_proximity\"]]\n",
        "housing_num = housing_clean.drop(\"ocean_proximity\", axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-5PmE8f-hqQ"
      },
      "source": [
        "num_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy=\"median\")),#hay mas opciones aca\n",
        "        ('std_scaler', StandardScaler()),\n",
        "    ])\n",
        "\n",
        "num_attribs = list(housing_num)\n",
        "cat_attribs = [\"ocean_proximity\"]\n",
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "        (\"num\", num_pipeline, num_attribs),\n",
        "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
        "    ])\n",
        "\n",
        "housing_prepared = full_pipeline.fit_transform(housing_clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-yn076as_3C"
      },
      "source": [
        "Aca pasaron muchas cosas... Vamos paso por paso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMHO-X5ktDRi"
      },
      "source": [
        "El `num_pipeline` tiene dos pasos. Primero, el `SimpleImputer` se ocupa de rellenar los datos faltantes. Utiliza la mediana del feature faltante. Segundo, el `StandardScaler()` se ocupa de estandarizar los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvkYbsdMiC0d"
      },
      "source": [
        "housing_num_transformed=num_pipeline.fit_transform(housing_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bzJI35AtW6b"
      },
      "source": [
        "Veamos el imputer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1TcDUHPtfCM"
      },
      "source": [
        "housing_num.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omrMKgHotrMI"
      },
      "source": [
        "len(housing_num_transformed[np.where(housing_num['total_bedrooms'].isnull()>0)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHUbR6jruTFU"
      },
      "source": [
        "housing_num_transformed[np.where(housing_num['total_bedrooms'].isnull()>0),4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnpDibg2uczA"
      },
      "source": [
        "Y el StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqAs8YEBiTCu"
      },
      "source": [
        "num_pipeline.named_steps['std_scaler'].mean_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMEoGbiH_lhZ"
      },
      "source": [
        "housing_prepared.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOEumX243htu"
      },
      "source": [
        "# Regresion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7_k8wEMpB2Z"
      },
      "source": [
        "Jueguen ustedes un poco con las distintas opciones de modelo que damos a continuacion. Elijan cual les parece el mejor argumentandolo. El pseudo codigo de lo **minimo** que hay que hacer es el siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk7IMJLMpHAp"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\r\n",
        "from sklearn.tree import DecisionTreeRegressor\r\n",
        "from sklearn.ensemble import RandomForestRegressor\r\n",
        "\r\n",
        "''' pseudo codigo '''\r\n",
        "\r\n",
        "modelo = algoritmo()\r\n",
        "\r\n",
        "modelo.fit(algo...)\r\n",
        "\r\n",
        "y_pred_train = modelo.predict(algo...)\r\n",
        "\r\n",
        "print(metrica(algo...))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USN-rzuBrVrm"
      },
      "source": [
        "Recuerden que se puede ver la documentacion con "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uAXSfXJrZsi"
      },
      "source": [
        "LinearRegression?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOE4TBskrjMh"
      },
      "source": [
        "Y, si pueden, vayan mas alla del pseudo-codigo porque hay un trampa..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIyOSWcxo59b"
      },
      "source": [
        "# Mis soluciones (no lo vean antes de terminar con lo otro...)\r\n",
        "\r\n",
        "Vamos a resolver el problema. Voy a probar varios algoritmos (sin justificarlos bien, ya los vamos a ver) y evaluar la performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK2Zl05WpWXv"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from sklearn.metrics import mean_absolute_error\r\n",
        "from sklearn.tree import DecisionTreeRegressor\r\n",
        "from sklearn.ensemble import RandomForestRegressor\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn import tree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk6u0MaIGvVn"
      },
      "source": [
        "## Underfitting con Regresion Lineal\n",
        "\n",
        "Defino otro Pipeline solo para mostrar, no es estrictamente necesario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhCVMYbwvrjG"
      },
      "source": [
        "$y = w_0 + \\sum_{i=1}^{M}w_{i}\\phi_{i}(\\vec{x})$\n",
        "\n",
        "$\\phi_{i} = x_{i} $\n",
        "\n",
        "$y = w_0 + \\sum_{i=1}^{16}w_{i}x_{i}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emq7A8ZU3kGc"
      },
      "source": [
        "full_pipeline_with_predictor_lr = Pipeline([\n",
        "        (\"preparation\", full_pipeline),\n",
        "        (\"linear\", LinearRegression())\n",
        "    ])\n",
        "\n",
        "scores_lr=cross_val_score(full_pipeline_with_predictor_lr, housing, housing_labels,scoring=\"neg_mean_squared_error\", cv=10)#no lo aplico en housing_prepared, no deberia cambiar pero igual\n",
        "cross_scores_lr = np.sqrt(-scores_lr)\n",
        "\n",
        "print(\"Puntajes:\", cross_scores_lr)\n",
        "print(\"Media:\", cross_scores_lr.mean())\n",
        "print(\"Desviacion Estandar:\", cross_scores_lr.std())\n",
        "\n",
        "full_pipeline_with_predictor_lr.fit(housing, housing_labels)\n",
        "predictions_lr=full_pipeline_with_predictor_lr.predict(housing)\n",
        "print(\"Ejemplo: \", (round(predictions_lr[100]),housing_labels[100]))\n",
        "print(\"MSE Total del conjunto de entrenamiento:\", np.sqrt(mean_squared_error(predictions_lr,housing_labels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbhC7vCuNcpk"
      },
      "source": [
        "x=np.linspace(min(housing_labels),max(housing_labels),3)\n",
        "plt.scatter(housing_labels,predictions_lr)\n",
        "plt.plot(x,x,color='red')\n",
        "plt.xlabel('t')\n",
        "plt.ylabel('y')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCp1NAk0Gy3H"
      },
      "source": [
        "## Overfitting con Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEaHs-zmIWZh"
      },
      "source": [
        "tree_reg = DecisionTreeRegressor(random_state=42,max_depth=7)#,max_depth=5\n",
        "tree_reg.fit(housing_prepared, housing_labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-wPx3r3MUXZ"
      },
      "source": [
        "x=np.linspace(min(housing_labels),max(housing_labels),3)\n",
        "plt.scatter(housing_labels,tree_reg.predict(housing_prepared))\n",
        "plt.plot(x,x,color='red')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu8WTVBoIrE_"
      },
      "source": [
        "housing_predictions = tree_reg.predict(housing_prepared)\n",
        "tree_mse = mean_squared_error(housing_labels, housing_predictions)\n",
        "tree_rmse = np.sqrt(tree_mse)\n",
        "tree_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jezUic762ggk"
      },
      "source": [
        "tree_reg.get_n_leaves()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmopLBfwImyp"
      },
      "source": [
        "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
        "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
        "tree_rmse_scores = np.sqrt(-scores)\n",
        "\n",
        "print(\"Puntajes:\", tree_rmse_scores)\n",
        "print(\"Media:\", tree_rmse_scores.mean())\n",
        "print(\"Desviacion Estandar:\", tree_rmse_scores.std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LJVjpSxzvRk"
      },
      "source": [
        "tree.plot_tree(tree_reg) \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZL_AUp4G1jT"
      },
      "source": [
        "## Fitting con RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k6F8Vnx3kuZ"
      },
      "source": [
        "param_grid = [\n",
        "    # try 12 (3×4) combinations of hyperparameters\n",
        "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
        "    # then try 6 (2×3) combinations with bootstrap set as False\n",
        "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
        "  ]\n",
        "\n",
        "forest_reg = RandomForestRegressor(random_state=42)\n",
        "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
        "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
        "                           scoring='neg_mean_squared_error',\n",
        "                           return_train_score=True)\n",
        "grid_search.fit(housing_prepared, housing_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3OKERviFNSF"
      },
      "source": [
        "print(\"Best params:\", grid_search.best_params_)\n",
        "print(\"Best estimator:\", grid_search.best_estimator_)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNiu6CURFlfP"
      },
      "source": [
        "cvres = grid_search.cv_results_\n",
        "for mean_score, std, params in zip(cvres[\"mean_test_score\"], cvres[\"std_test_score\"], cvres[\"params\"]):\n",
        "    print(np.sqrt(-mean_score), np.sqrt(std),params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDjUbtpfFidO"
      },
      "source": [
        "best_rf=grid_search.best_estimator_\n",
        "print(\"Example: \", round(best_rf.predict(housing_prepared)[100]),housing_labels[100])\n",
        "print(\"Train MSE: \",np.sqrt(mean_squared_error(best_rf.predict(housing_prepared),housing_labels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvVUpsx2IRqC"
      },
      "source": [
        "x=np.linspace(min(housing_labels),max(housing_labels),3)\n",
        "plt.scatter(housing_labels,best_rf.predict(housing_prepared))\n",
        "plt.plot(x,x,color='red')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc6cdepvJQhN"
      },
      "source": [
        "## Una Red Neuronal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1Db32UoJS__"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "tf.random.set_seed(42)\n",
        "keras.backend.clear_session()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CJdx1htbm0S"
      },
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(housing_num, housing_labels, random_state=42)\n",
        "X_train_proc=num_pipeline.fit_transform(X_train)\n",
        "X_valid_proc=num_pipeline.transform(X_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDYUTqTUd3YF"
      },
      "source": [
        "# print(np.asarray(y_train).reshape(-1,1).shape)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "y_train_proc = scaler.fit_transform(np.asarray(y_train).reshape(-1,1))\n",
        "y_valid_proc = scaler.transform(np.asarray(y_valid).reshape(-1,1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsUBLQGsfPBB"
      },
      "source": [
        "y_train_proc[:,0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxVcZz0WOiFj"
      },
      "source": [
        "input_shape = X_train_proc.shape[1:]\n",
        "batch_size = 128\n",
        "epochs = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqx1scsFO-JW"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Dense(30, activation=\"relu\", input_shape=input_shape),\n",
        "    keras.layers.Dense(30, activation=\"relu\"),\n",
        "    keras.layers.Dense(1)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onONJ2yEO_uN"
      },
      "source": [
        "\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(lr=1e-3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo5Hs0yKPvXQ"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKzhZcUR7DBV"
      },
      "source": [
        "tf.keras.utils.plot_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDYKCCKXP6yP"
      },
      "source": [
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
        "                                                  restore_best_weights=True)\n",
        "history = model.fit(X_train_proc, y_train_proc[:,0], epochs=epochs,\n",
        "                    validation_data=(X_valid_proc, y_valid_proc[:,0]),\n",
        "                    callbacks=[early_stopping_cb])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlLswlXvQHp8"
      },
      "source": [
        "pd.DataFrame(history.history).plot()\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iysn_jZBS2V4"
      },
      "source": [
        "mse_train = model.evaluate(X_train_proc, y_train_proc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSe0PZJaRKxg"
      },
      "source": [
        "x=np.linspace(min(y_train_proc[:,0]),max(y_train_proc[:,0]),3)\n",
        "plt.scatter(y_train_proc[:,0],model.predict(X_train_proc))\n",
        "plt.plot(x,x,color='red')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wkk2c-q9TH7g"
      },
      "source": [
        "np.sqrt(mean_squared_error(scaler.inverse_transform(model.predict(X_train_proc)),scaler.inverse_transform(y_train_proc)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHfKOCz_stF5"
      },
      "source": [
        "# Vamos al Test\n",
        "\n",
        "En clase podemos hacerlo incompleto..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDbfuVQHsubH"
      },
      "source": [
        "housing_test_clean[\"rooms_per_household\"] = housing_test_clean[\"total_rooms\"]/housing_test_clean[\"households\"]\n",
        "housing_test_clean[\"bedrooms_per_room\"] = housing_test_clean[\"total_bedrooms\"]/housing_test_clean[\"total_rooms\"]\n",
        "housing_test_clean[\"population_per_household\"]=housing_test_clean[\"population\"]/housing_test_clean[\"households\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LT7dSsng29_"
      },
      "source": [
        "housing_test_labels = housing_test_clean[\"median_house_value\"].copy()\n",
        "housing_test_clean = housing_test_clean.drop(\"median_house_value\", axis=1) # drop labels for training set\n",
        "housing_test_cat = housing_test_clean[[\"ocean_proximity\"]]\n",
        "housing_test_num = housing_test_clean.drop(\"ocean_proximity\", axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxiZY_9XhGk4"
      },
      "source": [
        "housing_test_prepared = full_pipeline.transform(housing_test_clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G0YSzjH8ynT"
      },
      "source": [
        "Evaluo el mejor algoritmo: RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc-mBAVI81vd"
      },
      "source": [
        "print(\"Test MSE: \",np.sqrt(mean_squared_error(best_rf.predict(housing_test_prepared),housing_test_labels)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJsApqyDWTr8"
      },
      "source": [
        "# Algunos ejericicios (que pueden ser para dentro de unas clases...)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIul1KxrMXPV"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n",
        "*   Esta bueno poder mostrar un grafico lindo. En particular, el mapa de latitud y longitud es bastante claro. Jueguen con los tres algoritmos que utilizamos pero ahora utilizando como features latitud y longitud. Para cada algoritmo dibuje el mapa y las regiones inferidas de precio utilizando plt.contourf. Por que sugerimos utilizar unicamente dos variables a la hora de entrenar en lugar de utilizar los algoritmos ya entrenados?\r\n",
        "*   Reemplacen el GridSearchCV por el RandomizedSearchCV. No se preocupen, lo vamos a ver en detalle mas adelante.\r\n",
        "*   Fijense si puede juntar el preprocesado de los datos y los distintos algoritmos en un solo Pipeline. Que hiperparametros tiene? Pueden implementarlos en GridSearchCV?\r\n",
        "\r\n",
        "\r\n"
      ]
    }
  ]
}