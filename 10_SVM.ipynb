{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": [],
    "toc": true
   },
   "source": [
    "<h2>Índice<span class=\"tocSkip\"></span></h2>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preparatory-cells\" data-toc-modified-id=\"Preparatory-cells-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preparatory cells</a></span></li><li><span><a href=\"#Hard-margin-classifier\" data-toc-modified-id=\"Hard-margin-classifier-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Hard margin classifier</a></span></li><li><span><a href=\"#Soft-margin-classfier\" data-toc-modified-id=\"Soft-margin-classfier-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Soft margin classfier</a></span></li><li><span><a href=\"#The-power-of-kernels\" data-toc-modified-id=\"The-power-of-kernels-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>The power of kernels</a></span><ul class=\"toc-item\"><li><span><a href=\"#Kernel-construction\" data-toc-modified-id=\"Kernel-construction-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Kernel construction</a></span></li><li><span><a href=\"#Properties-of-some-kernels\" data-toc-modified-id=\"Properties-of-some-kernels-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Properties of some <em>kernels</em></a></span></li></ul></li><li><span><a href=\"#Kernels-in-action\" data-toc-modified-id=\"Kernels-in-action-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Kernels in action</a></span></li><li><span><a href=\"#Your-turn!\" data-toc-modified-id=\"Your-turn!-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Your turn!</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pqoMFY44ddu6",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Preparatory cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6Qox9gcddu7"
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os, sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=16)\n",
    "mpl.rc('xtick', labelsize=14)\n",
    "mpl.rc('ytick', labelsize=14)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"10_SVM\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"plots\", CHAPTER_ID)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "# import warnings\n",
    "# warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in sys.modules:\n",
    "    # Let us define a couple of useful functions\n",
    "    def plot_clasi(x, t, ws, labels=[], xp=[-1., 1.], thr=[0,], spines='zero', equal=True, join_centers=False,\n",
    "                   margin=None):\n",
    "        \"\"\"\n",
    "        Figura con el resultado del ajuste lineal\n",
    "        \"\"\"\n",
    "        assert len(labels) == len(ws) or len(labels) == 0\n",
    "        assert len(ws) == len(thr)\n",
    "\n",
    "        if margin is None:\n",
    "            margin = [False] * len(ws)\n",
    "        else:    \n",
    "            margin = np.atleast_1d(margin)\n",
    "        assert len(margin) == len(ws)\n",
    "\n",
    "        if len(labels) == 0:\n",
    "            labels = np.arange(len(ws)).astype('str')\n",
    "\n",
    "        # Agregemos el vector al plot\n",
    "    #     fig = plt.figure(figsize=(6, 6))\n",
    "        fig = plt.figure(figsize=(9, 7))\n",
    "        ax = fig.add_subplot(111)\n",
    "\n",
    "        xc1 = x[t == np.unique(t).max()]\n",
    "        xc2 = x[t == np.unique(t).min()]\n",
    "\n",
    "        ax.plot(*xc1.T, 'ob', mfc='None', label='C1')\n",
    "        ax.plot(*xc2.T, 'or', mfc='None', label='C2')\n",
    "\n",
    "        for i, w in enumerate(ws):\n",
    "\n",
    "            # Compute vector norm\n",
    "            wnorm = np.sqrt(np.sum(w**2))\n",
    "\n",
    "            # Ploteo vector de pesos\n",
    "            x0 = 0.5 * (xp[0] + xp[1])\n",
    "            ax.quiver(0, thr[i]/w[1], w[0]/wnorm, w[1]/wnorm, \n",
    "                      color='C{}'.format(i+2), scale=10, label=labels[i], \n",
    "                      zorder=10)\n",
    "\n",
    "            # ploteo plano perpendicular\n",
    "            xp = np.array(xp)\n",
    "            yp = (thr[i] - w[0]*xp)/w[1] \n",
    "\n",
    "            plt.plot(xp, yp, '-', color='C{}'.format(i+2))\n",
    "\n",
    "            # Plot margin\n",
    "            if margin[i]:\n",
    "                for marg in [-1, 1]:\n",
    "                    ym = yp + marg/w[1]\n",
    "                    plt.plot(xp, ym, ':', color='C{}'.format(i+2))\n",
    "\n",
    "\n",
    "        if join_centers:\n",
    "            # Ploteo línea que une centros de los conjuntos\n",
    "            mu1 = xc1.mean(axis=1)\n",
    "            mu2 = xc2.mean(axis=1)\n",
    "            ax.plot([mu1[0], mu2[0]], [mu1[1], mu2[1]], 'o:k', mfc='None', ms=10)    \n",
    "\n",
    "        ax.legend(loc=0, fontsize=12)\n",
    "        if equal:\n",
    "            ax.set_aspect('equal')\n",
    "\n",
    "        if spines is not None:\n",
    "            for a in ['left', 'bottom']:\n",
    "                ax.spines[a].set_position('zero')\n",
    "            for a in ['top', 'right']:\n",
    "                ax.spines[a].set_visible(False)\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def makew(fitter):\n",
    "\n",
    "        # # Obtengamos los pesos y normalicemos\n",
    "        w = fitter.coef_.copy()\n",
    "\n",
    "        # # Incluye intercept\n",
    "        if fitter.fit_intercept:\n",
    "            w = np.hstack([fitter.intercept_.reshape(1,1), w])\n",
    "\n",
    "        # # Normalizon\n",
    "        #w /= np.linalg.norm(w)\n",
    "        return w.T\n",
    "    \n",
    "else:\n",
    "    from utils import makew, plot_clasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OvXvAf62ddu2",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Hard margin classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\vv}[1]{\\boldsymbol{#1}}$\n",
    "$\\newcommand{\\om}[0]{\\boldsymbol{\\omega}}$\n",
    "$\\newcommand{\\norm}[0]{\\mathcal{N}}$\n",
    "$\\newcommand{\\b}[1]{\\mathrm{\\mathbf{#1}}}$\n",
    "$\\newcommand{\\T}{^\\mathrm{T}}$\n",
    "$\\newcommand{\\cu}{\\mathcal{C}_1}$\n",
    "$\\newcommand{\\cd}{\\mathcal{C}_2}$\n",
    "\n",
    "We have seen that for linearly separable problems there exists a linear combination of base functions, $\\phi_i(\\boldsymbol{x})$ that create a decision boundary with exact classification.\n",
    "\n",
    "However, it is not clear that this is the optimal frontier in terms of generalisation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "\n",
    "def make_dataset(mu1=[0, 0], mu2=[-4, 1.5], \n",
    "                 cov1=[[1, 0.95],[0.95, 1]], cov2=[[1, 0.8],[0.8, 1]], \n",
    "                 size1=250, size2=200, random_state=20200922):\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "    # Sample classes\n",
    "    xc1 = multivariate_normal(mean=mu1, cov=cov1, size=size1).T\n",
    "    xc2 = multivariate_normal(mean=mu2, cov=cov2, size=size2).T\n",
    "\n",
    "    print(xc1.shape, xc2.shape)\n",
    "\n",
    "    # Concatenate both classes\n",
    "    x = np.hstack([xc1, xc2]).T\n",
    "    tc1 = np.ones(xc1.shape[1])\n",
    "    tc2 = -np.ones(xc2.shape[1])\n",
    "    # tc2 = -np.ones((1, xc2.shape[1]))\n",
    "    t = np.hstack([tc1, tc2])\n",
    "    \n",
    "    # Make desing matrix adding a column of ones\n",
    "    phi0 = np.ones([len(x), 1])\n",
    "    phi = np.hstack([phi0, x.copy()])\n",
    "\n",
    "    return x, t, phi\n",
    "\n",
    "x, t, phi = make_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos cómo se ven\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(*x[t==1].T, 'ob', mfc='None', label='C1')\n",
    "ax.plot(*x[t==-1].T, 'or', mfc='None', label='C2')\n",
    "\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.legend(loc='lower right', fontsize=16)\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how a **single layer perceptron** works in this case.\n",
    "\n",
    "**What would you expect?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "perce = Perceptron(max_iter=1000, fit_intercept=False, warm_start=False)\n",
    "\n",
    "# Pero podríamos probar otras cosas, como esto que está comentado más abajo.\n",
    "perce = perce.fit(phi, t.flatten())\n",
    "\n",
    "w_perce = makew(perce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparamos con Fischer\n",
    "plot_clasi(x, t, [w_perce[1:],], ['Perceptron',], xp=[-4, 2], thr=[-w_perce[0],])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, but what happens if a new starting point is provided?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perce2 = Perceptron(max_iter=1000, fit_intercept=False, warm_start=True, random_state=1234)\n",
    "perce2 = perce2.fit(phi, t.flatten(), coef_init=[1.0, 4.0, 55.0])\n",
    "w_perce2 = makew(perce2)\n",
    "\n",
    "print(perce2.coef_)\n",
    "# print(w_perce2)\n",
    "# Comparamos con Fischer\n",
    "plot_clasi(x, t, [w_perce[1:], w_perce2[1:]], ['Perceptron', 'Perceptron2'], xp=[-4, 2], \n",
    "           thr=[-w_perce[0], -w_perce2[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may ask which of the two decision curves is best. And why..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use hard-margin classifiers, as we have just seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "# Instatiate\n",
    "svc = LinearSVC(loss='hinge', C=1e120, fit_intercept=False, penalty='l2')\n",
    "\n",
    "# Fit\n",
    "svc.fit(phi, t.flatten())\n",
    "\n",
    "# Obtain parameter vector for plot\n",
    "w_hmc = makew(svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the *support vectors* by looking for the data points which are on the margin (or inside it), which means that $y(\\mathbf{x})<= 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find support vectors\n",
    "isv = np.abs(svc.decision_function(phi)) <= 1.0 + 1e-12\n",
    "\n",
    "svc.decision_function(phi[isv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us use plot_clasi with the margin=True option\n",
    "plot_clasi(x, t, [w_hmc[1:],], ['Hard-margin',], xp=[-6, 2], \n",
    "           thr=[-w_hmc[0],], margin=True)\n",
    "\n",
    "# Plot support vectors\n",
    "plt.plot(x[isv, 0], x[isv, 1], 'o', ms=12, mfc='None', mec='k', mew=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = svc.decision_function(phi)\n",
    "plt.hist(y, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this classifier is sensitive to outliers.\n",
    "Let us be mean, and add an outlier point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outlier in design matrix\n",
    "phi_out = np.array([[1, -3, 0.75],])\n",
    "t_out = np.array([1,])\n",
    "\n",
    "phi_bad = np.vstack([phi, phi_out])\n",
    "t_bad = np.concatenate([t, t_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clasi(phi_bad[:, 1:], t_bad, [], [], xp=[-6, 2], thr=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img src=\"https://media.giphy.com/media/3o6Ztq5WTvF7LqQBgI/giphy.gif\" width=\"500\" height=\"270\" frameBorder=\"0\"></img> -->\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/3orif7aLUehOfdmlXy/giphy.gif\" width=\"450\" height=\"270\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC(loss='hinge', C=1e120, fit_intercept=False, penalty='l2', max_iter=100000)\n",
    "\n",
    "# Fit\n",
    "svc.fit(phi_bad, t_bad)\n",
    "\n",
    "# Obtain parameter vector for plot\n",
    "w_hmc_ol = makew(svc)\n",
    "\n",
    "# Find support vectors\n",
    "isv = np.abs(svc.decision_function(phi_bad)) <= 1.0 + 1e-5\n",
    "\n",
    "svc.decision_function(phi_bad[isv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(w_hmc_ol**2), np.sum(w_hmc**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clasi(phi_bad[:, 1:], t_bad, [w_hmc_ol[1:],], ['Hard-margin',], xp=[-6, 2], \n",
    "           thr=[-w_hmc_ol[0],], margin=True)\n",
    "\n",
    "# Plot support vectors\n",
    "plt.plot(phi_bad[isv, 1], phi_bad[isv, 2], 'o', ms=12, mfc='None', mec='k', mew=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = svc.decision_function(phi_bad)\n",
    "\n",
    "plt.hist(y, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Soft margin classfier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid these issues, we can use a soft margin classifier that allows some misclassification in order to reach a larger margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Igual que antes, excepto por el valor de C\n",
    "svc = LinearSVC(loss='hinge', C=1.0, fit_intercept=False, penalty='l2', max_iter=100000)\n",
    "\n",
    "# Fit\n",
    "svc.fit(phi_bad, t_bad)\n",
    "\n",
    "# Obtain parameter vector for plot\n",
    "w_smc_ol = makew(svc)\n",
    "\n",
    "# Find support vectors\n",
    "isv = np.abs(svc.decision_function(phi_bad)) <= 1.0 + 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_clasi(x, t, [w_hmc[1:], w_perce2[1:]], ['Hard-margin', 'Perceptron'], xp=[-6, 2], \n",
    "#            thr=[-w_hmc[0], -w_perce2[0]], margin=[True, False])\n",
    "plot_clasi(phi_bad[:, 1:], t_bad, [w_smc_ol[1:],], ['Soft-margin',], xp=[-6, 2], \n",
    "           thr=[-w_smc_ol[0],], margin=True)\n",
    "\n",
    "# Plot support vectors\n",
    "plt.plot(phi_bad[isv, 1], phi_bad[isv, 2], 'o', ms=12, mfc='None', mec='k', mew=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Where, of course, we should find the value of the penalisation `C` by cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should also work for non-separable sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, t, phi = make_dataset(mu2=[-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos cómo se ven\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(*x[t==1].T, 'ob', mfc='None', label='C1')\n",
    "ax.plot(*x[t==-1].T, 'or', mfc='None', label='C2')\n",
    "\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.legend(loc='lower right', fontsize=16)\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancio\n",
    "# hmc = LinearSVC(C=np.infty, fit_intercept=True, max_iter=1000)\n",
    "smc = LinearSVC(C=1.0, fit_intercept=False, max_iter=10000, loss='hinge')\n",
    "# smc = SVC(kernel='linear', degree=1, C=0.005, max_iter=1000)\n",
    "\n",
    "# Ajusto\n",
    "# hmc.fit(phi, t.flatten())\n",
    "smc.fit(phi, t.flatten())\n",
    "\n",
    "# w_hmc = makew(hmc).T\n",
    "w_smc = makew(smc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find support vectors\n",
    "isv = np.abs(smc.decision_function(phi)) <= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clasi(x, t, [w_smc[1:],], ['SMC', ], xp=[-4, 2], thr=[-w_smc[0],], margin=True)\n",
    "\n",
    "plt.plot(x[isv, 0], x[isv, 1], 'o', ms=12, mfc='None', mec='k', mew=2)\n",
    "# plt.plot(smc.support_vectors_[:, 0], smc.support_vectors_[:, 1], 'o', ms=12, mfc='None', mec='k', mew=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>¡A sus teclados!</font>\n",
    "\n",
    "* Encuentre el valor óptimo del parámetro C usando validación cruzada.\n",
    "* Usando la función `plot_clasi`, haga un gráfico como el de arriba para el valor óptimo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "import scipy.stats as st\n",
    "\n",
    "# Using randomised search\n",
    "parameters = {'C': st.loguniform(a=1e-3, b=10)}\n",
    "cvs = RandomizedSearchCV(smc, parameters, cv=10, n_iter=50)\n",
    "cvs.fit(phi, t.flatten())\n",
    "\n",
    "# Using grid search\n",
    "# parameters = {'C': np.logspace(-3, 0, 10)}\n",
    "# cvs = GridSearchCV(smc, parameters, cv=5)\n",
    "# cvs.fit(phi, t.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = st.loguniform(a=1e-3, b=10)\n",
    "dist.rvs(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cvs.best_params_)\n",
    "\n",
    "# Fit best model\n",
    "smc = cvs.best_estimator_\n",
    "smc.fit(phi, t.flatten())\n",
    "\n",
    "w_smc = makew(smc)\n",
    "\n",
    "# Identify support-vectors\n",
    "isv = np.abs(smc.decision_function(phi)) <= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clasi(x, t, [w_smc[1:],], ['SMC', ], xp=[-4, 2], thr=[-w_smc[0],], margin=True)\n",
    "\n",
    "plt.plot(x[isv, 0], x[isv, 1], 'o', ms=12, mfc='None', mec='k', mew=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The power of kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that predictions from a SVM can be expressed as:\n",
    "\n",
    "$$\n",
    "y(x_k) = \\sum_{i=1}^N a_i\\, t_i\\, k(x_i, x_k)\n",
    "$$\n",
    "\n",
    "where the $a_i$, with $i = \\{1, \\ldots, N\\}$ are the Lagrange multipliers that allow maximizing the margin. But more importantly, they can be shown to follow these conditions in the soft-margin case.\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    "a_i \\geq 0\\\\\n",
    "t_i y(\\boldsymbol{x}_i) -1 + \\xi_i \\geq 0\\\\\n",
    "a_i (t_i y(\\boldsymbol{x}_i) -1 + \\xi_i) = 0\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "and where \n",
    " $$\n",
    " \\boxed{k(x, x^\\prime) = \\boldsymbol{\\phi}(x)\\T \\boldsymbol{\\phi}(x^\\prime) = \\sum_{i=1}^M \\phi_i(x)\\T \\phi_i(x^\\prime)}\\;\\;.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, it can be seen that the only terms contributing to the prediction of a new point $y(\\boldsymbol{x_k})$ are the points which satisfy $t_i y(\\boldsymbol{x}_i) = 1 - \\xi_i$ . These are the **support vectors**.\n",
    "\n",
    "The soft-margin classifier we have been using it is therefore a **sparse vector machine**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel construction\n",
    "\n",
    "To build a valid *kernel* we can define a set of base functions $\\phi_i$, for $i = \\{1, \\ldots, M\\}$ and then perform the product as above.\n",
    " \n",
    "However, the true versatility of the technique appears when one directly defines a *kernel* function. But in this case, we must ensure that it is a valid function, that is, that it corresponds to a product of vectors of base functions, $\\boldsymbol{\\phi}(x)$.\n",
    " \n",
    "A necessary and sufficient condition is that the Gram matrix, $\\mathbf{K}$, whose element $n, m$ is $k(\\mathbf{x}_n, \\mathbf{x}_m)$ be positive semi-definite. This means that it must be fulfilled that\n",
    "\n",
    "$$\n",
    "\\mathbf{v}\\T \\mathbf{K} \\mathbf{v} \\geq 0\\;\\;\n",
    "$$\n",
    "for any vector $\\mathbf{v}$. This is equivalent to asking that the eigenvalues of the Gram matrix be greater than or equal to zero.\n",
    "\n",
    "**Note**: this is not equivalent to saying that all the elements of the matrix are positive. In the book by Bishop you can find the case of this innocent-looking matrix\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 2\\\\\n",
    "3 & 4\\\\\n",
    "\\end{pmatrix}\\;\\;,\n",
    "$$\n",
    "\n",
    "whose eigenvalues are $\\lambda_1 = 5.37$ and $\\lambda_2 = -0.37$ and is therefore not positive semi-definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One powerful way is to use *basic* kernels to build *more sophisticated* kernels, using the following properties (taken from Bishop):\n",
    "\n",
    "<!-- ![title](\"images/kernelconstruction.png\") -->\n",
    "<img src='images/kernelconstruction.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of some *kernels*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\k}[0]{k(\\mathbf{x}, \\mathbf{x}^\\prime)}$\n",
    "$\\newcommand{\\x}[0]{\\mathbf{x}}$\n",
    "We can see that the kernel $\\k = (\\mathbf{x}\\T \\mathbf{x}^\\prime)^2$ has degree-two terms. In two dimensions, $\\mathbf{x} = (x_1, x_2)$\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "(\\mathbf{x}\\T \\mathbf{z})^2 &= (x_1 z_1 + x_2 z_2)^2\\\\\n",
    "                            &= x_1^2 z_1^2 + 2 x_1 z_1 x_2 z_2 + x_2^2 z_2^2\\\\\n",
    "                            &= (x_1^2, \\sqrt{2}x_1 x_2, x_2^2) (z_1^2, \\sqrt{2}z_1 z_2, z_2^2)\\T\\\\\n",
    "                            &= \\boldsymbol{\\phi}(\\mathbf{x})\\T \\boldsymbol{\\phi}(\\mathbf{z})\\;\\;.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "In a similar way, $\\k = (\\mathbf{x}\\T \\mathbf{x}^\\prime)^M$ has all degree-$M$ terms. \n",
    "\n",
    "Besides $\\k = (\\mathbf{x}\\T \\mathbf{x}^\\prime + c)^2$ corresponds to a vector of base functions that contains constant and linear terms, in addition to those of order two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the more interesting *kernels* is called the \"Gaussian kernel\" or Radial Basis Function (RBF):\n",
    "\n",
    "$$\n",
    "\\k = \\exp{\\left(-\\gamma ||\\mathbf{x} - \\mathbf{x}^\\prime||^2\\right)}\\;\\;,\n",
    "$$\n",
    "\n",
    "which has a gaussian shape, but it has no normalization or anything like that. The parameter $\\gamma$ is the scale of distances between the points.\n",
    "\n",
    "An interesting feature of this *kernel* is that it corresponds to a vector of base functions of infinite dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.giphy.com/media/jNYUeDwoUoloEswJm8/giphy.gif\" width=\"350\" height=\"270\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>¡Muy importante!</font> Con kernels como el gaussiano, es crucial que los features tengan la misma escala. Por lo tanto, es fundamental usar `StandardScaler` o similar antes de analizar datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Kernels in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this *kernel* works with the dataset we've just prepared, and try to see what role the $\\gamma$ and $C$ hyperparameters play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in sys.modules:\n",
    "\n",
    "    def plot_svm(svc, x, t):\n",
    "\n",
    "        plt.figure(figsize=(9, 7))\n",
    "\n",
    "        xx, yy = np.meshgrid(np.linspace(x[:, 0].min()-1, x[:, 0].max()+1, 200), \n",
    "                             np.linspace(x[:, 1].min()-1, x[:, 1].max()+1, 200))\n",
    "\n",
    "        # evaluate decision function\n",
    "        Z = svc.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        # veamos la función de decisión y la frontera de decisión\n",
    "        plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu_r)\n",
    "        plt.contour(xx, yy, -Z, 0, colors='0.5', zorder=1)\n",
    "        plt.contour(xx, yy, -Z, [-1, 1], colors='0.25', linestyles='dashed', zorder=1)\n",
    "\n",
    "        xc1 = x[t == np.unique(t.flatten()).max()]\n",
    "        xc2 = x[t == np.unique(t.flatten()).min()]\n",
    "\n",
    "        plt.plot(*xc1.T, 'ob', mfc='None', label='C1')\n",
    "        plt.plot(*xc2.T, 'or', mfc='None', label='C2')\n",
    "\n",
    "        # Get suppor vector\n",
    "        xsv = svc.support_vectors_\n",
    "        plt.plot(xsv[:, 0], xsv[:, 1], 'o', ms=12, mfc='None', mec='k', mew=2)\n",
    "\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.axis('tight')\n",
    "\n",
    "        return\n",
    "    \n",
    "else:\n",
    "    from utils import plot_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, t, phi = make_dataset(mu2=[-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clasi(x, t, [], [], [], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standarise\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Instantiate class\n",
    "svc_rbf = SVC(kernel='rbf', C=100.0, gamma=10.0)\n",
    "svc_rbf.fit(x, t.flatten())\n",
    "\n",
    "plot_svm(svc_rbf, x, t)\n",
    "\n",
    "# svc_poly = SVC(kernel='poly', C=10000.0, degree=3)\n",
    "# svc_poly.fit(X, t.flatten())\n",
    "\n",
    "# plot_svm(svc_poly, X, t, colorbar=True,)# vmin=-200, vmax=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very nice, but let's try something that we could not have done otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, t = make_moons(n_samples=200, noise=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "plot_clasi(X, t, [], [], [], [], join_centers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(X)\n",
    "# Scaled\n",
    "plot_clasi(X, t, [], [], [], [], join_centers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_rbf = SVC(kernel='rbf', C=1000.0, gamma=0.1)\n",
    "svc_rbf.fit(X, t.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import utils\n",
    "# importlib.reload(utils)\n",
    "# from utils import plot_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svm(svc_rbf, X, t, colorbar=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_poly = SVC(kernel='poly', degree=5, coef0=10.0, C=1.0)\n",
    "svc_poly.fit(X, t.flatten())\n",
    "\n",
    "plot_svm(svc_poly, X, t, colorbar=True, vmin=-100, vmax=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## ¡De vuelta su turno!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Usen Cross-Validation para encontrar los hiperparámetros óptimos para el SVC usando un kernel `rbf` en el dataset `make_moons`. Optimizar ambos parámetros.\n",
    "* Exploren las posibilidades de las SVM en el dataset de Iris, usando primero dos clases (las que ustedes elijan), y luego para una clasificación multi-clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "params = {'C': np.logspace(-2, 3, 20), 'gamma': np.logspace(-2, 3, 20)}\n",
    "\n",
    "gscv = GridSearchCV(svc_rbf, param_grid=params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "gscv.fit(X, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gscv.best_params_, gscv.best_score_)\n",
    "plot_svm(gscv.best_estimator_, X, t, colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris=datasets.load_iris()\n",
    "X = iris.data\n",
    "t = iris.target\n",
    "\n",
    "X = X[t > 0]\n",
    "t = t[t > 0]\n",
    "\n",
    "# Plot also the training points\n",
    "for pair in ([0, 1], [0, 2], [2, 3]):\n",
    "  plt.scatter(X[:, pair[0]], X[:, pair[1]], c=t, edgecolors='k', cmap=plt.cm.Paired)\n",
    "  plt.xlabel(iris.feature_names[pair[0]])\n",
    "  plt.ylabel(iris.feature_names[pair[1]])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Índice",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
